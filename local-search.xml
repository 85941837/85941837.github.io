<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>使用卷积进行泛化</title>
    <link href="/2024/11/03/2.%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/"/>
    <url>/2024/11/03/2.%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="卷积神经网络基础概念"><a href="#卷积神经网络基础概念" class="headerlink" title="卷积神经网络基础概念"></a>卷积神经网络基础概念</h2><p>卷积神经网络*(convolutional neural network，CNN)*是一种前馈神经网络，由一个或多个卷积层和顶端的全连通层，同时也包括关联权重和池化层。卷积具有两种性质：平移不变性和局部性。</p><p>重新考虑全连接层(全连接层的公式考虑为$y &#x3D; w^{T}X+b$)的概念，将输入和输出变形为矩阵，可以得到以下的公式：<br>$$<br>[H]<em>{i,j} &#x3D; [U]</em>{i,j}+ {\textstyle \sum_{k}^{}}  {\textstyle \sum_{l}^{}}[W] <em>{i,j,k,l}\times [X]</em>{k,l}<br>$$<br>其中公式解释如下：</p><ul><li>$[H]_{i,j}$：表示隐藏层在位$(i,j)$的值。</li><li>$[U]_{i，j}$表示一个偏置项</li><li>$[W] _{i,j,k,l}$ 是一个四阶权重张量，表示输入像素$(k，l)$对隐藏层位置$(i,j)$的影响权重，$[W] <em>{i,j,k,l}\times [X]</em>{k,l}$表示像素$(K,L)$对隐藏层位置$(i,j)$的贡献。</li><li>$[X]_{k,l}$是输入图像在位置$(i,j)$的像素位置。</li></ul><p>进行一个假设用$[V] <em>{i,j,a,b}$替换$[W] <em>{i,j,k,l}$，并假设：$k &#x3D; i+a$， $l &#x3D; i+a$ ，我们通过改变下标，得到全新的权重(v是w的重新索引)$v</em>{i,j,a,b} &#x3D; w</em>{i,j,i+a,j+b}$最终得到公式：<br>$$<br>[H]<em>{i,j} &#x3D; [U]</em>{i,j}+ {\textstyle \sum_{a}^{}}  {\textstyle \sum_{b}^{}}[V] <em>{i,j,a,b}\times [X]</em>{x+a,j+b}<br>$$<br>这实际上是一个典型的卷积操作，其中$[V] <em>{i,j,a,b}$是卷积核，$[X]</em>{x+a,j+b}$是图像在卷积核覆盖的区域内的像素。这种表示方式在图像处理和神经网络中被广泛使用，因为它利用图像的局部空间关系，同时减少模型的参数量。</p><h2 id="卷积的性质"><a href="#卷积的性质" class="headerlink" title="卷积的性质"></a>卷积的性质</h2><h3 id="平移不变性"><a href="#平移不变性" class="headerlink" title="平移不变性"></a>平移不变性</h3><p>引用上述第一个原则：平移不变性。及意味着检测对象在输入$X$中的平移，应仅仅导致隐藏向量$H$的平移，也就是说$v$不应该依赖于$j$。因此，公式可以简化为<br>$$<br>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum_a\sum_b [\mathbf{V}]</em>{a, b} [\mathbf{X}]_{i+a, j+b}.<br>$$</p><h3 id="局部性"><a href="#局部性" class="headerlink" title="局部性"></a>局部性</h3><p>局部性，用来训练参数$[\mathbf{H}]<em>{i, j}$的相关信息，我们不应偏离到距$(i,j)$很远的地方。所以给出的解决方案是**$|a|&gt; \Delta$**或者$|b| &gt; \Delta$，设置$[\mathbf{V}]</em>{a, b} &#x3D; 0$,及公式表示为：<br>$$<br>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum</em>{a &#x3D; -\Delta}^{\Delta} \sum_{b &#x3D; -\Delta}^{\Delta} [\mathbf{V}]<em>{a, b}  [\mathbf{X}]</em>{i+a, j+b}.<br>$$</p><h2 id="卷积的数学公式"><a href="#卷积的数学公式" class="headerlink" title="卷积的数学公式"></a>卷积的数学公式</h2><p>数学中，两个函数形如如下形式的($f, g: \mathbb{R}^d \to \mathbb{R}$)，定义为卷积：<br>$$<br>(f * g)(\mathbf{x}) &#x3D; \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.<br>$$<br>卷积是把一个函数”翻转“并位移x，测量$f$和$g$之间的重叠。党为离散对象时，积分则变为求和，可以得到如下定义，该一维公式多用于文本、语言、时间序列的模型中。<br>$$<br>(f * g)(i) &#x3D; \sum_a f(a) g(i-a).<br>$$<br>以此类推，二维卷积的公式为：<br>$$<br>(f * g)(i, j) &#x3D; \sum_a\sum_b f(a, b) g(i-a, j-b).<br>$$</p><h2 id="感受野的概念"><a href="#感受野的概念" class="headerlink" title="感受野的概念"></a>感受野的概念</h2><p>卷积神经网络中每一层输出的特征图上的像素点在原始图像上映射的区域大小，<strong>第一层</strong>卷积层的输出特征图像素的<strong>感受野大小等于卷积核的大小</strong>，其它卷积层的输出特征的感受野的大小和它之前所有层的卷积核的大小和步长都有关。</p><p><img src="/../imgs/2.%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/image-20241107111532799.png" alt="感受野图例"></p><p>感受野的计算公式：<br>$$<br>r_{n} &#x3D; r_{n-1}+(k_{n-1})\coprod_{i &#x3D; 1}^{n-1}S_{i}<br>$$<br>其中$n\ge 2$</p><p>$k:$kernel size</p><p>$S_{n}:$Stride</p><p>$r_{n}:$receptive-field，感受野，n表示层数</p><p><img src="/../imgs/2.%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/image-20241107112843015.png" alt="感受野计算例子"></p><p>计算过程：</p><p>$r_{1} &#x3D; 11$</p><p>$r_{2} &#x3D; 11+(3-1)\times 4 &#x3D; 19$</p><p>$r_{3} &#x3D; 19+(5-1)\times 4\times 2 &#x3D; 51$</p><h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>卷积的输出形状填充$(padding)$和步幅$(stride)$的影响。</p><p><strong>填充</strong>，以下图为例在图像的边界填充元素：</p><p><img src="/../imgs/2.%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/image-20241108140413936.png" alt="在卷积周围填充0"></p><p>如果我们添加$p_{h}$行填充和$p_{w}$列填充，则输出的形状为：<br>$$<br>(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)。<br>$$<br><strong>步幅</strong>，滑块滑动的距离被称为步幅，如下图为垂直步幅为3，水平步幅为2的二维互相关运算。</p><p><img src="/../imgs/2.%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/image-20241108142335406.png" alt="image-20241108142335406"></p><p>如果$p_{h}$行填充和$p_{w}$列填充，水平步幅为$s_{w}$、垂直步幅为$s_{h}$则输出的形状为：<br>$$<br>\lfloor(n_h-k_h+p_h+s_h)&#x2F;s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)&#x2F;s_w\rfloor.<br>$$</p><h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><p>卷积对位置信息较为敏感，用池化减少位置信息对卷积层：</p><p><img src="/../imgs/2.%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/image-20241108155255475.png" alt="最大池化层样例"></p><p>简单池化层基础实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pool2d</span>(<span class="hljs-params">X, pool_size, mode=<span class="hljs-string">&#x27;max&#x27;</span></span>):<br>    <span class="hljs-comment"># 定义池化层的长、宽参数</span><br>    p_h, p_w = pool_size<br>    <span class="hljs-comment"># 初始化池化层的输出</span><br>    Y = torch.zeros((X.shape[<span class="hljs-number">0</span>] - p_h + <span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>] - p_w + <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;max&#x27;</span>:<br>                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="hljs-built_in">max</span>()<br>                <br>            <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;avg&#x27;</span>:<br>                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()<br></code></pre></td></tr></table></figure><h2 id="卷积的代码理解"><a href="#卷积的代码理解" class="headerlink" title="卷积的代码理解"></a>卷积的代码理解</h2><p>一维卷积的运算：</p><p><img src="/../imgs/2.%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/image-20241107105657295.png" alt="单通道计算"></p><p>多输入通道的计算：</p><p><img src="/../imgs/2.%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/image-20241107105809574.png" alt="两个输入通道的互相关计算"></p><p>定义一个卷积层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Conv2D</span>(nn.Block):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, kernel_size, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.weight = <span class="hljs-variable language_">self</span>.params.get(<span class="hljs-string">&#x27;weight&#x27;</span>, shape=kernel_size)<br>        <span class="hljs-variable language_">self</span>.bias = <span class="hljs-variable language_">self</span>.params.get(<span class="hljs-string">&#x27;bias&#x27;</span>, shape=(<span class="hljs-number">1</span>,))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> corr2d(x, <span class="hljs-variable language_">self</span>.weight.data()) + <span class="hljs-variable language_">self</span>.bias.data()<br></code></pre></td></tr></table></figure><p>定义一个可学习的卷积核：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核</span><br>conv2d = nn.Conv2D(<span class="hljs-number">1</span>, kernel_size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), use_bias=<span class="hljs-literal">False</span>)<br>conv2d.initialize()<br><br><span class="hljs-comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span><br><span class="hljs-comment"># 其中批量大小和通道数都为1</span><br><br><br>X = X.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>)<br>Y = Y.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>)<br>lr = <span class="hljs-number">3e-2</span>  <span class="hljs-comment"># 学习率</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    <span class="hljs-keyword">with</span> autograd.record():<br>        Y_hat = conv2d(X)<br>        l = (Y_hat - Y) ** <span class="hljs-number">2</span><br>    l.backward()<br>    <span class="hljs-comment"># 迭代卷积核</span><br>    conv2d.weight.data()[:] -= lr * conv2d.weight.grad()<br>    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;epoch <span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>, loss <span class="hljs-subst">&#123;<span class="hljs-built_in">float</span>(l.<span class="hljs-built_in">sum</span>()):<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">epoch</span> <span class="hljs-number">2</span>, loss <span class="hljs-number">4</span>.<span class="hljs-number">949</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">4</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">831</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">6</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">140</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">8</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">024</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">10</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">004</span><span class="hljs-meta"></span><br><span class="hljs-meta">[07:16:32] ../src/base.cc:48: GPU context requested, but no GPUs found.</span><br></code></pre></td></tr></table></figure></blockquote><p>带有步幅和填充的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">conv2d = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), padding=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), stride=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>博客搭建</title>
    <link href="/2024/11/02/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    <url>/2024/11/02/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="一-搭建准备"><a href="#一-搭建准备" class="headerlink" title="一. 搭建准备"></a>一. 搭建准备</h2><p> 搭建之前需要准备的软件：</p><p><a href="https://nodejs.cn/">nodejs</a> </p><p><a href="https://git-scm.com/">git</a> ，git安装详见网上安装步骤</p><h2 id="二，-安装hexo，完成简单本地页面展示"><a href="#二，-安装hexo，完成简单本地页面展示" class="headerlink" title="二， 安装hexo，完成简单本地页面展示"></a>二， 安装hexo，完成简单本地页面展示</h2><p>1.以管理员身份进入cmd窗口输入指令：</p><p><code>npm install -g hexo-cli</code></p><p>2.先创建一个文件夹myblog，在这个文件夹下直接右键git bash打开</p><p>然后初始化hexo</p><p><code>hexo init  // 初始化 hexo g  // 生成静态文件 hexo s  // 启动服务预览</code></p><p>在浏览器中输入<a href="http://localhost:4000/%E5%8D%B3%E5%8F%AF%E7%9C%8B%E5%88%B0">http://localhost:4000/即可看到</a></p><p> <em>新建完成后，指定文件夹目录下有：</em></p><ul><li>node_modules: 依赖包 </li><li>public：存放生成的页面 </li><li>scaffolds：生成文章的一些模板 </li><li>source：用来存放你的文章 t</li><li>hemes：主题 </li><li>_config.yml: 博的配置文件</li></ul><h2 id="三，-变更主题"><a href="#三，-变更主题" class="headerlink" title="三， 变更主题"></a>三， 变更主题</h2><p>在github上搜索：<a href="https://github.com/fluid-dev/hexo-theme-fluid%EF%BC%8C%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6">https://github.com/fluid-dev/hexo-theme-fluid，下载文件</a></p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102204608486.png" alt="image-20241102204608486"></p><p>将文件拷贝到博客根目录themes文件内，同时在根目录创建_config.fluid.yml文件，</p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102204855804.png" alt="image-20241102204855804"></p><p>然后使用记事本打开<code>Blog</code>文件中的<code>_config.yml</code>文件，更换主题名称：</p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102205130911.png" alt="image-20241102205130911"></p><h2 id="四-配置SSH-Key"><a href="#四-配置SSH-Key" class="headerlink" title="四. 配置SSH Key"></a>四. 配置SSH Key</h2><p>鼠标右击打开<code>Git Bash</code></p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/0ab87d6b4d864f97817f818d0f9b6948.png" alt="img"></p><p>若还没有user.name 和user.email，先配置</p><p><code>git config --global user.name &quot;你的GitHub用户名&quot; git config --global user.email &quot;你的GitHub注册邮箱&quot;</code></p><h3 id="生成ssh-key"><a href="#生成ssh-key" class="headerlink" title="生成ssh key"></a>生成ssh key</h3><p>使用下面命令生成ssh-key</p><p><code>ssh-keygen -t rsa -C &quot;xxx@xxx.com&quot;  // 将 &quot;xxx@xxx.com&quot; 替换为你自己GitHub的邮箱地址</code></p><p>然后一直按 “enter”键，如下图</p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102205558493.png" alt="image-20241102205558493"></p><p>然后到c盘目录（C:\Users\用户名）下查找**.shh<strong>文件，复制</strong>cat id_rsa.pub<strong>文件内的密钥，将内容</strong>全部**复制，找到Github Setting keys页面，新建new SSH Key</p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102210255510.png" alt="image-20241102210255510"></p><h3 id="检查是否设置成功"><a href="#检查是否设置成功" class="headerlink" title="检查是否设置成功"></a>检查是否设置成功</h3><p><code>$ ssh -T git@github.com</code></p><p>看到<strong>successfully</strong>字样就成功了</p><h2 id="五-连接Hexo与GitHub"><a href="#五-连接Hexo与GitHub" class="headerlink" title="五. 连接Hexo与GitHub"></a>五. 连接Hexo与GitHub</h2><p>打开blog文件中的_config.yml（即站点配置文件），翻到最后修改为：</p><blockquote><p>deploy:<br>     type: git<br>     repository:<a href="mailto:&#103;&#105;&#x74;&#64;&#x67;&#105;&#116;&#x68;&#x75;&#98;&#46;&#x63;&#111;&#x6d;">&#103;&#105;&#x74;&#64;&#x67;&#105;&#116;&#x68;&#x75;&#98;&#46;&#x63;&#111;&#x6d;</a>:github邮箱&#x2F;github邮箱.github.io.git<br>     branch: main</p></blockquote><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102210556676.png"></p><p>在github上新建一个仓库</p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102211126683.png" alt="image-20241102211126683"></p><p>最后安装Git部署插件：（在根目录下执行，而不是在git bash里）</p><p><code>npm install hexo-deployer-git --save</code></p><p>这时再输入以下命令：</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">hexo c   #清除缓存文件 db<span class="hljs-variable">.json</span> 和已生成的静态文件 public<br>hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo <span class="hljs-keyword">generate</span> 的缩写)<br>hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)<br></code></pre></td></tr></table></figure><p><strong>现在就可以使用xxx.github.io来访问你的博客啦</strong></p><p> <strong>例如：我的用户名是85941837，那么我的博客地址就是<code>85941837.github.io</code></strong></p><h2 id="六-配置域名"><a href="#六-配置域名" class="headerlink" title="六. 配置域名"></a>六. 配置域名</h2><p>在华为云上购买域名，在查询上搜索<strong>域名</strong></p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102211321606.png" alt="image-20241102211321606"></p><p>购买域名，第一次购买需要填写信息模板</p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102211450237.png" alt="image-20241102211450237"></p><p>然后进行域名解析，</p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102211640695.png" alt="image-20241102211640695"></p><p>点击快速添加域名解析，选择网站解析中提供的cname域名，将github上的xxx.github.io填写上即可。</p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102211820124.png" alt="image-20241102211820124"></p><h2 id="七-其他"><a href="#七-其他" class="headerlink" title="七. 其他"></a>七. 其他</h2><h3 id="hexo-d-部署后总需要重新改域名解决办法"><a href="#hexo-d-部署后总需要重新改域名解决办法" class="headerlink" title="hexo d 部署后总需要重新改域名解决办法"></a>hexo d 部署后总需要重新改域名解决办法</h3><p>在source目录下（不是<a href="https://so.csdn.net/so/search?q=hexo&spm=1001.2101.3001.7020">hexo</a>根目录下），创建一个CNAME文件，可以用sublime创建，然后保存成（All files格式）CNAME文件里写自己新的域名</p><p><img src="/../imgs/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20241102212052474.png" alt="image-20241102212052474"></p><p>hexo g 重新生成一下</p><p>hexo d 部署到github上</p>]]></content>
    
    
    <categories>
      
      <category>博客搭建</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>张量的基本操作</title>
    <link href="/2024/11/02/1.%E5%BC%A0%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <url>/2024/11/02/1.%E5%BC%A0%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="一-张量的基础概念"><a href="#一-张量的基础概念" class="headerlink" title="一. 张量的基础概念"></a>一. 张量的基础概念</h2><p>在学深度学习里，<strong>Tensor实际上就是一个多维数组（multidimensional array）</strong>，是一种最基础的数据结构。</p><h2 id="二-张量常用操作"><a href="#二-张量常用操作" class="headerlink" title="二. 张量常用操作"></a>二. 张量常用操作</h2><h4 id="访问某一个元素（最后一个跳跃访问，每三行访问一个元素，每两列访问一个元素）"><a href="#访问某一个元素（最后一个跳跃访问，每三行访问一个元素，每两列访问一个元素）" class="headerlink" title="访问某一个元素（最后一个跳跃访问，每三行访问一个元素，每两列访问一个元素）"></a><strong>访问某一个元素（最后一个跳跃访问，每三行访问一个元素，每两列访问一个元素）</strong></h4><p><img src="/../imgs/1.%E5%BC%A0%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/1715695979821-5ddedfb0-17de-4ac8-a7b8-8361a8c60ea9.webp" alt="image-20240514215004991.png"></p><p><strong>构造一个张量：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>a = torch.ones(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p><strong>使用ndim查看张量的维度：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>t1.ndim <br></code></pre></td></tr></table></figure><p>输出：1</p><p><strong>使用shape查看形状</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>t1.shape<br></code></pre></td></tr></table></figure><p><strong>由两个形状相同的二维数组创建一个三维的张量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a1 = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">4</span>]])<br>a2 = np.array([[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">8</span>]])<br>t3 = torch.tensor([a1,a2])<br></code></pre></td></tr></table></figure><p>输出：结果是一个三位向量</p><p>tensor([[[1, 2, 2],<br>[3, 4, 4]],<br>[[5, 6, 6],<br>[7, 8, 8]]], dtype&#x3D;torch.int32)</p><p><strong>flatten拉平，将任意维度张量转化为一维张量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t2 = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>t2.flatten()<br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([1, 2, 3, 4])</p><p><strong>reshape方法，任意变形</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>t1.reshape(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[1],[2]])</p><p>torch.Size([2, 1])</p><p><strong>特殊张量创建：****全零张量 .zeros()、全1张量 .ones()、单位矩阵 .eyes()、对角矩阵 .diag(一维张量)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.eye(<span class="hljs-number">5</span>)<br>t1 = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br>torch.diag(t1)<br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[1., 0., 0., 0., 0.],<br>[0., 1., 0., 0., 0.],<br>[0., 0., 1., 0., 0.],<br>[0., 0., 0., 1., 0.],<br>[0., 0., 0., 0., 1.]])</p><p>tensor([[1, 0],</p><p>[0, 2]])</p><p><strong>服从0-1均匀分布的张量rand</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<span class="hljs-comment">#2行3列的随机数所构成的张量</span><br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[-0.8110, -1.1295, -0.2913],<br>[-1.1786, -0.8882, 0.2433]])</p><p><strong>arange&#x2F;linspace:生成数列</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.arange(<span class="hljs-number">5</span>)<br>torch.arange(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>) <span class="hljs-comment">#从1到5，每隔0.5取一个数</span><br>torch.linspace(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>) <span class="hljs-comment">#从1取到5，等距取三个数</span><br></code></pre></td></tr></table></figure><p><strong>empty，初始化指定形状矩阵</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.full([<span class="hljs-number">2</span>,<span class="hljs-number">4</span>],<span class="hljs-number">2</span>) <span class="hljs-comment">#2行4列，数值为2</span><br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[2, 2, 2, 2],<br>[2, 2, 2, 2]])</p><p><strong>创建指定形状的数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.full_like(t1,<span class="hljs-number">2</span>) <span class="hljs-comment">#根据t1的形状，填充数值2</span><br>torch.randint_like(t2,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>) <span class="hljs-comment">#在1到10中随机抽取一些整数，并将其填充进t2的形状中去</span><br></code></pre></td></tr></table></figure><p><strong>张量与其它相关类型(数组、列表)之间转化方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>])<br>t1.numpy()<br>t1.tolist()<br></code></pre></td></tr></table></figure><p>输出：</p><p>array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype&#x3D;int64)</p><p>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</p><p><strong>切片，一小点需要注意</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">t2[[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>],<span class="hljs-number">1</span>] <span class="hljs-comment"># 第一行和第3行的第2列元素</span><br></code></pre></td></tr></table></figure><p><strong>分块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tc = torch.chunk(t2,<span class="hljs-number">4</span>,dim=<span class="hljs-number">0</span>) <br></code></pre></td></tr></table></figure><p>输出：<br>(tensor([[0, 1, 2]]),<br>tensor([[3, 4, 5]]),<br>tensor([[6, 7, 8]]),<br>tensor([[ 9, 10, 11]]))</p><p><strong>拆分****：split函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t2 = torch.arange(<span class="hljs-number">12</span>).reshape(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>)<br>torch.split(t2,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>)  <br></code></pre></td></tr></table></figure><p>输出：</p><p>(tensor([[0, 1, 2],<br>[3, 4, 5]]),<br>tensor([[ 6, 7, 8],<br>[ 9, 10, 11]]))</p><p>torch.split(t2,[1,3],0)</p><p>[1,3]表示“第一块长度为1，第二块长度为3</p><p><strong>torch.cat 的用法：多个张量合并在一起,默认按行(dim&#x3D;0)【记住二维张量dim&#x3D;0表示行】进行拼接，维度不匹配会报错</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">z = torch.arange(<span class="hljs-number">12</span>, dtype=torch.float32).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>k = torch.tensor([[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">1</span>]])<br>h = torch.cat((z,k), dim=<span class="hljs-number">0</span>), torch.cat((z,k), dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(h)<br><br>输出：<br>(tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>],<br>        [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">7.</span>],<br>        [ <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">11.</span>],<br>        [ <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>,  <span class="hljs-number">1.</span>]]),<br> tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">7.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">11.</span>,  <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>,  <span class="hljs-number">1.</span>]]))<br></code></pre></td></tr></table></figure><p>【-1】访问最后一个元素，【1：3】选择第一行和第二行的元素（左闭右开）</p><p><strong>张量的维度变化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>输出:<br>tensor([[ <span class="hljs-number">0.6480</span>, <span class="hljs-number">1.5947</span>, <span class="hljs-number">0.6264</span>, <span class="hljs-number">0.6051</span>],<br>[ <span class="hljs-number">1.6784</span>, <span class="hljs-number">0.2768</span>, -<span class="hljs-number">1.8780</span>, -<span class="hljs-number">0.1133</span>],<br>[-<span class="hljs-number">0.6442</span>, <span class="hljs-number">0.8570</span>, <span class="hljs-number">0.1677</span>, <span class="hljs-number">0.2378</span>]])<br>mask = x.ge(<span class="hljs-number">0.5</span>)<br>输出：<br>tensor([[ <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>],<br>[ <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>],<br>[<span class="hljs-literal">False</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]])<br><br>torch.masked_select(x,mask) <br>输出：<br>tensor([<span class="hljs-number">0.6480</span>, <span class="hljs-number">1.5947</span>, <span class="hljs-number">0.6264</span>, <span class="hljs-number">0.6051</span>, <span class="hljs-number">1.6784</span>, <span class="hljs-number">0.8570</span>])<br></code></pre></td></tr></table></figure><p><strong>删除不必要的维度</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.zeros(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)<br>torch.squeeze(t).shape<span class="hljs-comment">#剔除所有属性为1的维度</span><br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([3])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">b.squeeze(<span class="hljs-number">0</span>).shape <span class="hljs-comment">#挤压掉第0维</span><br>b.squeeze(-<span class="hljs-number">1</span>).shape  <span class="hljs-comment">#挤压掉最后一维</span><br>b.squeeze(<span class="hljs-number">1</span>).shape <span class="hljs-comment">#挤压掉第一维</span><br></code></pre></td></tr></table></figure><p><strong>unsqueeze手动升维</strong></p><p>调用方法1：torch.unsqueeze(t, dim&#x3D;n)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.zeros(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 在第0位索引上升高一个维度变成五维</span><br>torch.unsqueeze(t,dim=<span class="hljs-number">0</span>).shape <br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([1, 1, 2, 1, 2])</p><p>调用方法2：a.unsqueeze(dim&#x3D;n)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在0索引前面插入了一个额外的维度</span><br>a.unsqueeze(<span class="hljs-number">0</span>).shape <br><span class="hljs-comment"># 在末尾插入一个额外的维度，可以理解为像素的属性</span><br>a.unsqueeze(-<span class="hljs-number">1</span>).shape <br></code></pre></td></tr></table></figure><p><strong>expand:broadcasting #只改变理解方式，不增加数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">b = torch.rand(<span class="hljs-number">1</span>,<span class="hljs-number">32</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>b.expand(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>,<span class="hljs-number">14</span>,<span class="hljs-number">14</span>).shape   <br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 32, 14, 14])</p><p><strong>repeat的参数表示重复的次数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 4表示对0维重复4次，32表示对1维重复32次</span><br>b.repeat(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>).shape<br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 1024, 1, 1])</p><p><strong>矩阵转置</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>a.shape<br>a.t()<br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 3])</p><p><strong>维度转换</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># transpose实现维度两两交换</span><br>a = torch.rand(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)<br><span class="hljs-number">2</span> = a.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>).contiguous().view(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>*<span class="hljs-number">32</span>*<span class="hljs-number">32</span>).view(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>解释：</p><p>#transpose包含了要交换的两个维度[b,c,h,w]→[b,w,h,c]</p><p>#数据的维度顺序必须与存储顺序一致,用.contiguous把数据变成连续的</p><p>#.view(4,33232) [b,whc]</p><p>#.view(4,3,32,32) [b,w,h,c]</p><p>#.transpose(1,3) [b,c,g,w]</p><p><strong>permute维度转换</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">b = torch.rand(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">32</span>) <br>b.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>).shape    <br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 28, 32, 3])</p>]]></content>
    
    
    <categories>
      
      <category>pytorch基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
