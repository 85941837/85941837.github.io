<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>LaTeX-OCR安装教程</title>
    <link href="/2025/03/04/latexocr%E5%AE%89%E8%A3%85/"/>
    <url>/2025/03/04/latexocr%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<h1 id="准备工作">准备工作</h1><h2 id="安装anaconda">安装Anaconda</h2><p>下载地址: https://www.anaconda.com/download/</p><p>安装过程较为简单，参考网站安装教程即可。</p><p>下载安装完成，右键以管理员身份运行:</p><figure><img src="image-20250304164139802.png" alt="image-20250304164139802"><figcaption aria-hidden="true">image-20250304164139802</figcaption></figure><h2 id="创建环境">创建环境</h2><p>在控制台输入<code>conda create -n latexocr python=3.7</code>,python版本建议大于3.7,本人安装3.9版本</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-comment">// 1. 创建一个名为latexocr的环境</span><br><span class="hljs-symbol">$</span> conda create -n latexocr python=<span class="hljs-number">3.9</span><br></code></pre></td></tr></table></figure><h3 id="激活环境">激活环境</h3><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-comment">// 2. 进入名为latexocr的环境</span><br>conda activate latexocr<br></code></pre></td></tr></table></figure><h3 id="安装pytorch环境">安装pytorch环境</h3><p>地址：https://pytorch.org/</p><p>安装CPU版本即可，复制运行代码<code>pip3 install torch torchvision torchaudio</code>，在控制台输入。</p><figure><img src="image-20250304164652585.png" alt="image-20250304164652585"><figcaption aria-hidden="true">image-20250304164652585</figcaption></figure><h3 id="安装rust程序">安装Rust程序</h3><p>官网链接：https://www.rust-lang.org/zh-CN/</p><p>根据自身配置进行下载。</p><figure><img src="image-20250304165309298.png" alt="image-20250304165309298"><figcaption aria-hidden="true">image-20250304165309298</figcaption></figure><p>运行下载程序(这里比较懒直接搬用原博客的图片)：</p><figure><img src="82bcd2897c2b31e451ebe77fd48de32b.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>上图的1选项要求必须安装C/C++的编译环境，默认是 visualstudio安装器，而此次使用mingw64，因此需要手动修改为<strong>2</strong>，然后输入 <strong>y</strong>，如下图：</p><figure><img src="054e5351e141ca73a10d1c9b23fbbe56.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>继续输入 <strong>2</strong></p><figure><img src="8ece7f6511005a375ef0709e41d6bb3e.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>输入 <strong>x86_64-pc-windows-gnu</strong> 表示安装64位的gnu版本</p><figure><img src="27909c21a322d238e8f5674274c61dc8.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>接下来都是<strong>回车</strong>，使用默认配置</p><figure><img src="3424d624e2c7fc367e3e6d492b9454df.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>上面配置完毕后，到最后一步还是回车，然后开始安装</p><figure><img src="47d165aa490371b69a80570db9065d68.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>检查是否安装成功</p><figure><img src="c6ce3b3f8844a7ae1e608091f8f94088.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h3 id="安装latex-ocr">安装latex-OCR</h3><p>在控制台输入：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> <span class="hljs-string">&quot;pix2tex[gui]&quot;</span><br></code></pre></td></tr></table></figure><p>权重初始化，控制台输入<code>pix2tex</code>，运行完退出。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">pix2tex<br></code></pre></td></tr></table></figure><p>输入<code>latexocr</code>，即可使用。(注意，numpy版本问题，numpy应选择2.0本一下，否则会出现兼容性问题，)</p><figure><img src="image-20250304171310864.png" alt="image-20250304171310864"><figcaption aria-hidden="true">image-20250304171310864</figcaption></figure><blockquote><p><strong>参考链接：</strong></p><p>rust安装：</p><p>https://blog.csdn.net/xinyingzai/article/details/135459640</p><p>latex安装：</p><p>https://blog.csdn.net/tqlisno1/article/details/108908775</p><p>https://zhuanlan.zhihu.com/p/15911172938</p><p>https://blog.csdn.net/weixin_45423965/article/details/143716379</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>软件安装</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>FLIP:Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction论文精读</title>
    <link href="/2025/03/01/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BBFLIP/"/>
    <url>/2025/03/01/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BBFLIP/</url>
    
    <content type="html"><![CDATA[<p><strong>关键词：</strong>细粒度对齐、预训练语言模型、CTR预测、推荐系统</p><p><strong>论文地址：</strong>https://arxiv.org/pdf/2310.19453</p><p><strong>代码链接：</strong>https://github.com/justarter/FLIP</p><p><strong>发表会议：</strong>RecSys 24</p><h1 id="摘要">摘要</h1><p>​文章中提到传统<strong>基于ID嵌入模型</strong>和<strong>预训练语言模型PretrainedLanguage Models,PLMs）</strong>。传统基于ID的点击率模型以one-hot特征编码作为输入，通过特征交互建模获取协同信号；预训练语言模型通过hardprompt模版获得文本情态句子作为输入，利用PLMs提取语义知识，但plm在捕获领域协作信号和区分具有细微文本差异的特征方面面临挑战。文章利用这两种范式的优点，设计一种基于ID和预训练语言模型(FLIP)，该方法将以上两种方法融合，让未被mask的模态数据帮助重构被mask的模态的数据，同时，文章还提出通过自适应组合基于id的模型和PLM的输出，对两者进行联合微调。</p><h1 id="引言">引言</h1><p>​传统基于ID的CTR预测采用one-hot编码将输入数据转换为ID特征，这种方式丢失文本特征中包含的语义信息，无法捕获特征中语义的相关性，此外基于ID的模型依赖用户的交互。PLM擅长理解文本特征和上线文的含义，利用其知识推理能力在稀疏交互性下也具有稳定性能。但PLM也具有局限性，它难以理解领域协作信号，因为它们的输入数据表述为文本句子，。此外，PLM无法识别不同特征描述之间的细微差距(如就电影而言，“房间”和“房间”是两个相似的电影)</p><figure><img src="image-20250301135056896.png" alt="image-20250301135056896"><figcaption aria-hidden="true">image-20250301135056896</figcaption></figure><p>​(三个跨模态预训练任务，a通过对比学习提供粗粒度的实例对齐，bc通过联合掩模态实现细粒度特征级对齐)</p><p>​</p><h1 id="正文">正文</h1><h2 id="flip概述">FLIP概述</h2><p>​FLIP包含三个阶段：<strong>模态转换、模态对齐训练和自适应微调</strong>。首先，FLIP将原始数据从表格形式转换为文本形式。然后，在模态对齐预训练中，采用联合屏蔽语言/表格建模任务来学习细粒度的模态对齐。最后，提出了一种简单而有效的自适应微调策略，以进一步提高CTR预测的性能。</p><figure><img src="image-20250301164357139.png" alt="image-20250301164357139"><figcaption aria-hidden="true">image-20250301164357139</figcaption></figure><h2 id="模态转化">模态转化</h2><p>​标准plm采用单词序列作为输入[13,57]。模态转换的目的是通过硬提示模板将表格数据<span class="math inline">\(x^{tab}_{i}\)</span>转换为文本数据<span class="math inline">\(x^{text}_{i}\)</span></p><figure><img src="image-20250301165814179.png" alt="image-20250301165814179"><figcaption aria-hidden="true">image-20250301165814179</figcaption></figure><p>​ 其中<span class="math inline">\(m_{f}\)</span>表示第<span class="math inline">\(f\)</span>个字段的名称(例如：性别)，<span class="math inline">\(v_{i,f}\)</span>表示输入的变量<span class="math inline">\(x^{tab}_{i}\)</span>为𝑓-th字段的特征值。(例如：女)，⊕表示连接操作符。2给出了一个说明性示例。</p><h2 id="模态对齐预训练">模态对齐预训练</h2><p>​ 模态对齐训练包括mask数据生产、数据编码、mask语言模型建模、masktabular模型建模（这里的tabular模型就是传统基于ID的协同模型）和实例级对比学习</p><p>​ 如图2（阶段2）所示，在从相同的原始输入中获得成对的文本文本数据(<span class="math inline">\(x^{text}_{i}\)</span>，<span class="math inline">\(x^{tab}_{i}\)</span>)后，我们首先执行字段级数据屏蔽以获得输入对的损坏版本，（<span class="math inline">\(\hat{x}^{text}_{i}\)</span>，<span class="math inline">\(\hat{x}^{tab}_{i}\)</span>）。然后，利用PLM -PLM和基于ID的模型- ID对输入对进行编码，得到密度表示(<span class="math inline">\(w_{i}\)</span>，<span class="math inline">\(\hat{w}_{i}\)</span>)和(<span class="math inline">\(v_{i}\)</span>，<span class="math inline">\(\hat{v}_{i}\)</span>)分别用于文本模式和表格模式。接下来，我们应用三个不同的预训练目标来实现plm和基于id的模型之间的特征级和实例级对齐：</p><figure><img src="image-20250301171104199.png" alt="image-20250301171104199"><figcaption aria-hidden="true">image-20250301171104199</figcaption></figure><h2 id="字段级数据屏蔽">字段级数据屏蔽</h2><p>​举例：假设来自职业字段的句子标记为[“occupation”，“is”，“college”，“student”]，段级屏蔽的结果应该是["occupation ", " is ", [MASK],[MASK]]。但令牌级屏蔽的结果可能是[[MASK]，“is”，“college”，“student”]或[“occupation”，“is”，“college”，[MASK]]。</p><p>​ 对于table数据，采用一定比例的<span class="math inline">\(r_{tab}\)</span>字段，用一个额外的<MASK>特征替换对应的特征，<MASK>特性不是特定于字段的，而是由所有特性字段共享，屏蔽字段的索引集表示为<span class="math inline">\(I^{tab}\)</span>。</MASK></MASK></p><p>​ 字段级屏蔽之后，得到屏蔽样本(<span class="math inline">\(\hat{x}^{text}_{i}\)</span>，<span class="math inline">\(\hat{x}^{tab}_{i}\)</span>)，文本模态数据的mask和table模态数据的mask比例用两个超参数控制。</p><h2 id="数据编码">数据编码</h2><p>​ PLM模型（ <span class="math inline">\(h_{PLM}\)</span>）和ID-based模型（ <span class="math inline">\(h_{ID}\)</span>）分别对文本模态和table模态的数据进行编码。</p><figure><img src="image-20250301172947758.png" alt="image-20250301172947758"><figcaption aria-hidden="true">image-20250301172947758</figcaption></figure><p>​ <span class="math inline">\(l\)</span>为令牌个数<span class="math inline">\(w^{text}_{i}\)</span>和并且<span class="math inline">\(D_{text}\)</span>为PLM的隐藏大小。<span class="math inline">\(w_{i，1}\)</span>是表示整个文本输入的[CLS]令牌向量。</p><h2 id="掩码数据建模mlm">掩码数据建模(MLM)</h2><p>​ 将文本-表示对<span class="math inline">\((\hat{w}_{i},v_{i})\)</span>作为输入，屏蔽令牌的索引集表示为<span class="math inline">\(I^{text}\)</span>,将mask的token对应的向量和table模态的输出<span class="math inline">\(v_{i}\)</span>concat起来过一个预测层，预测层是个两层的MLP。优化loss是交叉熵。</p><figure><img src="image-20250302135150094.png" alt="image-20250302135150094"><figcaption aria-hidden="true">image-20250302135150094</figcaption></figure><figure><img src="image-20250302135200663.png" alt="image-20250302135200663"><figcaption aria-hidden="true">image-20250302135200663</figcaption></figure><figure><img src="image-20250301171104199.png" alt="image-20250301171104199"><figcaption aria-hidden="true">image-20250301171104199</figcaption></figure><h2 id="mask-tabular模型建模">mask tabular模型建模</h2><p>​ 将文本-表对(<span class="math inline">\(w_i\)</span>，<span class="math inline">\(\hat{v}_{i}\)</span>)作为输入，目标是利用文本模态的输出和mask的table数据重构mask掉的数据，首先：</p><figure><img src="image-20250302135643707.png" alt="image-20250302135643707"><figcaption aria-hidden="true">image-20250302135643707</figcaption></figure><p>​ 其中<span class="math inline">\(Q ∈ R^{D_{tab}×D_{text}}\)</span>是可训练的注意力矩阵，并且<span class="math inline">\(\sqrt{D_{text}}\)</span>是比例因子。</p><p>​ 对于索引为<span class="math inline">\(f∈I^{tab}\)</span>的每个屏蔽特征，使用MLP+softmax计算在特征空间上的分布，所有掩蔽特征都采用交叉熵损失。</p><figure><img src="image-20250302141857004.png" alt="image-20250302141857004"><figcaption aria-hidden="true">image-20250302141857004</figcaption></figure><figure><img src="image-20250302141922497.png" alt="image-20250302141922497"><figcaption aria-hidden="true">image-20250302141922497</figcaption></figure><p>​论文中提到，上述loss需要在全部特征空间上计算softmax，效率比较低，所以采用了噪声对比估计NCE。</p><figure><img src="image-20250302142036019.png" alt="image-20250302142036019"><figcaption aria-hidden="true">image-20250302142036019</figcaption></figure><h2 id="实例级对比学习icl">实例级对比学习（ICL）</h2><p>​MLM和MTM是从特征层面对两个模态进行对齐，ICL的是显式的从样本层面对齐两个模态。方法采用的是对比学习，模板是同一个样本的文本和table两个模态的表征尽可能接近，不同样本的两个模态表征尽可能远离。</p><p>​ 使用[CLS]令牌向量<span class="math inline">\(w_{i，1}\)</span>来表示文本输入<span class="math inline">\(x^{text}_{i}\)</span>，为保持维度的一致性，使用<span class="math inline">\(w_{i,1}\)</span>和表格表示<span class="math inline">\(v_{i}\)</span>投影到𝑑-dimensional向量，即：<span class="math inline">\(z^{text}\)</span>和<span class="math inline">\(z^{tab}_{i}\)</span>，损失函数为：</p><figure><img src="image-20250302142646125.png" alt="image-20250302142646125"><figcaption aria-hidden="true">image-20250302142646125</figcaption></figure><p>​ 其中B为批量大小，<span class="math inline">\(\mathscr{C}\)</span>为温度超参数，相似函数sim（·）用点积表示，最后总损失表示为：</p><figure><img src="image-20250302143652351.png" alt="image-20250302143652351"><figcaption aria-hidden="true">image-20250302143652351</figcaption></figure><h2 id="自适应微调">自适应微调</h2><p>​上述预训练之后，PLM和ID模型学到了细粒度多模态表征，这个阶段主要是在下游ctr任务上进行两个模态的联合微调，以获得更好的效果。</p><p>​FLIP对ID模型（tabular模态）和PLM模型的输出分别一个随机初始化的线性层，以使两个模型输出各自的概率估计<span class="math inline">\(\hat{y}^{id}_{i}\)</span>和<span class="math inline">\(\hat{y}^{PLM}_{i}\)</span>FLIP模型最终的ctr预估概率是两个概率的加权和。</p><figure><img src="image-20250302144102595.png" alt="image-20250302144102595"><figcaption aria-hidden="true">image-20250302144102595</figcaption></figure><figure><img src="image-20250302144650378.png" alt="image-20250302144650378"><figcaption aria-hidden="true">image-20250302144650378</figcaption></figure><figure><img src="image-20250302144655816.png" alt="image-20250302144655816"><figcaption aria-hidden="true">image-20250302144655816</figcaption></figure><p>​ 损失函数为：</p><figure><img src="image-20250302144708792.png" alt="image-20250302144708792"><figcaption aria-hidden="true">image-20250302144708792</figcaption></figure><h2 id="实验">实验</h2><p>​ 数据集：MovieLens-1M、BookCrossing、GoodReads</p><figure><img src="image-20250302145516653.png" alt="image-20250302145516653"><figcaption aria-hidden="true">image-20250302145516653</figcaption></figure><p>​ 评价指标为：AUC （ROC曲线下面积）和Logloss</p><p>​基线算法：1)基于ID的模型：AFM[79]、PNN[55]、Wide&amp;Deep[9]、DCN[73]、DeepFM[20]、xDeepFM[36]、AFN[10]、AutoInt[65]和DCNv2[74]；(2)基于PLM的模型：CTRL- bert[50]、P5[18]和PTab[43]；(3)基于ID模型和PLM相结合的IDPLM模型：CTRL[35]、MoRec[81]。</p><p>​实现细节：文本模态和table模态的mask比例均为15%，PLM模型使用TinyBERT，ID模型使用DCNv2。</p><figure><img src="image-20250302145839507.png" alt="image-20250302145839507"><figcaption aria-hidden="true">image-20250302145839507</figcaption></figure><p>​ 不同plm的兼容性。基于id的模型固定为DCNv2。</p><figure><img src="image-20250302150020707.png" alt="image-20250302150020707"><figcaption aria-hidden="true">image-20250302150020707</figcaption></figure><p>消融实验:</p><figure><img src="image-20250302150043721.png" alt="image-20250302150043721"><figcaption aria-hidden="true">image-20250302150043721</figcaption></figure><p>mask率和对比学习温度系数的实验：</p><figure><img src="image-20250302150123211.png" alt="image-20250302150123211"><figcaption aria-hidden="true">image-20250302150123211</figcaption></figure><figure><img src="image-20250302150150199.png" alt="image-20250302150150199"><figcaption aria-hidden="true">image-20250302150150199</figcaption></figure><p>​ 级的对齐。</p><p>​ 使用基于PLM和id的模型将它们编码成归一化样本表示<span class="math inline">\(\{z^{text}_{i,(f)},z^{tab}_{i,(f)}\}^{F}_{f=1}\)</span>计算每个跨模态表示对的相互相似性分数（通过点积测量），并将热图可视化.</p><figure><img src="image-20250302150808092.png" alt="image-20250302150808092"><figcaption aria-hidden="true">image-20250302150808092</figcaption></figure><p>不同模型变体在MovieLens-1M上学习的特征ID嵌入的可视化。使用奇异值分解将特征嵌入矩阵投影到二维数据中:</p><figure><img src="image-20250302151304378.png" alt="image-20250302151304378"><figcaption aria-hidden="true">image-20250302151304378</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>推荐系统 大语言模型</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>优化算法</title>
    <link href="/2024/12/05/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    <url>/2024/12/05/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="局部最小值">局部最小值</h2><p>对任意目标函数<span class="math inline">\(f(x)\)</span>，如果在<span class="math inline">\(x\)</span>处对应的<span class="math inline">\(f(x)\)</span>的最小值小于<span class="math inline">\(x\)</span>附近任意点的<span class="math inline">\(f(x)\)</span>值，那么该点是局部最小值。如果<span class="math inline">\(f(x)\)</span>在<span class="math inline">\(x\)</span>处的值是整个域中目标函数的小值，那么该点为<span class="math inline">\(f(x)\)</span>全局最小值。 <span class="math display">\[f(x) = x \cdot \text{cos}(\pi x) \text{ for } -1.0 \leq x \leq 2.0,\]</span></p><p><img src="image-20241205142605577.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>注意力机制</title>
    <link href="/2024/11/23/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2024/11/23/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="注意力机制的基本思想">注意力机制的基本思想</h2><p>2007年美国心理学之父提出非自主性提示和自主性提示的概念。<strong>非自主性提示</strong>是基于环境中物体的突出性和易见性。所有纸制品都是黑白印刷的，但咖啡杯是红色的。换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的，不由自主地引起人们的注意。所以我们会把视力最敏锐的地方放到咖啡上(图1)，喝咖啡后，我们会变得兴奋并想读书，所以转过头，重新聚焦眼睛，然后看看书(图二)。</p><p>在注意力机制中，自主性提示被称为<em>查询</em>（query），而非自主性提示（客观存在的咖啡杯和书本）作为<em>键</em>（key）与感官输入（sensoryinputs）的<em>值</em>（value）构成一组 pair作为输入。而给定任何查询，注意力机制通过<em>注意力汇聚</em>（attentionpooling）将非自主性提示的 key 引导至感官输入。</p><p>注意力机制通过注意力汇聚将<em>查询</em>（自主性提示）和<em>键</em>（非自主性提示）结合在一起，实现对<em>值</em>（感官输入）的<strong>选择倾向</strong>，这就是与CNN 等模型的关键区别。</p><figure><img src="image-20241123195041184.png" alt="图1"><figcaption aria-hidden="true">图1</figcaption></figure><figure><img src="image-20241123195116755.png" alt="图2"><figcaption aria-hidden="true">图2</figcaption></figure><h2 id="注意力机制背景">注意力机制背景</h2><h3 id="非参注意力池化层">非参注意力池化层</h3><p>给定数据<span class="math inline">\((x_{i},y_{i})\)</span>，<span class="math inline">\(i = 1，...，n\)</span>，平均池化层简单方案为$f(x)=<em>{i}^{}y</em>{i} $</p><p>更好的方案为60年代提出的Nadaraya-Watson核回归 <span class="math display">\[f(x)=\sum_{i = 1}^{n}\frac{K(x-x_{i})}{ {\textstyle \sum_{j =1}^{n}K(x-x_{j})} } y_{i}\]</span> 如果<span class="math inline">\(K\)</span>为高斯核，定义为：<span class="math display">\[K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).\]</span> 带入可得到： <span class="math display">\[\begin{split}\begin{aligned} f(x) &amp;=\sum_{i=1}^n \alpha(x, x_i)y_i\\ &amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x -x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)}y_i \\&amp;= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x -x_i)^2\right) y_i. \end{aligned}\end{split}\]</span></p><h3 id="参数化注意力机制">参数化注意力机制</h3><p>将<span class="math inline">\(w\)</span>作为可学习参数 <span class="math display">\[\begin{split}\begin{aligned}f(x) &amp;= \sum_{i=1}^n \alpha(x, x_i) y_i\\&amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x -x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x -x_j)w)^2\right)} y_i \\&amp;= \sum_{i=1}^n\mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right)y_i.\end{aligned}\end{split}\]</span></p><h2 id="注意力评分函数">注意力评分函数</h2><p><img src="image-20241125104952915.png"></p><p>假设有一个查询<span class="math inline">\(q \in \mathbb{R}^q\)</span>和<span class="math inline">\(m\)</span>个"键-值"对<span class="math inline">\((\mathbf{k}_1, \mathbf{v}_1), \ldots,(\mathbf{k}_m, \mathbf{v}_m)，\)</span>其中<span class="math inline">\(\mathbf{k}_i \in \mathbb{R}^k,\mathbf{v}_i \in\mathbb{R}^v\)</span>。注意力汇聚函数<span class="math inline">\(f\)</span>就被表示成值的加权求和： <span class="math display">\[f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m,\mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i)\mathbf{v}_i \in \mathbb{R}^v,\]</span> 其中查询<span class="math inline">\(q\)</span>和键<span class="math inline">\(k_{i}\)</span>的注意力权重(标量)是通过注意力评分函数<span class="math inline">\(a\)</span>将两个向量映射成标量，在通过softmax运算得到的:<span class="math display">\[\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q},\mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m\exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}.\]</span></p><h3 id="隐蔽softmax函数">隐蔽softmax函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">masked_softmax</span>(<span class="hljs-params">X, valid_lens</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># X:3D张量，valid_lens:1D或2D张量</span><br>    <span class="hljs-keyword">if</span> valid_lens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> nn.functional.softmax(X, dim=-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">else</span>:<br>        shape = X.shape<br>        <span class="hljs-keyword">if</span> valid_lens.dim() == <span class="hljs-number">1</span>:<br>            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">else</span>:<br>            valid_lens = valid_lens.reshape(-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span><br>        X = d2l.sequence_mask(X.reshape(-<span class="hljs-number">1</span>, shape[-<span class="hljs-number">1</span>]), valid_lens,<br>                              value=-<span class="hljs-number">1e6</span>)<br>        <span class="hljs-keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">masked_softmax(torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>), torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]))<br></code></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[<span class="hljs-number">0.5980</span>, <span class="hljs-number">0.4020</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.5548</span>, <span class="hljs-number">0.4452</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>]],<br><br>        [[<span class="hljs-number">0.3716</span>, <span class="hljs-number">0.3926</span>, <span class="hljs-number">0.2358</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.3455</span>, <span class="hljs-number">0.3337</span>, <span class="hljs-number">0.3208</span>, <span class="hljs-number">0.0000</span>]]])<br></code></pre></td></tr></table></figure><h3 id="加性注意力">加性注意力</h3><p>给定查询<span class="math inline">\(\mathbf{q} \in\mathbb{R}^q\)</span>和键<span class="math inline">\(\mathbf{k} \in\mathbb{R}^k\)</span>，加性注意力的评分函数为： <span class="math display">\[a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbfW_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AdditiveAttention</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(AdditiveAttention, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.w_v = nn.Linear(num_hiddens, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, queries, keys, values, valid_lens</span>):<br>        queries, keys = <span class="hljs-variable language_">self</span>.W_q(queries), <span class="hljs-variable language_">self</span>.W_k(keys)<br>        <span class="hljs-comment"># 在维度扩展后，</span><br>        <span class="hljs-comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span><br>        <span class="hljs-comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span><br>        <span class="hljs-comment"># 使用广播方式进行求和</span><br>        features = queries.unsqueeze(<span class="hljs-number">2</span>) + keys.unsqueeze(<span class="hljs-number">1</span>)<br>        features = torch.tanh(features)<br>        <span class="hljs-comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span><br>        <span class="hljs-comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span><br>        scores = <span class="hljs-variable language_">self</span>.w_v(features).squeeze(-<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)<br>        <span class="hljs-comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span><br>        <span class="hljs-keyword">return</span> torch.bmm(<span class="hljs-variable language_">self</span>.dropout(<span class="hljs-variable language_">self</span>.attention_weights), values)<br></code></pre></td></tr></table></figure><h3 id="缩放点积注意力">缩放点积注意力</h3><p><span class="math display">\[a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}.\]</span></p><p>在实践中，通常使用小批量的角度考虑提高效率，例如基于<span class="math inline">\(n\)</span>个查询和<span class="math inline">\(m\)</span>个键—值对计算注意力，其中查询和键的长度为<span class="math inline">\(d\)</span>，值的长度为<span class="math inline">\(v\)</span>。 <span class="math display">\[\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right)\mathbf V \in \mathbb{R}^{n\times v}.\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DotProductAttention</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dropout, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(DotProductAttention, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br><br>    <span class="hljs-comment"># queries的形状：(batch_size，查询的个数，d)</span><br>    <span class="hljs-comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span><br>    <span class="hljs-comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span><br>    <span class="hljs-comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, queries, keys, values, valid_lens=<span class="hljs-literal">None</span></span>):<br>        d = queries.shape[-<span class="hljs-number">1</span>]<br>        <span class="hljs-comment"># 设置transpose_b=True为了交换keys的最后两个维度</span><br>        scores = torch.bmm(queries, keys.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)) / math.sqrt(d)<br>        <span class="hljs-variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)<br>        <span class="hljs-keyword">return</span> torch.bmm(<span class="hljs-variable language_">self</span>.dropout(<span class="hljs-variable language_">self</span>.attention_weights), values)<br></code></pre></td></tr></table></figure><h2 id="自注意力">自注意力</h2><p><img src="image-20241130211407115.png"></p><p>输入一个由词元组成的序列<span class="math inline">\(\mathbf{x}_1,\ldots, \mathbf{x}_n\)</span>，其中任意<span class="math inline">\(\mathbf{x}_i \in \mathbb{R}^d 1 \leq i \leqn\)</span>。该序列的自注意力输出为一个长度相同的序列<span class="math inline">\(\mathbf{y}_1, \ldots,\mathbf{y}_n\)</span>，其中： <span class="math display">\[\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots,(\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d\]</span> 代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">num_hiddens, num_heads = <span class="hljs-number">100</span>, <span class="hljs-number">5</span><br>attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,<br>                                   num_hiddens, num_heads, <span class="hljs-number">0.5</span>)<br>attention.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size, num_queries, valid_lens = <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, torch.tensor([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br>X = torch.ones((batch_size, num_queries, num_hiddens))<br>attention(X, X, X, valid_lens).shape<br></code></pre></td></tr></table></figure><p><img src="image-20241130211512564.png"></p><ul><li><p><strong>k</strong>：窗口大小，每次看到的长度为 k</p></li><li><p><strong>n</strong>：长度</p></li><li><p><strong>d</strong>：dimension，每个 x 的维度（长度）</p></li><li><p><strong>并行度</strong>：每个输出( yi )可以自己并行做运算，因为GPU有大量的并行单元，所以<strong>并行度越高，计算的速度就越快</strong></p></li><li><p><strong>最长路径</strong>：对于最长的那个序列，前面时刻的信息通过神经元传递到后面时刻,对应于计算机视觉中的感受野的概念（每一个神经元的输出对应的图片中的视野）</p></li></ul><h2 id="位置编码">位置编码</h2><p><img src="image-20241130220010393.png"></p><p><img src="image-20241130221008612.png"></p><p>p改变相对位置</p><h2 id="transformer">transformer</h2><p>transformer是由编码器和解码器组成的，基于自注意力叠加而成，源序列和目标序列的嵌入表示加上位置编码，再分别输入到编码器和解码器中。</p><p>Transformer的编码器是由多个相同的层叠加而成的，每个层都有<strong>两个子层</strong>（每个子层都采用了残差连接，并且在残差连接的加法计算之后，都使用了层归一化，因此Transformer 编码器都将输出一个 d 维表示向量）</p><p><img src="image-20241201104921804.png"></p><h3 id="模型理解">模型理解</h3><h4 id="transformer的编码器">transformer的编码器</h4><p>第一个子层是<strong>多头自注意力汇聚</strong>：Transformer块中的多头注意力实际上就是自注意力，在计算编码器的自注意力时，key、value 和 query 的值都来自<strong>前一个编码器层的输出</strong></p><p>第二个子层是<strong>基于位置的前馈网络</strong>：全连接，本质上和编码器-解码器的架构没有本质上的区别，将Transformer 编码器最后一层的输出作为解码器的输入来完成信息的传递</p><h4 id="transformer的解码器">transformer的解码器</h4><p>每层都有<strong>三个子层</strong>，并且在每个子层中也使用了<strong>残差连接</strong>和<strong>层归一化</strong></p><ul><li>在解码器自注意力中，key 、value 和 query都来自<strong>上一个解码器层的输出</strong></li><li>解码器中的每个位置只能考虑该位置之前的所有位置</li><li>带掩码的自注意力保留了自回归的属性，确保预测仅仅依赖于已生成的输出词元</li></ul><p>第二个子层是<strong>编码器-解码器注意力</strong></p><ul><li>除了编码器中所描述的两个子层之外，解码器还在这两个子层之间插入了编码器-解码器注意力层，作为第三个子层，它的<strong>query 来自上一个解码器层的输出，key 和 value来自整个编码器的输出</strong></li></ul><p>第三个子层是<strong>基于位置的前馈网络</strong></p><h4 id="多头注意力multi-head-attention">多头注意力（Multi-headattention）</h4><p>对<strong>同一个 key 、value 、query</strong> 抽取不同的信息</p><p>多头注意力使用 h个独立的注意力池化，合并各个头（head）输出得到最终输出</p><p><img src="image-20241201110628069.png"></p><p><img src="image-20241201111041318.png"></p><p><img src="image-20241201111112756.png"></p><h4 id="带掩码的多头注意力masked-multi-head-attention">带掩码的多头注意力（MaskedMulti-head attention）</h4><p>解码器对序列中一个元素输出时，不应该考虑该元素之后的元素</p><ul><li><p><strong>注意力中是没有时间信息的</strong>，在输出中间第 i个信息的时候，也能够看到后面的所有信息，这在编码的时候是可以的，但是在解码的时候是不行的，在解码的时候不应该考虑该元素本身或者该元素之后的元素</p></li><li><p>可以通过掩码来实现，也就是计算 xi 输出时，假装当前序列长度为i</p></li></ul><h4 id="基于位置的前馈网络positionwise-ffn">基于位置的前馈网络（PositionwiseFFN）</h4><ul><li>基于位置的前馈网络<strong>对序列中的所有位置的表示进行变换时</strong>使用的是<strong>同一个多层感知机（MLP）</strong>，这就是称前馈网络是基于位置的原因</li></ul><p>其实就是全连接层，将输入形状由（b，n，d）变成（bn，d），然后作用两个全连接层，最后输出形状由（bn，d）变回（b，n，d），等价于两层核窗口为1 的一维卷积层</p><ul><li>b：batchsize</li><li>n：序列长度</li><li>d：dimension</li></ul><p>在做卷积的时候是将 n 和 d 合成一维，变成 nd ；但是现在 n是序列的长度，会变化，要使模型能够处理任意的特征，所以不能将 n作为一个特征，因此对每个序列中的每个元素作用一个全连接（将每个序列中的xi 当作是一个样本）</p><h4 id="残差连接和归一化add-norm">残差连接和归一化（Add &amp;norm）</h4><p>加入归一化能够更好地训练比较深的网络，但是这里不能使用批量归一化，<strong>批量归一化对每个特征/通道里元素进行归一化</strong></p><p>在做 NLP 的时候，如果选择将 d 作为特征的话，那么批量归一化的输入是n*b ，b 是批量大小，n是序列长度，<strong>序列的长度是会变的</strong>，所以每次做批量归一化的输入大小都不同，所以会导致不稳定，训练和预测的长度本来就不一样，预测的长度会慢慢变长，所以批量归一化不适合长度会变的NLP 应用</p><p><img src="image-20241201112613764.png"></p><ul><li><p>层归一化和批量归一化的目标相同，但是层归一化是基于特征维度进行归一化的</p></li><li><p>层归一化和批量归一化的区别在于：批量归一化在 d的维度上找出一个矩阵，将其均值变成 0 ，方差变成1，层归一化每次选的是一个元素，也就是每个 batch里面的一个样本进行归一化</p></li><li><p>尽管批量归一化在计算机视觉中被广泛应用，但是在自然语言处理任务中，批量归一化通常不如层归一化的效果好，因为<strong>在自然语言处理任务中，输入序列的长度通常是变化的</strong></p></li><li><p>然在做层归一化的时候，长度也是变化的，但是至少来说还是在一个<strong>单样本</strong>中，不管批量多少，都给定一个特征，这样对于变化的长度来讲，稍微稳定一点，不会因为长度变化，导致稳定性发生很大的变化</p></li></ul><h4 id="信息传递">信息传递</h4><p>假设编码器中的输出是 y1，... ，yn ，将其作为解码中第 i 个 Transformer块中多头注意力的 key 和 value</p><p>它是普通的注意力（它的 key 和 value 来自编码器的输出， query来自目标序列）</p><h4 id="预测">预测</h4><p><img src="2ec22a2ff26e0cc5d8e855899e79f8c12a4fcbd1.png@682w_!web-note.webp"></p><p>预测第 t+1 个输出时，解码器中输入前 t 个预测值</p><ul><li>在自注意力中，前 t 个预测值作为 key 和 value ，第 t 个预测值还作为query</li><li>关于序列到序列模型，在训练阶段，输出序列的所有位置（时间步）的词元都是已知的；但是在预测阶段，输出序列的次元是逐个生成的</li></ul><h3 id="代码">代码</h3><h4 id="基于位置的前馈网络">基于位置的前馈网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionWiseFFN</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span><br><span class="hljs-params">                 **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(PositionWiseFFN, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.dense2(<span class="hljs-variable language_">self</span>.relu(<span class="hljs-variable language_">self</span>.dense1(X)))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化参数</span><br>ffn = PositionWiseFFN(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>)<br>ffn.<span class="hljs-built_in">eval</span>()<br><span class="hljs-comment"># 根据计算，最后输出的shape是(2，3，8)</span><br>ffn(torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)))[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[-<span class="hljs-number">0.8290</span>,  <span class="hljs-number">1.0067</span>,  <span class="hljs-number">0.3619</span>,  <span class="hljs-number">0.3594</span>, -<span class="hljs-number">0.5328</span>,  <span class="hljs-number">0.2712</span>,  <span class="hljs-number">0.7394</span>,  <span class="hljs-number">0.0747</span>],<br>        [-<span class="hljs-number">0.8290</span>,  <span class="hljs-number">1.0067</span>,  <span class="hljs-number">0.3619</span>,  <span class="hljs-number">0.3594</span>, -<span class="hljs-number">0.5328</span>,  <span class="hljs-number">0.2712</span>,  <span class="hljs-number">0.7394</span>,  <span class="hljs-number">0.0747</span>],<br>        [-<span class="hljs-number">0.8290</span>,  <span class="hljs-number">1.0067</span>,  <span class="hljs-number">0.3619</span>,  <span class="hljs-number">0.3594</span>, -<span class="hljs-number">0.5328</span>,  <span class="hljs-number">0.2712</span>,  <span class="hljs-number">0.7394</span>,  <span class="hljs-number">0.0747</span>]],<br>       grad_fn=&lt;SelectBackward0&gt;)<br></code></pre></td></tr></table></figure><h4 id="残差连接和层规范化">残差连接和层规范化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">ln = nn.LayerNorm(<span class="hljs-number">2</span>)<br>bn = nn.BatchNorm1d(<span class="hljs-number">2</span>)<br>X = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]], dtype=torch.float32)<br><span class="hljs-comment"># 在训练模式下计算X的均值和方差</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;layer norm:&#x27;</span>, ln(X), <span class="hljs-string">&#x27;\nbatch norm:&#x27;</span>, bn(X))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">layer norm: tensor([[-<span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.0000</span>],<br>        [-<span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.0000</span>]], grad_fn=&lt;NativeLayerNormBackward0&gt;)<br>batch norm: tensor([[-<span class="hljs-number">1.0000</span>, -<span class="hljs-number">1.0000</span>],<br>        [ <span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.0000</span>]], grad_fn=&lt;NativeBatchNormBackward0&gt;)<br></code></pre></td></tr></table></figure><p>现在可以使用残差连接和层规范化来实现<code>AddNorm</code>类。暂退法也被作为正则化方法使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AddNorm</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, normalized_shape, dropout, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(AddNorm, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.ln = nn.LayerNorm(normalized_shape)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, Y</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.ln(<span class="hljs-variable language_">self</span>.dropout(Y) + X)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">add_norm = AddNorm([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], <span class="hljs-number">0.5</span>)<br>add_norm.<span class="hljs-built_in">eval</span>()<br>add_norm(torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))).shape<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torch</span>.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><h4 id="编码器">编码器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderBlock</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Transformer编码器块&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, key_size, query_size, value_size, num_hiddens,</span><br><span class="hljs-params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="hljs-params">                 dropout, use_bias=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(EncoderBlock, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.attention = d2l.MultiHeadAttention(<br>            key_size, query_size, value_size, num_hiddens, num_heads, dropout,<br>            use_bias)<br>        <span class="hljs-variable language_">self</span>.addnorm1 = AddNorm(norm_shape, dropout)<br>        <span class="hljs-variable language_">self</span>.ffn = PositionWiseFFN(<br>            ffn_num_input, ffn_num_hiddens, num_hiddens)<br>        <span class="hljs-variable language_">self</span>.addnorm2 = AddNorm(norm_shape, dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, valid_lens</span>):<br>        Y = <span class="hljs-variable language_">self</span>.addnorm1(X, <span class="hljs-variable language_">self</span>.attention(X, X, X, valid_lens))<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.addnorm2(Y, <span class="hljs-variable language_">self</span>.ffn(Y))<br></code></pre></td></tr></table></figure><p>正如从代码中所看到的，Transformer编码器中的任何层都不会改变其输入的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">100</span>, <span class="hljs-number">24</span>))<br>valid_lens = torch.tensor([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br>encoder_blk = EncoderBlock(<span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, [<span class="hljs-number">100</span>, <span class="hljs-number">24</span>], <span class="hljs-number">24</span>, <span class="hljs-number">48</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0.5</span>)<br>encoder_blk.<span class="hljs-built_in">eval</span>()<br>encoder_blk(X, valid_lens).shape<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torch</span>.Size([<span class="hljs-number">2</span>, <span class="hljs-number">100</span>, <span class="hljs-number">24</span>])<br></code></pre></td></tr></table></figure><p>Transformer编码器的代码中，堆叠了<code>num_layers</code>个<code>EncoderBlock</code>类的实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerEncoder</span>(d2l.Encoder):<br>    <span class="hljs-string">&quot;&quot;&quot;Transformer编码器&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, key_size, query_size, value_size,</span><br><span class="hljs-params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="hljs-params">                 num_heads, num_layers, dropout, use_bias=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(TransformerEncoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.num_hiddens = num_hiddens<br>        <span class="hljs-variable language_">self</span>.embedding = nn.Embedding(vocab_size, num_hiddens)<br>        <span class="hljs-variable language_">self</span>.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)<br>        <span class="hljs-variable language_">self</span>.blks = nn.Sequential()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>            <span class="hljs-variable language_">self</span>.blks.add_module(<span class="hljs-string">&quot;block&quot;</span>+<span class="hljs-built_in">str</span>(i),<br>                EncoderBlock(key_size, query_size, value_size, num_hiddens,<br>                             norm_shape, ffn_num_input, ffn_num_hiddens,<br>                             num_heads, dropout, use_bias))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, valid_lens, *args</span>):<br>        <span class="hljs-comment"># 因为位置编码值在-1和1之间，</span><br>        <span class="hljs-comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span><br>        <span class="hljs-comment"># 然后再与位置编码相加。</span><br>        X = <span class="hljs-variable language_">self</span>.pos_encoding(<span class="hljs-variable language_">self</span>.embedding(X) * math.sqrt(<span class="hljs-variable language_">self</span>.num_hiddens))<br>        <span class="hljs-variable language_">self</span>.attention_weights = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.blks)<br>        <span class="hljs-keyword">for</span> i, blk <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.blks):<br>            X = blk(X, valid_lens)<br>            <span class="hljs-variable language_">self</span>.attention_weights[<br>                i] = blk.attention.attention.attention_weights<br>        <span class="hljs-keyword">return</span> X<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">encoder = TransformerEncoder(<br>    <span class="hljs-number">200</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, [<span class="hljs-number">100</span>, <span class="hljs-number">24</span>], <span class="hljs-number">24</span>, <span class="hljs-number">48</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.5</span>)<br>encoder.<span class="hljs-built_in">eval</span>()<br>encoder(torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">100</span>), dtype=torch.long), valid_lens).shape<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torch</span>.Size([<span class="hljs-number">2</span>, <span class="hljs-number">100</span>, <span class="hljs-number">24</span>])<br></code></pre></td></tr></table></figure><h4 id="解码器">解码器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderBlock</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, key_size, query_size, value_size, num_hiddens,</span><br><span class="hljs-params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="hljs-params">                 dropout, i, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(DecoderBlock, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.i = i<br>        <span class="hljs-comment"># 多投注意力</span><br>        <span class="hljs-variable language_">self</span>.attention1 = d2l.MultiHeadAttention(<br>            key_size, query_size, value_size, num_hiddens, num_heads, dropout)<br>        <span class="hljs-variable language_">self</span>.addnorm1 = AddNorm(norm_shape, dropout)<br>        <span class="hljs-variable language_">self</span>.attention2 = d2l.MultiHeadAttention(<br>            key_size, query_size, value_size, num_hiddens, num_heads, dropout)<br>        <span class="hljs-variable language_">self</span>.addnorm2 = AddNorm(norm_shape, dropout)<br>        <span class="hljs-comment"># 前馈神经网络</span><br>        <span class="hljs-variable language_">self</span>.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,<br>                                   num_hiddens)<br>        <span class="hljs-variable language_">self</span>.addnorm3 = AddNorm(norm_shape, dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, state</span>):<br>        enc_outputs, enc_valid_lens = state[<span class="hljs-number">0</span>], state[<span class="hljs-number">1</span>]<br>        <span class="hljs-comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span><br>        <span class="hljs-comment"># 因此state[2][self.i]初始化为None。</span><br>        <span class="hljs-comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span><br>        <span class="hljs-comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span><br>        <span class="hljs-keyword">if</span> state[<span class="hljs-number">2</span>][<span class="hljs-variable language_">self</span>.i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            key_values = X<br>        <span class="hljs-keyword">else</span>:<br>            key_values = torch.cat((state[<span class="hljs-number">2</span>][<span class="hljs-variable language_">self</span>.i], X), axis=<span class="hljs-number">1</span>)<br>        state[<span class="hljs-number">2</span>][<span class="hljs-variable language_">self</span>.i] = key_values<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training:<br>            batch_size, num_steps, _ = X.shape<br>            <span class="hljs-comment"># dec_valid_lens的开头:(batch_size,num_steps),</span><br>            <span class="hljs-comment"># 其中每一行是[1,2,...,num_steps]</span><br>            dec_valid_lens = torch.arange(<br>                <span class="hljs-number">1</span>, num_steps + <span class="hljs-number">1</span>, device=X.device).repeat(batch_size, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:<br>            dec_valid_lens = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># 自注意力</span><br>        X2 = <span class="hljs-variable language_">self</span>.attention1(X, key_values, key_values, dec_valid_lens)<br>        Y = <span class="hljs-variable language_">self</span>.addnorm1(X, X2)<br>        <span class="hljs-comment"># 编码器－解码器注意力。</span><br>        <span class="hljs-comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span><br>        Y2 = <span class="hljs-variable language_">self</span>.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)<br>        Z = <span class="hljs-variable language_">self</span>.addnorm2(Y, Y2)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.addnorm3(Z, <span class="hljs-variable language_">self</span>.ffn(Z)), state<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">decoder_blk = DecoderBlock(<span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, [<span class="hljs-number">100</span>, <span class="hljs-number">24</span>], <span class="hljs-number">24</span>, <span class="hljs-number">48</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0</span>)<br>decoder_blk.<span class="hljs-built_in">eval</span>()<br>X = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">100</span>, <span class="hljs-number">24</span>))<br>state = [encoder_blk(X, valid_lens), valid_lens, [<span class="hljs-literal">None</span>]]<br>decoder_blk(X, state)[<span class="hljs-number">0</span>].shape<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerDecoder</span>(d2l.AttentionDecoder):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, key_size, query_size, value_size,</span><br><span class="hljs-params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="hljs-params">                 num_heads, num_layers, dropout, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(TransformerDecoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.num_hiddens = num_hiddens<br>        <span class="hljs-variable language_">self</span>.num_layers = num_layers<br>        <span class="hljs-variable language_">self</span>.embedding = nn.Embedding(vocab_size, num_hiddens)<br>        <span class="hljs-variable language_">self</span>.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)<br>        <span class="hljs-variable language_">self</span>.blks = nn.Sequential()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>            <span class="hljs-variable language_">self</span>.blks.add_module(<span class="hljs-string">&quot;block&quot;</span>+<span class="hljs-built_in">str</span>(i),<br>                DecoderBlock(key_size, query_size, value_size, num_hiddens,<br>                             norm_shape, ffn_num_input, ffn_num_hiddens,<br>                             num_heads, dropout, i))<br>        <span class="hljs-variable language_">self</span>.dense = nn.Linear(num_hiddens, vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, enc_outputs, enc_valid_lens, *args</span>):<br>        <span class="hljs-keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="hljs-literal">None</span>] * <span class="hljs-variable language_">self</span>.num_layers]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, state</span>):<br>        X = <span class="hljs-variable language_">self</span>.pos_encoding(<span class="hljs-variable language_">self</span>.embedding(X) * math.sqrt(<span class="hljs-variable language_">self</span>.num_hiddens))<br>        <span class="hljs-variable language_">self</span>._attention_weights = [[<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.blks) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> (<span class="hljs-number">2</span>)]<br>        <span class="hljs-keyword">for</span> i, blk <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.blks):<br>            X, state = blk(X, state)<br>            <span class="hljs-comment"># 解码器自注意力权重</span><br>            <span class="hljs-variable language_">self</span>._attention_weights[<span class="hljs-number">0</span>][<br>                i] = blk.attention1.attention.attention_weights<br>            <span class="hljs-comment"># “编码器－解码器”自注意力权重</span><br>            <span class="hljs-variable language_">self</span>._attention_weights[<span class="hljs-number">1</span>][<br>                i] = blk.attention2.attention.attention_weights<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.dense(X), state<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">attention_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._attention_weights<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>循环神经网络</title>
    <link href="/2024/11/14/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/11/14/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p>本内容参考动手学深度学习<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="动手学深度学习">[1]</span></a></sup>。</p><p>在学习循环神经网络<span class="math inline">\((Recurrent NeuralNetwork,RNN)\)</span>之前先了解一下序列模型，序列模型是专门用于处理和预测序列数据的模型。在自然语言处理、音频处理和时间序列处理有广泛的应用。</p><p>举例来说，用<span class="math inline">\(x_{t}\)</span>表示价格，即在时间步<span class="math inline">\(t \in \mathbb{Z}^+\)</span>，观察价格<span class="math inline">\(x_{t}\)</span>，假设交易员想在<span class="math inline">\(t\)</span>日预测股市的价格，可以表现为： <span class="math display">\[x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1).\]</span></p><h2 id="自回归模型">自回归模型</h2><p><strong>自回归模型</strong>：假设在现实情况下<span class="math inline">\(x_{t-1}, \ldots,x_1\)</span>是不必要的，因此只需要满足长度为<span class="math inline">\(\tau\)</span>的时间跨度， 即使用观测序列<span class="math inline">\(x_{t-1}, \ldots,x_{t-\tau}\)</span>。当下获得的最直接的好处就是参数的数量总是不变的，至少在<span class="math inline">\(t &gt;\tau\)</span>是如此，这种模型被称为<strong>自回归模型</strong><span class="math inline">\(（autoregressivemodels）\)</span>，因为它对自己执行回归。</p><p><strong>隐变量自回归模型</strong>：是保留一些对过去观测的总结<span class="math inline">\(h_{t}\)</span>， 并且同时更新预测<span class="math inline">\(\hat{x}_t\)</span>和总结<span class="math inline">\(h_{t}\)</span>。这就产生了基于<span class="math inline">\(\hat{x}_t = P(x_t \mid h_{t})\)</span>估计<span class="math inline">\(x_t\)</span>,以及公式<span class="math inline">\(h_t = g(h_{t-1},x_{t-1})\)</span>更新模型，因为<span class="math inline">\(h_{t}\)</span>从未被观测到，这种模型被称为<strong>隐变量自回归模型</strong><span class="math inline">\(（latent autoregressive models）\)</span></p><h2 id="马尔可夫模型">马尔可夫模型</h2><p>在自回归模型的近似法中，使用<span class="math inline">\(x_{t-1},\ldots, x_{t-\tau}\)</span>来估计<span class="math inline">\(x_{t}\)</span> ，而不是<span class="math inline">\(x_{t-1}, \ldots,x_1\)</span>，只要这种是近似精确的，我们就说满足马尔可夫条件。特殊的，如果<span class="math inline">\(\tau\)</span>&lt;1得到一阶马尔可夫模型<span class="math inline">\(first-order Markov model）\)</span> <span class="math display">\[P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}) \text{ 当 }P(x_1 \mid x_0) = P(x_1).\]</span> 基于条件概率公式，我们可以写出： <span class="math display">\[P(x_1, \ldots, x_T) = \prod_{t=T}^1 P(x_t \mid x_{t+1}, \ldots, x_T).\]</span></p><h2 id="文本预处理">文本预处理</h2><h3 id="读取数据集">读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br>d2l.DATA_HUB[<span class="hljs-string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="hljs-string">&#x27;timemachine.txt&#x27;</span>,<br>                                <span class="hljs-string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_time_machine</span>():  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(d2l.download(<span class="hljs-string">&#x27;time_machine&#x27;</span>), <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()<br>    <span class="hljs-keyword">return</span> [re.sub(<span class="hljs-string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, line).strip().lower() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br><br>lines = read_time_machine()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;# 文本总行数: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(lines)&#125;</span>&#x27;</span>)<br><span class="hljs-built_in">print</span>(lines[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(lines[<span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">Downloading ../data/timemachine.txt <span class="hljs-built_in">from</span> <span class="hljs-keyword">http</span>://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...<br><span class="hljs-comment"># 文本总行数: 3221</span><br><span class="hljs-keyword">the</span> <span class="hljs-built_in">time</span> machine <span class="hljs-keyword">by</span> h g wells<br>twinkled <span class="hljs-keyword">and</span> his usually pale face was flushed <span class="hljs-keyword">and</span> animated <span class="hljs-keyword">the</span><br></code></pre></td></tr></table></figure></blockquote><h3 id="词元化">词元化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">lines, token=<span class="hljs-string">&#x27;word&#x27;</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># &#x27;word&#x27;：按单词拆分（默认值）</span><br>    <span class="hljs-keyword">if</span> token == <span class="hljs-string">&#x27;word&#x27;</span>:<br>        <span class="hljs-keyword">return</span> [line.split() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-comment"># &#x27;char&#x27;：按字符拆分</span><br>    <span class="hljs-keyword">elif</span> token == <span class="hljs-string">&#x27;char&#x27;</span>:<br>        <span class="hljs-keyword">return</span> [<span class="hljs-built_in">list</span>(line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;错误：未知词元类型：&#x27;</span> + token)<br><br>tokens = tokenize(lines)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">11</span>):<br>    <span class="hljs-built_in">print</span>(tokens[i])<br></code></pre></td></tr></table></figure><p>例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 示例文本</span><br>lines = [<br>    <span class="hljs-string">&quot;Hello world&quot;</span>,<br>    <span class="hljs-string">&quot;This is a test.&quot;</span>,<br>    <span class="hljs-string">&quot;Tokenize me!&quot;</span>,<br>    <span class="hljs-string">&quot;How are you?&quot;</span>,<br>    <span class="hljs-string">&quot;I hope this works!&quot;</span>,<br>    <span class="hljs-string">&quot;Let&#x27;s see the output.&quot;</span>,<br>    <span class="hljs-string">&quot;This function is simple.&quot;</span>,<br>    <span class="hljs-string">&quot;Please split by words.&quot;</span>,<br>    <span class="hljs-string">&quot;Test the character tokenization.&quot;</span>,<br>    <span class="hljs-string">&quot;What do you think?&quot;</span>,<br>    <span class="hljs-string">&quot;Enjoy learning!&quot;</span><br>]<br><br><span class="hljs-comment"># 按单词拆分</span><br>tokens = tokenize(lines, token=<span class="hljs-string">&#x27;word&#x27;</span>)<br><br><span class="hljs-comment"># 输出前 11 行拆分结果</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">11</span>):<br>    <span class="hljs-built_in">print</span>(tokens[i])<br><br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><p>['Hello', 'world'] ['This', 'is', 'a', 'test.'] ['Tokenize', 'me!']['How', 'are', 'you?'] ['I', 'hope', 'this', 'works!'] ['Let's', 'see','the', 'output.'] ['This', 'function', 'is', 'simple.'] ['Please','split', 'by', 'words.'] ['Test', 'the', 'character', 'tokenization.']['What', 'do', 'you', 'think?'] ['Enjoy', 'learning!']</p></blockquote><h3 id="词表">词表</h3><p>将字符串类型得词元映射到从0开始的数字索引中。根据词元出现的频率，分配数字索引。语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。同时增加一个列表，用于保存那些被保留的词元，例如：填充词元（“<pad>”）； 序列开始词元（“<bos>”）；序列结束词元（“<eos>”）。</eos></bos></pad></unk></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Vocab</span>:  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokens=<span class="hljs-literal">None</span>, min_freq=<span class="hljs-number">0</span>, reserved_tokens=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># token词元列表</span><br>        <span class="hljs-keyword">if</span> tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            tokens = []<br>        <span class="hljs-comment"># 保留词元列表</span><br>        <span class="hljs-keyword">if</span> reserved_tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            reserved_tokens = []<br>        <span class="hljs-comment"># 按出现频率排序</span><br>        counter = count_corpus(tokens)<br>        <span class="hljs-variable language_">self</span>._token_freqs = <span class="hljs-built_in">sorted</span>(counter.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>],<br>                                   reverse=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 未知词元的索引为0</span><br>        <span class="hljs-variable language_">self</span>.idx_to_token = [<span class="hljs-string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens<br>        <span class="hljs-variable language_">self</span>.token_to_idx = &#123;token: idx<br>                             <span class="hljs-keyword">for</span> idx, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.idx_to_token)&#125;<br>        <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._token_freqs:<br>            <span class="hljs-keyword">if</span> freq &lt; min_freq:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> token <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.token_to_idx:<br>                <span class="hljs-variable language_">self</span>.idx_to_token.append(token)<br>                <span class="hljs-variable language_">self</span>.token_to_idx[token] = <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.idx_to_token) - <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.idx_to_token)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, tokens</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(tokens, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.token_to_idx.get(tokens, <span class="hljs-variable language_">self</span>.unk)<br>        <span class="hljs-keyword">return</span> [<span class="hljs-variable language_">self</span>.__getitem__(token) <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">to_tokens</span>(<span class="hljs-params">self, indices</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(indices, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.idx_to_token[indices]<br>        <span class="hljs-keyword">return</span> [<span class="hljs-variable language_">self</span>.idx_to_token[index] <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> indices]<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unk</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 未知词元的索引为0</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">token_freqs</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._token_freqs<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_corpus</span>(<span class="hljs-params">tokens</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 这里的tokens是1D列表或2D列表</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">isinstance</span>(tokens[<span class="hljs-number">0</span>], <span class="hljs-built_in">list</span>):<br>        <span class="hljs-comment"># 将词元列表展平成一个列表</span><br>        tokens = [token <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">return</span> collections.Counter(tokens)<br></code></pre></td></tr></table></figure><h3 id="整合所有功能">整合所有功能</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_corpus_time_machine</span>(<span class="hljs-params">max_tokens=-<span class="hljs-number">1</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span><br>    lines = read_time_machine()<br>    tokens = tokenize(lines, <span class="hljs-string">&#x27;char&#x27;</span>)<br>    vocab = Vocab(tokens)<br>    <span class="hljs-comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span><br>    <span class="hljs-comment"># 所以将所有文本行展平到一个列表中</span><br>    corpus = [vocab[token] <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">if</span> max_tokens &gt; <span class="hljs-number">0</span>:<br>        corpus = corpus[:max_tokens]<br>    <span class="hljs-keyword">return</span> corpus, vocab<br><br>corpus, vocab = load_corpus_time_machine()<br><span class="hljs-built_in">len</span>(corpus), <span class="hljs-built_in">len</span>(vocab)<br></code></pre></td></tr></table></figure><h2 id="语言模型">语言模型</h2><p>根据上面提到的自回归模型和马尔可夫模型，将语言模型定义为： <span class="math display">\[P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots,x_{t-1}).\]</span> 由此，预测四个单词的文本序列的概率为： <span class="math display">\[P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep})P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep},\text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning},\text{is}).\]</span> 为了训练语言模型，需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。训练中次的出现概率可表示为，其中<span class="math inline">\(n(x)\)</span>和<span class="math inline">\(n(x,x′)\)</span>分别是单个单词和连续单词对的出现次数。<span class="math display">\[\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep,learning})}{n(\text{deep})},\]</span> 通常，连续单词对要比“deeplearning”出现频率要低很多，所以对于不常见的组合，要想找到足够的出现次数来获得估计会非常不容易。三个以上的单词组合，预测的结果会更差。为了缓解这种问题，Wood,F等人<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wood, F., Gasthaus, J., Archambeau, C., James, L., &amp; Teh, Y. W. (2011). The sequence memoizer. *Communications of the ACM*, *54*(2), 91–98.">[2]</span></a></sup>提出如下公式。其中<span class="math inline">\(\epsilon_1,\epsilon_2，\epsilon_3\)</span>为超参数，以<span class="math inline">\(\epsilon_1\)</span>为例：当<span class="math inline">\(\epsilon_1 =0\)</span>时，不应用平滑；当ϵ1接近正无穷大时，<span class="math inline">\(\hat{P}(x)\)</span>接近均匀概率分布1/m。 <span class="math display">\[\begin{split}\begin{aligned}    \hat{P}(x) &amp; = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\    \hat{P}(x&#39; \mid x) &amp; = \frac{n(x, x&#39;) + \epsilon_2\hat{P}(x&#39;)}{n(x) + \epsilon_2}, \\    \hat{P}(x&#39;&#39; \mid x,x&#39;) &amp; = \frac{n(x,x&#39;,x&#39;&#39;) + \epsilon_3 \hat{P}(x&#39;&#39;)}{n(x, x&#39;) +\epsilon_3}.\end{aligned}\end{split}\]</span> 然而，这样的模型很容易变得无效，原因如下：首先，我们需要存储所有的计数； 其次，这完全忽略了单词的意思。例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中，但是想根据上下文调整这类模型其实是相当困难的。最后，长单词序列大部分是没出现过的，因此一个模型如果只是简单地统计先前“看到”的单词序列频率，那么模型面对这种问题肯定是表现不佳的。</p><h3 id="马尔可夫模型与n元语法">马尔可夫模型与n元语法</h3><p>如果<span class="math inline">\(P(x_{t+1} \mid x_t, \ldots, x_1) =P(x_{t+1} \mid x_t)\)</span>， 则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。</p><p><span class="math display">\[\begin{split}\begin{aligned}P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2) P(x_3) P(x_4),\\P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2)P(x_4  \mid  x_3),\\P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1,x_2) P(x_4  \mid  x_2, x_3).\end{aligned}\end{split}\]</span></p><h2 id="循环神经网络">循环神经网络</h2><p><em>循环神经网络</em>（recurrent neural networks，RNNs）是具有隐状态的神经网络。与感知机不同的是，保存了前一个时间步的隐藏变量<span class="math inline">\(\mathbf{H}_{t-1}\)</span>,并引入了一个新的权重参数<span class="math inline">\(\mathbf{W}_{hh} \in \mathbb{R}^{h \timesh}\)</span>当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：</p><p><img src="image-20241116184514051.png" alt="image-20241116184514051"> <span class="math display">\[\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1}\mathbf{W}_{hh}  + \mathbf{b}_h).\]</span></p><p><span class="math display">\[\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.\]</span></p><h3 id="循环神经网络的损失函数困惑度">循环神经网络的损失函数(困惑度)</h3><p><span class="math display">\[\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),\]</span></p><p>其中P表示语言模型的预测概率，<span class="math inline">\(x_{t}\)</span>表示真实值。</p><h3 id="循环神经网络的应用">循环神经网络的应用</h3><figure><img src="image-20241116185023652.png" alt="image-20241116185023652"><figcaption aria-hidden="true">image-20241116185023652</figcaption></figure><h3 id="代码实现手动">代码实现(手动)</h3><h4 id="整体模型架构">整体模型架构</h4><p><img src="%E7%BB%98%E5%9B%BE1.png" alt="绘图1" style="zoom:150%;"></p><h4 id="数据加载">数据加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>batch_size, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">35</span><br>train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)<br></code></pre></td></tr></table></figure><h4 id="初始化模型参数">初始化模型参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_params</span>(<span class="hljs-params">vocab_size, num_hiddens, device</span>):<br>    num_inputs = num_outputs = vocab_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">normal</span>(<span class="hljs-params">shape</span>):<br>        <span class="hljs-keyword">return</span> torch.randn(size=shape, device=device) * <span class="hljs-number">0.01</span><br><br>    <span class="hljs-comment"># 隐藏层参数</span><br>    W_xh = normal((num_inputs, num_hiddens))<br>    W_hh = normal((num_hiddens, num_hiddens))<br>    b_h = torch.zeros(num_hiddens, device=device)<br>    <span class="hljs-comment"># 输出层参数</span><br>    W_hq = normal((num_hiddens, num_outputs))<br>    b_q = torch.zeros(num_outputs, device=device)<br>    <span class="hljs-comment"># 附加梯度</span><br>    params = [W_xh, W_hh, b_h, W_hq, b_q]<br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>        param.requires_grad_(<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> params<br></code></pre></td></tr></table></figure><h4 id="在一个时间步内计算隐状态和输出">在一个时间步内计算隐状态和输出</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">rnn</span>(<span class="hljs-params">inputs, state, params</span>):<br>    <span class="hljs-comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span><br>    W_xh, W_hh, b_h, W_hq, b_q = params<br>    H, = state<br>    outputs = []<br>    <span class="hljs-comment"># X的形状：(批量大小，词表大小)</span><br>    <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> inputs:<br>        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)<br>        Y = torch.mm(H, W_hq) + b_q<br>        outputs.append(Y)<br>    <span class="hljs-keyword">return</span> torch.cat(outputs, dim=<span class="hljs-number">0</span>), (H,)<br></code></pre></td></tr></table></figure><h4 id="创建类包装函数">创建类包装函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNModelScratch</span>: <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, num_hiddens, device,</span><br><span class="hljs-params">                 get_params, init_state, forward_fn</span>):<br>        <span class="hljs-variable language_">self</span>.vocab_size, <span class="hljs-variable language_">self</span>.num_hiddens = vocab_size, num_hiddens<br>        <span class="hljs-variable language_">self</span>.params = get_params(vocab_size, num_hiddens, device)<br>        <span class="hljs-variable language_">self</span>.init_state, <span class="hljs-variable language_">self</span>.forward_fn = init_state, forward_fn<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X, state</span>):<br>        X = F.one_hot(X.T, <span class="hljs-variable language_">self</span>.vocab_size).<span class="hljs-built_in">type</span>(torch.float32)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.forward_fn(X, state, <span class="hljs-variable language_">self</span>.params)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">begin_state</span>(<span class="hljs-params">self, batch_size, device</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.init_state(batch_size, <span class="hljs-variable language_">self</span>.num_hiddens, device)<br></code></pre></td></tr></table></figure><h4 id="预测函数">预测函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_ch8</span>(<span class="hljs-params">prefix, num_preds, net, vocab, device</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span><br>    state = net.begin_state(batch_size=<span class="hljs-number">1</span>, device=device)<br>    outputs = [vocab[prefix[<span class="hljs-number">0</span>]]]<br>    get_input = <span class="hljs-keyword">lambda</span>: torch.tensor([outputs[-<span class="hljs-number">1</span>]], device=device).reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> prefix[<span class="hljs-number">1</span>:]:  <span class="hljs-comment"># 预热期</span><br>        _, state = net(get_input(), state)<br>        outputs.append(vocab[y])<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_preds):  <span class="hljs-comment"># 预测num_preds步</span><br>        y, state = net(get_input(), state)<br>        outputs.append(<span class="hljs-built_in">int</span>(y.argmax(dim=<span class="hljs-number">1</span>).reshape(<span class="hljs-number">1</span>)))<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> outputs])<br></code></pre></td></tr></table></figure><h4 id="梯度裁剪">梯度裁剪</h4><p><span class="math display">\[\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right)\mathbf{g}.\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad_clipping</span>(<span class="hljs-params">net, theta</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        params = [p <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> net.parameters() <span class="hljs-keyword">if</span> p.requires_grad]<br>    <span class="hljs-keyword">else</span>:<br>        params = net.params<br>    norm = torch.sqrt(<span class="hljs-built_in">sum</span>(torch.<span class="hljs-built_in">sum</span>((p.grad ** <span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> params))<br>    <span class="hljs-keyword">if</span> norm &gt; theta:<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>            param.grad[:] *= theta / norm<br></code></pre></td></tr></table></figure><h4 id="训练">训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_epoch_ch8</span>(<span class="hljs-params">net, train_iter, loss, updater, device, use_random_iter</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span><br>    state, timer = <span class="hljs-literal">None</span>, d2l.Timer()<br>    metric = d2l.Accumulator(<span class="hljs-number">2</span>)  <span class="hljs-comment"># 训练损失之和,词元数量</span><br>    <span class="hljs-keyword">for</span> X, Y <span class="hljs-keyword">in</span> train_iter:<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> use_random_iter:<br>            <span class="hljs-comment"># 在第一次迭代或使用随机抽样时初始化state</span><br>            state = net.begin_state(batch_size=X.shape[<span class="hljs-number">0</span>], device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(state, <span class="hljs-built_in">tuple</span>):<br>                <span class="hljs-comment"># state对于nn.GRU是个张量</span><br>                state.detach_()<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span><br>                <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> state:<br>                    s.detach_()<br>        y = Y.T.reshape(-<span class="hljs-number">1</span>)<br>        X, y = X.to(device), y.to(device)<br>        y_hat, state = net(X, state)<br>        l = loss(y_hat, y.long()).mean()<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(updater, torch.optim.Optimizer):<br>            updater.zero_grad()<br>            l.backward()<br>            grad_clipping(net, <span class="hljs-number">1</span>)<br>            updater.step()<br>        <span class="hljs-keyword">else</span>:<br>            l.backward()<br>            grad_clipping(net, <span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># 因为已经调用了mean函数</span><br>            updater(batch_size=<span class="hljs-number">1</span>)<br>        metric.add(l * y.numel(), y.numel())<br>    <span class="hljs-keyword">return</span> math.exp(metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]), metric[<span class="hljs-number">1</span>] / timer.stop()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_ch8</span>(<span class="hljs-params">net, train_iter, vocab, lr, num_epochs, device,</span><br><span class="hljs-params">              use_random_iter=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span><br>    loss = nn.CrossEntropyLoss()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, ylabel=<span class="hljs-string">&#x27;perplexity&#x27;</span>,<br>                            legend=[<span class="hljs-string">&#x27;train&#x27;</span>], xlim=[<span class="hljs-number">10</span>, num_epochs])<br>    <span class="hljs-comment"># 初始化</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        updater = torch.optim.SGD(net.parameters(), lr)<br>    <span class="hljs-keyword">else</span>:<br>        updater = <span class="hljs-keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)<br>    predict = <span class="hljs-keyword">lambda</span> prefix: predict_ch8(prefix, <span class="hljs-number">50</span>, net, vocab, device)<br>    <span class="hljs-comment"># 训练和预测</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        ppl, speed = train_epoch_ch8(<br>            net, train_iter, loss, updater, device, use_random_iter)<br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">&#x27;time traveller&#x27;</span>))<br>            animator.add(epoch + <span class="hljs-number">1</span>, [ppl])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;困惑度 <span class="hljs-subst">&#123;ppl:<span class="hljs-number">.1</span>f&#125;</span>, <span class="hljs-subst">&#123;speed:<span class="hljs-number">.1</span>f&#125;</span> 词元/秒 <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">&#x27;time traveller&#x27;</span>))<br>    <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">&#x27;traveller&#x27;</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs, lr = <span class="hljs-number">500</span>, <span class="hljs-number">1</span><br>train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())<br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vim">困惑度 <span class="hljs-number">1.0</span>, <span class="hljs-number">67212.6</span> 词元/秒 cud<span class="hljs-variable">a:0</span><br>time traveller <span class="hljs-keyword">for</span> <span class="hljs-keyword">so</span> it will <span class="hljs-keyword">be</span> convenient <span class="hljs-keyword">to</span> speak of himwas <span class="hljs-keyword">e</span><br>travelleryou can show black <span class="hljs-keyword">is</span> white by <span class="hljs-keyword">argument</span> said filby<br></code></pre></td></tr></table></figure></blockquote><h3 id="代码简洁实现">代码简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>batch_size, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">35</span><br>train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">num_hiddens = <span class="hljs-number">256</span><br>rnn_layer = nn.RNN(<span class="hljs-built_in">len</span>(vocab), num_hiddens)<br><span class="hljs-comment"># 形状是（隐藏层数，批量大小，隐藏单元数）</span><br>state = torch.zeros((<span class="hljs-number">1</span>, batch_size, num_hiddens))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(num_steps, batch_size, <span class="hljs-built_in">len</span>(vocab)))<br>Y, state_new = rnn_layer(X, state)<br>Y.shape, state_new.shape<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNModel</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, rnn_layer, vocab_size, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(RNNModel, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.rnn = rnn_layer<br>        <span class="hljs-variable language_">self</span>.vocab_size = vocab_size<br>        <span class="hljs-variable language_">self</span>.num_hiddens = <span class="hljs-variable language_">self</span>.rnn.hidden_size<br>        <span class="hljs-comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1,需要构造自己的输出层</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.rnn.bidirectional:<br>            <span class="hljs-variable language_">self</span>.num_directions = <span class="hljs-number">1</span><br>            <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-variable language_">self</span>.num_hiddens, <span class="hljs-variable language_">self</span>.vocab_size)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.num_directions = <span class="hljs-number">2</span><br>            <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-variable language_">self</span>.num_hiddens * <span class="hljs-number">2</span>, <span class="hljs-variable language_">self</span>.vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, state</span>):<br>        X = F.one_hot(inputs.T.long(), <span class="hljs-variable language_">self</span>.vocab_size)<br>        X = X.to(torch.float32)<br>        Y, state = <span class="hljs-variable language_">self</span>.rnn(X, state)<br>        <span class="hljs-comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span><br>        <span class="hljs-comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span><br>        output = <span class="hljs-variable language_">self</span>.linear(Y.reshape((-<span class="hljs-number">1</span>, Y.shape[-<span class="hljs-number">1</span>])))<br>        <span class="hljs-keyword">return</span> output, state<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">begin_state</span>(<span class="hljs-params">self, device, batch_size=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(<span class="hljs-variable language_">self</span>.rnn, nn.LSTM):<br>            <span class="hljs-comment"># nn.GRU以张量作为隐状态</span><br>            <span class="hljs-keyword">return</span>  torch.zeros((<span class="hljs-variable language_">self</span>.num_directions * <span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                                 batch_size, <span class="hljs-variable language_">self</span>.num_hiddens),<br>                                device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># nn.LSTM以元组作为隐状态</span><br>            <span class="hljs-keyword">return</span> (torch.zeros((<br>                <span class="hljs-variable language_">self</span>.num_directions * <span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                batch_size, <span class="hljs-variable language_">self</span>.num_hiddens), device=device),<br>                    torch.zeros((<br>                        <span class="hljs-variable language_">self</span>.num_directions * <span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                        batch_size, <span class="hljs-variable language_">self</span>.num_hiddens), device=device))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">device = d2l.try_gpu()<br>net = RNNModel(rnn_layer, vocab_size=<span class="hljs-built_in">len</span>(vocab))<br>net = net.to(device)<br>d2l.predict_ch8(<span class="hljs-string">&#x27;time traveller&#x27;</span>, <span class="hljs-number">10</span>, net, vocab, device)<br></code></pre></td></tr></table></figure><h2 id="循环神经网络的应用gru门控循环单元">循环神经网络的应用——GRU(门控循环单元)</h2><p>文章中举了一个书籍中段落的例子，各个章节可能存在逻辑中断，在这种情况下，也为缓解梯度的异常的问题，使用GRU的方法重置内部的状态。</p><p>门控循环单元与普通循环神经网路关键区别在于有专门的机制更新隐状态和重置隐状态。</p><h3 id="重置门和更新门">重置门和更新门</h3><figure><img src="image-20241117193249996.png" alt="image-20241117193249996"><figcaption aria-hidden="true">image-20241117193249996</figcaption></figure><p>输入小批量<span class="math inline">\(\mathbf{X}_t \in \mathbb{R}^{n\times d}\)</span>(样本个数<span class="math inline">\(n\)</span>，输入个数<span class="math inline">\(d\)</span>)，上一步的隐状态是<span class="math inline">\(\mathbf{H}_{t-1} \in \mathbb{R}^{n \timesh}\)</span>则重置门<span class="math inline">\(\mathbf{R}_t \in\mathbb{R}^{n \times h}\)</span>和更新门<span class="math inline">\(\mathbf{Z}_t \in \mathbb{R}^{n \timesh}\)</span>表示为 <span class="math display">\[\begin{split}\begin{aligned}\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1}\mathbf{W}_{hr} + \mathbf{b}_r),\\\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1}\mathbf{W}_{hz} + \mathbf{b}_z),\end{aligned}\end{split}\]</span> <span class="math inline">\(\mathbf{W}_{xr}, \mathbf{W}_{xz}\in \mathbb{R}^{d \times h}\)</span>为权重，<span class="math inline">\(\mathbf{b}_r, \mathbf{b}_z \in \mathbb{R}^{1\times h}\)</span>为偏置</p><h3 id="候选隐状态">候选隐状态</h3><p><span class="math display">\[\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} +\left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} +\mathbf{b}_h),\]</span></p><p><span class="math inline">\(\odot\)</span> 表示哈达玛积，<span class="math inline">\(W,b\)</span>分别为权重和偏置。</p><p><img src="image-20241117194218357.png"></p><h3 id="隐状态">隐状态</h3><p>上述的计算结果只是候选隐状态，我们仍然需要结合更新门Zt的效果。此时，来自<span class="math inline">\(X_{t}\)</span>的信息基本上被忽略。相反，当<span class="math inline">\(Z_{t}\)</span>接近0时， 新的隐状态<span class="math inline">\(\mathbf{H}_t\)</span>就会接近候选隐状态<span class="math inline">\(\tilde{\mathbf{H}}_t\)</span>。这些设计可以处理循环神经网络中的梯度消失问题.<span class="math display">\[\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t)\odot \tilde{\mathbf{H}}_t.\]</span> <img src="image-20241117194928980.png"></p><h2 id="长短期记忆网络lstm">长短期记忆网络(LSTM)</h2><h3 id="输入门忘记门输出门">输入门、忘记门、输出门</h3><p><img src="image-20241117203925653.png" alt="image-20241117203925653"> <span class="math display">\[\begin{split}\begin{aligned}\mathbf{I}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{xi} +\mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\\mathbf{F}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{xf} +\mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\\mathbf{O}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{xo} +\mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),\end{aligned}\end{split}\]</span></p><h3 id="候选记忆单元">候选记忆单元</h3><p><span class="math display">\[\tilde{\mathbf{C}}_t = \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} +\mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c),\]</span></p><figure><img src="image-20241117204012572.png" alt="image-20241117204012572"><figcaption aria-hidden="true">image-20241117204012572</figcaption></figure><h3 id="记忆元">记忆元</h3><p><span class="math display">\[\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot\tilde{\mathbf{C}}_t.\]</span></p><figure><img src="image-20241117204048297.png" alt="image-20241117204048297"><figcaption aria-hidden="true">image-20241117204048297</figcaption></figure><h3 id="隐状态-1">隐状态</h3><p><span class="math display">\[\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t).\]</span></p><p><img src="image-20241117204302395.png"></p><h2 id="深度循环神经网络">深度循环神经网络</h2><p><img src="image-20241120104501588.png"></p><p>基础公式为： <span class="math display">\[\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} +\mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)}),\]</span> 最后，输出层的计算仅基于第l个隐藏层最终的隐状态： <span class="math display">\[\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,\]</span></p><h2 id="双向循环神经网络">双向循环神经网络</h2><p><img src="image-20241120105334815.png"></p><p><span class="math display">\[\begin{split}\begin{aligned}\overrightarrow{\mathbf{H}}_t &amp;= \phi(\mathbf{X}_t\mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1}\mathbf{W}_{hh}^{(f)}  + \mathbf{b}_h^{(f)}),\\\overleftarrow{\mathbf{H}}_t &amp;= \phi(\mathbf{X}_t\mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1}\mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)}),\end{aligned}\end{split}\]</span> 接下来，将前向隐状态<span class="math inline">\(\overrightarrow{\mathbf{H}}_t\)</span>和反向隐状态<span class="math inline">\(\overleftarrow{\mathbf{H}}_t\)</span><strong>连接</strong>，获得需要送入输出层的隐状态<span class="math inline">\(\mathbf{H}_t \in \mathbb{R}^{n \times2h}\)</span>。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。 最后，输出层计算得到的输出为<span class="math inline">\(\mathbf{O}_t \in \mathbb{R}^{n \timesq}\)</span>(<span class="math inline">\(q\)</span>是输出单元的数目)<span class="math display">\[\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.\]</span>连接：常规操作有<strong>拼接（Concatenation）</strong>和<strong>加和（Summation）</strong></p><p>拼接：在双向RNN中，时间步t的隐藏层输出将是这两个隐藏状态的拼接：<span class="math display">\[\mathbf{H}_t =[\overrightarrow{\mathbf{H}}_t，\overleftarrow{\mathbf{H}}_t]\]</span> 加和:将前向和反向RNN的输出直接相加。 <span class="math display">\[\mathbf{H}_t =\overrightarrow{\mathbf{H}}_t+\overleftarrow{\mathbf{H}}_t\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><code>bidirectional=True</code> 双向循环网络的开关</p><h2 id="编码器与解码器">编码器与解码器</h2><p>序列转换是核心问题，为了处理输入和输出，设计一个包含两个组件的架构：编码器和解码器<strong>(encoder-decoder)</strong></p><p><img src="image-20241120194517674.png"></p><p>编码器，继承nn.module模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><br><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Encoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, *args</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br></code></pre></td></tr></table></figure><p>解码器，新增init_state函数，用于将编码器的输出（<code>enc_outputs</code>）转换为编码后的状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Decoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, enc_outputs, *args</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, state</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br></code></pre></td></tr></table></figure><p>合并编码器和解码器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderDecoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, encoder, decoder, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(EncoderDecoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.encoder = encoder<br>        <span class="hljs-variable language_">self</span>.decoder = decoder<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, enc_X, dec_X, *args</span>):<br>        enc_outputs = <span class="hljs-variable language_">self</span>.encoder(enc_X, *args)<br>        dec_state = <span class="hljs-variable language_">self</span>.decoder.init_state(enc_outputs, *args)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.decoder(dec_X, dec_state)<br></code></pre></td></tr></table></figure><h2 id="序列到序列学习">序列到序列学习</h2><p><img src="image-20241122154255162.png"></p><p>特定的“<eos>”表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。“<bos>”表示序列开始词元，它是解码器的输入序列的第一个词元。</bos></eos></p><h3 id="编码器">编码器</h3><p><span class="math inline">\(x_{t}\)</span>是文本序列中的第<span class="math inline">\(t\)</span>个词元，<span class="math inline">\(t\)</span>为时间步，<span class="math inline">\(h_{t-1}\)</span>为上一时间步的状态。使用一个函数<span class="math inline">\(f\)</span>来描述循环神经网络的循环层所做的变换：<span class="math display">\[\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).\]</span> 编码器通过选定的函数<span class="math inline">\(q\)</span>，将所有的时间步的隐状态转换为上下文变量:<span class="math display">\[\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T).\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Seq2SeqEncoder</span>(d2l.Encoder):<br>    <span class="hljs-string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, embed_size, num_hiddens, num_layers,</span><br><span class="hljs-params">                 dropout=<span class="hljs-number">0</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Seq2SeqEncoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-comment"># 嵌入层</span><br>        <span class="hljs-variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)<br>        <span class="hljs-variable language_">self</span>.rnn = nn.GRU(embed_size, num_hiddens, num_layers,<br>                          dropout=dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, *args</span>):<br>        <span class="hljs-comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span><br>        X = <span class="hljs-variable language_">self</span>.embedding(X)<br>        <span class="hljs-comment"># 在循环神经网络模型中，第一个轴对应于时间步</span><br>        X = X.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 如果未提及状态，则默认为0</span><br>        output, state = <span class="hljs-variable language_">self</span>.rnn(X)<br>        <span class="hljs-comment"># output的形状:(num_steps,batch_size,num_hiddens)</span><br>        <span class="hljs-comment"># state的形状:(num_layers,batch_size,num_hiddens)</span><br>        <span class="hljs-keyword">return</span> output, state<br></code></pre></td></tr></table></figure><h3 id="解码器">解码器</h3><p>编码器输出的上下文变量<span class="math inline">\(c\)</span>对整个序列<span class="math inline">\(x_1, \ldots,x_T\)</span>进行编码。来自训练数据集的输出序列<span class="math inline">\(y_1, y_2, \ldots,y_{T&#39;}\)</span>，对于每个时间步<span class="math inline">\(t&#39;\)</span>，解码器的<span class="math inline">\(y_{t&#39;}\)</span>概率取决于<span class="math inline">\(y_1, \ldots,y_{t&#39;-1}\)</span>和上下文变量<span class="math inline">\(c\)</span>，即<span class="math inline">\(P(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1},\mathbf{c})\)</span> <span class="math display">\[\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c},\mathbf{s}_{t^\prime-1}).\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Seq2SeqDecoder</span>(d2l.Decoder):<br>    <span class="hljs-string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, embed_size, num_hiddens, num_layers,</span><br><span class="hljs-params">                 dropout=<span class="hljs-number">0</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Seq2SeqDecoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)<br>        <span class="hljs-variable language_">self</span>.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,<br>                          dropout=dropout)<br>        <span class="hljs-variable language_">self</span>.dense = nn.Linear(num_hiddens, vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, enc_outputs, *args</span>):<br>        <span class="hljs-comment"># output, state = encoder(x),取出的是state</span><br>        <span class="hljs-keyword">return</span> enc_outputs[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, state</span>):<br>        <span class="hljs-comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)，时间步放在前面</span><br>        X = <span class="hljs-variable language_">self</span>.embedding(X).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 广播context，使其具有与X相同的num_steps ， state[-1]最后一层隐藏状态的输出</span><br>        context = state[-<span class="hljs-number">1</span>].repeat(X.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># torch.cat的结果是最后一层解码器的输出再加上最后一层隐藏层的内容</span><br>        X_and_context = torch.cat的结果是最后一层解码器的输出再加上最后一层隐藏层的内容((X, context), <span class="hljs-number">2</span>)<br>        output, state = <span class="hljs-variable language_">self</span>.rnn(X_and_context, state)<br>        output = <span class="hljs-variable language_">self</span>.dense(output).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># output的形状:(batch_size,num_steps,vocab_size)</span><br>        <span class="hljs-comment"># state的形状:(num_layers,batch_size,num_hiddens)</span><br>        <span class="hljs-keyword">return</span> output, state<br></code></pre></td></tr></table></figure><p><img src="image-20241123121134324.png"></p><h3 id="损失函数">损失函数</h3><p>构建sequence_mask函数通过零值化屏蔽不相关项：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sequence_mask</span>(<span class="hljs-params">X, valid_len, value=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot;</span><br>    maxlen = X.size(<span class="hljs-number">1</span>)<br>    mask = torch.arange((maxlen), dtype=torch.float32,<br>                        device=X.device)[<span class="hljs-literal">None</span>, :] &lt; valid_len[:, <span class="hljs-literal">None</span>]<br>    X[~mask] = value<br>    <span class="hljs-keyword">return</span> X<br><br>X = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br>sequence_mask(X, torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]))<br></code></pre></td></tr></table></figure><p>代码理解：</p><ul><li><p><strong><code>maxlen = X.size(1)</code></strong>：获取<code>X</code>张量的最大长度（即序列的最大步数）。在例子中，<code>X</code> 的形状是<code>(2, 3)</code>，所以 <code>maxlen</code> 将是<code>3</code></p></li><li><p><strong>mask = torch.arange((maxlen), dtype=torch.float32,device=X.device)[None, :] &lt; valid_len[:, None]：</strong></p></li></ul><p>​ None的作用：在 NumPy 和 PyTorch中是用来增加新维度的。它通常与切片操作结合使用，来添加一个新的轴。</p><p>​ [None, :] 的含义：<code>None</code>放在前面（<code>[None, :]</code>）表示在第一个轴（即行轴）上增加一个新的维度。</p><p>例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>b = a[<span class="hljs-literal">None</span>, :]<br><span class="hljs-built_in">print</span>(b.shape)  <span class="hljs-comment"># 输出 (1, 3)</span><br><br></code></pre></td></tr></table></figure><p>​ valid_len[:, None]的含义：在函数的末尾增加一个维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">valid_len = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>valid_len_expanded = valid_len[:, <span class="hljs-literal">None</span>]<br><span class="hljs-built_in">print</span>(valid_len_expanded)<br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><p>tensor([[1], [2], [3]])</p></blockquote><p>通过广播机制，PyTorch会自动将这两个张量的形状对齐，并按元素逐一比较，生成一个布尔值的张量。</p><p>(1, maxlen)与valid_len[:, None]按元素比较，比较的结果是：</p><p>如果当前时间步小于 <code>valid_len</code>，则返回<code>True</code>（表示这个时间步有效）。</p><p>否则，返回 <code>False</code>（表示这个时间步无效）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 输入数据</span><br>valid_len = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br>maxlen = <span class="hljs-number">3</span><br><br><span class="hljs-comment"># 创建张量，包含从0到maxlen-1的值</span><br>arange_tensor = torch.arange(maxlen)<br><br><span class="hljs-comment"># 将其转换为形状 (1, maxlen) 形式</span><br>arange_tensor = arange_tensor[<span class="hljs-literal">None</span>, :]  <span class="hljs-comment"># 结果是形状 (1, 3)</span><br><br><span class="hljs-comment"># 将 valid_len 转换为形状 (batch_size, 1) 形式</span><br>valid_len_expanded = valid_len[:, <span class="hljs-literal">None</span>]  <span class="hljs-comment"># 结果是形状 (2, 1)</span><br><br><span class="hljs-comment"># 比较操作</span><br>mask = arange_tensor &lt; valid_len_expanded  <span class="hljs-comment"># 广播后，形状为 (2, 3)</span><br><br><span class="hljs-built_in">print</span>(mask)<br></code></pre></td></tr></table></figure><ul><li>X[~mask] = value：</li></ul><p>~mask表示取反，该段代码整体表现为将 <code>X</code> 中对应<code>~mask</code> 为 <code>True</code> 的位置的元素赋值为<code>value</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MaskedSoftmaxCELoss</span>(nn.CrossEntropyLoss):<br>    <span class="hljs-string">&quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># pred的形状：(batch_size,num_steps,vocab_size)</span><br>    <span class="hljs-comment"># label的形状：(batch_size,num_steps)</span><br>    <span class="hljs-comment"># valid_len的形状：(batch_size,)</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, pred, label, valid_len</span>):<br>        weights = torch.ones_like(label)<br>        weights = sequence_mask(weights, valid_len)<br>        <span class="hljs-variable language_">self</span>.reduction=<span class="hljs-string">&#x27;none&#x27;</span><br>        unweighted_loss = <span class="hljs-built_in">super</span>().forward(<br>            pred.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>), label)<br>        <span class="hljs-comment"># 有效的地方留下来</span><br>        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> weighted_loss<br></code></pre></td></tr></table></figure><h3 id="训练-1">训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_seq2seq</span>(<span class="hljs-params">net, data_iter, lr, num_epochs, tgt_vocab, device</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">xavier_init_weights</span>(<span class="hljs-params">m</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>            nn.init.xavier_uniform_(m.weight)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.GRU:<br>            <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> m._flat_weights_names:<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;weight&quot;</span> <span class="hljs-keyword">in</span> param:<br>                    nn.init.xavier_uniform_(m._parameters[param])<br><br>    net.apply(xavier_init_weights)<br>    net.to(device)<br>    optimizer = torch.optim.Adam(net.parameters(), lr=lr)<br>    loss = MaskedSoftmaxCELoss()<br>    net.train()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, ylabel=<span class="hljs-string">&#x27;loss&#x27;</span>,<br>                     xlim=[<span class="hljs-number">10</span>, num_epochs])<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        timer = d2l.Timer()<br>        metric = d2l.Accumulator(<span class="hljs-number">2</span>)  <span class="hljs-comment"># 训练损失总和，词元数量</span><br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> data_iter:<br>            optimizer.zero_grad()<br>            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> batch]<br>            bos = torch.tensor([tgt_vocab[<span class="hljs-string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="hljs-number">0</span>],<br>                          device=device).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># 在句子的开头增加一个开始词元</span><br>            dec_input = torch.cat([bos, Y[:, :-<span class="hljs-number">1</span>]], <span class="hljs-number">1</span>)  <span class="hljs-comment"># 强制教学</span><br>            Y_hat, _ = net(X, dec_input, X_valid_len)<br>            l = loss(Y_hat, Y, Y_valid_len)<br>            l.<span class="hljs-built_in">sum</span>().backward()      <span class="hljs-comment"># 损失函数的标量进行“反向传播”</span><br>            d2l.grad_clipping(net, <span class="hljs-number">1</span>)<br>            num_tokens = Y_valid_len.<span class="hljs-built_in">sum</span>()<br>            optimizer.step()<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                metric.add(l.<span class="hljs-built_in">sum</span>(), num_tokens)<br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            animator.add(epoch + <span class="hljs-number">1</span>, (metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>],))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;loss <span class="hljs-subst">&#123;metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]:<span class="hljs-number">.3</span>f&#125;</span>, <span class="hljs-subst">&#123;metric[<span class="hljs-number">1</span>] / timer.stop():<span class="hljs-number">.1</span>f&#125;</span> &#x27;</span><br>        <span class="hljs-string">f&#x27;tokens/sec on <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">embed_size, num_hiddens, num_layers, dropout = <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.1</span><br>batch_size, num_steps = <span class="hljs-number">64</span>, <span class="hljs-number">10</span><br>lr, num_epochs, device = <span class="hljs-number">0.005</span>, <span class="hljs-number">300</span>, d2l.try_gpu()<br><br>train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)<br>encoder = Seq2SeqEncoder(<span class="hljs-built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers,<br>                        dropout)<br>decoder = Seq2SeqDecoder(<span class="hljs-built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers,<br>                        dropout)<br>net = d2l.EncoderDecoder(encoder, decoder)<br>train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)<br></code></pre></td></tr></table></figure><p>输出结果：</p><p><img src="image-20241123150844387.png"></p><h3 id="预测">预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_seq2seq</span>(<span class="hljs-params">net, src_sentence, src_vocab, tgt_vocab, num_steps,</span><br><span class="hljs-params">                    device, save_attention_weights=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 在预测时将net设置为评估模式</span><br>    net.<span class="hljs-built_in">eval</span>()<br>    src_tokens = src_vocab[src_sentence.lower().split(<span class="hljs-string">&#x27; &#x27;</span>)] + [<br>        src_vocab[<span class="hljs-string">&#x27;&lt;eos&gt;&#x27;</span>]]<br>    enc_valid_len = torch.tensor([<span class="hljs-built_in">len</span>(src_tokens)], device=device)<br>    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[<span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>])<br>    <span class="hljs-comment"># 添加批量轴</span><br>    enc_X = torch.unsqueeze(<br>        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="hljs-number">0</span>)<br>    enc_outputs = net.encoder(enc_X, enc_valid_len)<br>    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)<br>    <span class="hljs-comment"># 添加批量轴</span><br>    dec_X = torch.unsqueeze(torch.tensor(<br>        [tgt_vocab[<span class="hljs-string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class="hljs-number">0</span>)<br>    output_seq, attention_weight_seq = [], []<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>        Y, dec_state = net.decoder(dec_X, dec_state)<br>        <span class="hljs-comment"># 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span><br>        dec_X = Y.argmax(dim=<span class="hljs-number">2</span>)<br>        pred = dec_X.squeeze(dim=<span class="hljs-number">0</span>).<span class="hljs-built_in">type</span>(torch.int32).item()<br>        <span class="hljs-comment"># 保存注意力权重（稍后讨论）</span><br>        <span class="hljs-keyword">if</span> save_attention_weights:<br>            attention_weight_seq.append(net.decoder.attention_weights)<br>        <span class="hljs-comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span><br>        <span class="hljs-keyword">if</span> pred == tgt_vocab[<span class="hljs-string">&#x27;&lt;eos&gt;&#x27;</span>]:<br>            <span class="hljs-keyword">break</span><br>        output_seq.append(pred)<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27; &#x27;</span>.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq<br></code></pre></td></tr></table></figure><h3 id="预测序列的评估">预测序列的评估</h3><p><span class="math display">\[\exp\left(\min\left(0, 1 -\frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right)\prod_{n=1}^k p_n^{1/2^n},\]</span></p><p>其中<span class="math inline">\(\mathrm{len}_{\text{label}}\)</span>表示标签序列中的词元数<span class="math inline">\(\mathrm{len}_{\text{pred}}\)</span>表示预测序列中的词元数，表示用于匹配的最长的n元语法。<span class="math inline">\(p_n\)</span>表示<span class="math inline">\(n\)</span>元语法的精确度，举例来说，给定标签序列<span class="math inline">\(A,B,C,D,E,F\)</span>和预测序列<span class="math inline">\(A,B,B,C,D\)</span>，<span class="math inline">\(p_{1} =序列长度为1(A,B,B,C,D在给定标签中是否存在，存在则加1)/预测序列步长为1的个数= 4/5\)</span>，<span class="math inline">\(p_{2} =3(AB,BB,BC,CD在给定标签序列中存在的个数)/4(预测序列步长为2的个数)\)</span>,<span class="math inline">\(p_{3} = 1/3\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">bleu</span>(<span class="hljs-params">pred_seq, label_seq, k</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span><br>    pred_tokens, label_tokens = pred_seq.split(<span class="hljs-string">&#x27; &#x27;</span>), label_seq.split(<span class="hljs-string">&#x27; &#x27;</span>)<br>    len_pred, len_label = <span class="hljs-built_in">len</span>(pred_tokens), <span class="hljs-built_in">len</span>(label_tokens)<br>    score = math.exp(<span class="hljs-built_in">min</span>(<span class="hljs-number">0</span>, <span class="hljs-number">1</span> - len_label / len_pred))<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, k + <span class="hljs-number">1</span>):<br>        num_matches, label_subs = <span class="hljs-number">0</span>, collections.defaultdict(<span class="hljs-built_in">int</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(len_label - n + <span class="hljs-number">1</span>):<br>            label_subs[<span class="hljs-string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(len_pred - n + <span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> label_subs[<span class="hljs-string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="hljs-number">0</span>:<br>                num_matches += <span class="hljs-number">1</span><br>                label_subs[<span class="hljs-string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] -= <span class="hljs-number">1</span><br>        score *= math.<span class="hljs-built_in">pow</span>(num_matches / (len_pred - n + <span class="hljs-number">1</span>), math.<span class="hljs-built_in">pow</span>(<span class="hljs-number">0.5</span>, n))<br>    <span class="hljs-keyword">return</span> score<br></code></pre></td></tr></table></figure><p>预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">engs = [<span class="hljs-string">&#x27;go .&#x27;</span>, <span class="hljs-string">&quot;i lost .&quot;</span>, <span class="hljs-string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="hljs-string">&#x27;i\&#x27;m home .&#x27;</span>]<br>fras = [<span class="hljs-string">&#x27;va !&#x27;</span>, <span class="hljs-string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="hljs-string">&#x27;il est calme .&#x27;</span>, <span class="hljs-string">&#x27;je suis chez moi .&#x27;</span>]<br><span class="hljs-keyword">for</span> eng, fra <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(engs, fras):<br>    translation, attention_weight_seq = predict_seq2seq(<br>        net, eng, src_vocab, tgt_vocab, num_steps, device)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;eng&#125;</span> =&gt; <span class="hljs-subst">&#123;translation&#125;</span>, bleu <span class="hljs-subst">&#123;bleu(translation, fra, k=<span class="hljs-number">2</span>):<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出：</p><p><img src="image-20241123191624785.png"></p><h4 id="参考文献">参考文献:</h4><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>动手学深度学习<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Wood, F., Gasthaus, J.,Archambeau, C., James, L., &amp; Teh, Y. W. (2011). The sequencememoizer. <em>Communications of the ACM</em>, <em>54</em>(2), 91–98.<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>pytorch 基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>卷积神经网络LeNet5</title>
    <link href="/2024/11/11/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CLeNet/"/>
    <url>/2024/11/11/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CLeNet/</url>
    
    <content type="html"><![CDATA[<p>本文使用<strong>LeNet5</strong>识别手写数字。</p><h2 id="letnet-5-的基本结构">LetNet-5 的基本结构</h2><p>LeNet-5包含7层网络结构（不含输入层），包含两个卷积层、两个降采样层（池化层）、两个全连接层和输出层。</p><p><img src="image-20241111161255154.png"></p><h3 id="输入层input-layer">1.输入层(input layer)</h3><p>输入层的大小为32×32手写图像，在实际应用中，通常会对图像进行预处理，如对像素进行归一化。</p><h3 id="卷积层c1convolutional-layer-c1">2、卷积层C1（Convolutional layerC1）</h3><p><img src="image-20241111164556129.png"></p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>根据代码的含义，通道数从<strong>1变6</strong>，原图的尺寸从32×32变为28×28，这与卷积核大小、步幅和<span class="math inline">\(padding\)</span>有关，其输出的特征图像尺寸如下：<span class="math display">\[output size = \frac{W-kernelsize+2×padding}{stride}+1\]</span> 其中，<span class="math inline">\(W\)</span>表示输入图像的宽度。 <span class="math display">\[output size = \frac{32-5+2×0}{1}+1 = 28\]</span> 代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Sigmoid()<br></code></pre></td></tr></table></figure><p>在卷积操作完成之后会进行<strong>归一化（BatchNormalization）</strong>操作提高神经网络和加速收敛</p><h3 id="采样层s2subsampling-layer-s2">3、采样层S2（Subsampling layerS2）</h3><p><img src="image-20241111164616015.png"></p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p>通道数没有发生过改变，计算公式如下： <span class="math display">\[outputsize =\frac{W-poolsize}{stride}+1 = \frac{28-2}{2}+1 = 14\]</span> 然后同上，进行批量归一化。</p><h3 id="卷积层c3convolutional-layer-c3">4、卷积层C3（Convolutional layerC3）</h3><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p>通道数由6变16，输出特征图尺寸为10x10，具体计算如下： <span class="math display">\[output size = \frac{W-kernelsize+2×padding}{stride}+1 =\frac{14-5+2×0}{1}+1 = 10\]</span> 该层较为特殊，16 个卷积核并不是都与 S2 的 6个通道层进行卷积操作，如下图所示，C3 的前六个特征图（0,1,2,3,4,5）由 S2的相邻三个特征图作为输入，对应的卷积核尺寸为：5x5x3；接下来的 6个特征图（6,7,8,9,10,11）由 S2的相邻四个特征图作为输入对应的卷积核尺寸为：5x5x4；接下来的 3个特征图（12,13,14）号特征图由 S2间断的四个特征图作为输入对应的卷积核尺寸为：5x5x4；最后的 15 号特征图由S2 全部(6 个)特征图作为输入，对应的卷积核尺寸为：5x5x6。</p><p><img src="image-20241111170703849.png"></p><h3 id="采样层s4subsampling-layer-s4">5、采样层S4（Subsampling layerS4）</h3><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br></code></pre></td></tr></table></figure><p>特征图尺寸由10×10变成5×5，输出通道还是<strong>16</strong>。 <span class="math display">\[outputsize = \frac{W-kernelsize}{stride}+1 = \frac{10-2}{2} +1 = 5\]</span></p><h3 id="全连接层c5fully-connected-layer-c5">6、全连接层C5（Fullyconnected layer C5）</h3><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br></code></pre></td></tr></table></figure><p>C5将每个大小为 5×5的特征图拉成一个长度为400<strong>(维度为1X16X5X5)</strong>的向量，并通过一个带有120个神经元的全连接层进行连接。120是由LeNet-5的设计者根据实验得到的最佳值。</p><h3 id="全连接层f6fully-connected-layer-f6">7、全连接层F6（Fullyconnected layer F6）</h3><p>全连接层F6将120个神经元连接到84个神经元。</p><h3 id="输出层output-layer">8、输出层（Output layer）</h3><p>输出层由10个神经元组成，每个神经元对应0-9中的一个数字，并输出最终的分类结果。</p><h2 id="搭建网络">搭建网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>net = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><h2 id="加载数据集">加载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载训练集</span><br>train_dataset = torchvision.datasets.MNIST(root = <span class="hljs-string">&#x27;./data&#x27;</span>,<span class="hljs-comment"># 数据集保存路径</span><br>                                           train = <span class="hljs-literal">True</span>,<span class="hljs-comment"># 是否为训练集</span><br>                                           <span class="hljs-comment"># 数据预处理</span><br>                                           transform = transforms.Compose([<br>                                                  transforms.Resize((<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)),<br>                                                  transforms.ToTensor(),<br>                                                  transforms.Normalize(mean = (<span class="hljs-number">0.1307</span>,), <br>                     std = (<span class="hljs-number">0.3081</span>,))]),<br>                                           download = <span class="hljs-literal">True</span>)<span class="hljs-comment">#是否下载</span><br> <br><span class="hljs-comment"># 加载测试集</span><br>test_dataset = torchvision.datasets.MNIST(root = <span class="hljs-string">&#x27;./data&#x27;</span>,<br>                                          train = <span class="hljs-literal">False</span>,<br>                                          transform = transforms.Compose([<br>                                                  transforms.Resize((<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)),<br>                                                  transforms.ToTensor(),<br>                                                  transforms.Normalize(mean = (<span class="hljs-number">0.1325</span>,), <br>                     std = (<span class="hljs-number">0.3105</span>,))]),<br>                                          download=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 一次抓64张牌</span><br>batch_size = <span class="hljs-number">64</span><br><span class="hljs-comment"># 加载训练数据</span><br>train_loader = torch.utils.data.DataLoader(dataset = train_dataset,<br>                                           batch_size = batch_size,<br>                                           shuffle = <span class="hljs-literal">True</span>)<span class="hljs-comment"># 是否打乱</span><br><span class="hljs-comment"># 加载测试数据</span><br>test_loader = torch.utils.data.DataLoader(dataset = test_dataset,<br>                                           batch_size = batch_size,<br>                                           shuffle = <span class="hljs-literal">False</span>)<span class="hljs-comment"># 是否打乱</span><br></code></pre></td></tr></table></figure><ul><li>测试阶段的<code>shuffle=False</code>：在测试阶段，通常不需要打乱数据的顺序。测试时模型是在未见过的数据上进行评估，因此希望模型看到的是原始数据的有序顺序，以便能够更好地评估模型的泛化性能。如果在测试时也打乱数据，可能会导致模型在评估时看到的数据分布与实际场景不一致。（其实如果是<code>True</code>影响也不大）</li></ul><h2 id="设置评价指标">设置评价指标</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_accuracy_gpu</span>(<span class="hljs-params">net, data_iter, device=<span class="hljs-literal">None</span></span>): <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        net.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 设置为评估模式</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> device:<br>            device = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(net.parameters())).device<br>    <span class="hljs-comment"># 正确预测的数量，总预测的数量</span><br>    metric = d2l.Accumulator(<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(X, <span class="hljs-built_in">list</span>):<br>                <span class="hljs-comment"># BERT微调所需的（之后将介绍）</span><br>                X = [x.to(device) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X]<br>            <span class="hljs-keyword">else</span>:<br>                X = X.to(device)<br>            y = y.to(device)<br>            metric.add(d2l.accuracy(net(X), y), y.numel())<br>    <span class="hljs-keyword">return</span> metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><h2 id="模型训练">模型训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_ch6</span>(<span class="hljs-params">net, train_iter, test_iter, num_epochs, lr, device</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear <span class="hljs-keyword">or</span> <span class="hljs-built_in">type</span>(m) == nn.Conv2d:<br>            nn.init.xavier_uniform_(m.weight)<br>    net.apply(init_weights)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;training on&#x27;</span>, device)<br>    net.to(device)<br>    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<br>    loss = nn.CrossEntropyLoss()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, xlim=[<span class="hljs-number">1</span>, num_epochs],<br>                            legend=[<span class="hljs-string">&#x27;train loss&#x27;</span>, <span class="hljs-string">&#x27;train acc&#x27;</span>, <span class="hljs-string">&#x27;test acc&#x27;</span>])<br>    timer, num_batches = d2l.Timer(), <span class="hljs-built_in">len</span>(train_iter)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        <span class="hljs-comment"># 训练损失之和，训练准确率之和，样本数</span><br>        metric = d2l.Accumulator(<span class="hljs-number">3</span>)<br>        net.train()<br>        <span class="hljs-keyword">for</span> i, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_iter):<br>            timer.start()<br>            optimizer.zero_grad()<br>            X, y = X.to(device), y.to(device)<br>            y_hat = net(X)<br>            l = loss(y_hat, y)<br>            l.backward()<br>            optimizer.step()<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                metric.add(l * X.shape[<span class="hljs-number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="hljs-number">0</span>])<br>            timer.stop()<br>            train_l = metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">2</span>]<br>            train_acc = metric[<span class="hljs-number">1</span>] / metric[<span class="hljs-number">2</span>]<br>            <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % (num_batches // <span class="hljs-number">5</span>) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> i == num_batches - <span class="hljs-number">1</span>:<br>                animator.add(epoch + (i + <span class="hljs-number">1</span>) / num_batches,<br>                             (train_l, train_acc, <span class="hljs-literal">None</span>))<br>        test_acc = evaluate_accuracy_gpu(net, test_iter)<br>        animator.add(epoch + <span class="hljs-number">1</span>, (<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, test_acc))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;loss <span class="hljs-subst">&#123;train_l:<span class="hljs-number">.3</span>f&#125;</span>, train acc <span class="hljs-subst">&#123;train_acc:<span class="hljs-number">.3</span>f&#125;</span>, &#x27;</span><br>          <span class="hljs-string">f&#x27;test acc <span class="hljs-subst">&#123;test_acc:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;metric[<span class="hljs-number">2</span>] * num_epochs / timer.<span class="hljs-built_in">sum</span>():<span class="hljs-number">.1</span>f&#125;</span> examples/sec &#x27;</span><br>          <span class="hljs-string">f&#x27;on <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><ul><li><code>animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],legend=['train loss', 'train acc', 'test acc'])</code>此代码为绘制折线图</li><li>以上代码个 epoch结束时都会计算并得到一次测试集的准确率，并将其记录下来</li></ul><h2 id="最终得到训练结果">最终得到训练结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs = <span class="hljs-number">0.9</span>, <span class="hljs-number">10</span><br>train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><blockquote><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">loss</span> <span class="hljs-number">0</span>.<span class="hljs-number">469</span>, train acc <span class="hljs-number">0</span>.<span class="hljs-number">823</span>, test acc <span class="hljs-number">0</span>.<span class="hljs-number">779</span><br><span class="hljs-attribute">55296</span>.<span class="hljs-number">6</span> examples/sec <span class="hljs-literal">on</span> cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></blockquote>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>张量的基本操作</title>
    <link href="/2024/11/09/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <url>/2024/11/09/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="一.-张量的基础概念">一. 张量的基础概念</h2><p>在学深度学习里，<strong>Tensor实际上就是一个多维数组（multidimensionalarray）</strong>，是一种最基础的数据结构。</p><h2 id="二.-张量常用操作">二. 张量常用操作</h2><h4 id="访问某一个元素最后一个跳跃访问每三行访问一个元素每两列访问一个元素"><strong>访问某一个元素（最后一个跳跃访问，每三行访问一个元素，每两列访问一个元素）</strong></h4><figure><img src="image-20241109210857386.png" alt="image-20241109210857386"><figcaption aria-hidden="true">image-20241109210857386</figcaption></figure><p><strong>构造一个张量：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>a = torch.ones(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p><strong>使用ndim查看张量的维度：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>t1.ndim <br></code></pre></td></tr></table></figure><p>输出：1</p><p><strong>使用shape查看形状</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>t1.shape<br></code></pre></td></tr></table></figure><p><strong>由两个形状相同的二维数组创建一个三维的张量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a1 = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">4</span>]])<br>a2 = np.array([[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">8</span>]])<br>t3 = torch.tensor([a1,a2])<br></code></pre></td></tr></table></figure><p>输出：结果是一个三位向量</p><p>tensor([[[1, 2, 2], [3, 4, 4]], [[5, 6, 6], [7, 8, 8]]],dtype=torch.int32)</p><p><strong>flatten拉平，将任意维度张量转化为一维张量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t2 = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>t2.flatten()<br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([1, 2, 3, 4])</p><p><strong>reshape方法，任意变形</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>t1.reshape(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[1],[2]])</p><p>torch.Size([2, 1])</p><p><strong>特殊张量创建：全零张量 .zeros()、全1张量 .ones()、单位矩阵.eyes()、对角矩阵 .diag(一维张量)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.eye(<span class="hljs-number">5</span>)<br>t1 = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br>torch.diag(t1)<br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 1., 0.,0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]])</p><p>tensor([[1, 0],</p><p>[0, 2]])</p><p><strong>服从0-1均匀分布的张量rand</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<span class="hljs-comment">#2行3列的随机数所构成的张量</span><br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[-0.8110, -1.1295, -0.2913], [-1.1786, -0.8882, 0.2433]])</p><p><strong>arange/linspace:生成数列</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.arange(<span class="hljs-number">5</span>)<br>torch.arange(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>) <span class="hljs-comment">#从1到5，每隔0.5取一个数</span><br>torch.linspace(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>) <span class="hljs-comment">#从1取到5，等距取三个数</span><br></code></pre></td></tr></table></figure><p><strong>empty，初始化指定形状矩阵</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.full([<span class="hljs-number">2</span>,<span class="hljs-number">4</span>],<span class="hljs-number">2</span>) <span class="hljs-comment">#2行4列，数值为2</span><br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[2, 2, 2, 2], [2, 2, 2, 2]])</p><p><strong>创建指定形状的数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.full_like(t1,<span class="hljs-number">2</span>) <span class="hljs-comment">#根据t1的形状，填充数值2</span><br>torch.randint_like(t2,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>) <span class="hljs-comment">#在1到10中随机抽取一些整数，并将其填充进t2的形状中去</span><br></code></pre></td></tr></table></figure><p><strong>张量与其它相关类型(数组、列表)之间转化方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>])<br>t1.numpy()<br>t1.tolist()<br></code></pre></td></tr></table></figure><p>输出：</p><p>array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=int64)</p><p>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</p><p><strong>切片，一小点需要注意</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">t2[[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>],<span class="hljs-number">1</span>] <span class="hljs-comment"># 第一行和第3行的第2列元素</span><br></code></pre></td></tr></table></figure><p><strong>分块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tc = torch.chunk(t2,<span class="hljs-number">4</span>,dim=<span class="hljs-number">0</span>) <br></code></pre></td></tr></table></figure><p>输出： (tensor([[0, 1, 2]]), tensor([[3, 4, 5]]), tensor([[6, 7,8]]), tensor([[ 9, 10, 11]]))</p><p><strong>拆分：split函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t2 = torch.arange(<span class="hljs-number">12</span>).reshape(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>)<br>torch.split(t2,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>)  <br></code></pre></td></tr></table></figure><p>输出：</p><p>(tensor([[0, 1, 2], [3, 4, 5]]), tensor([[ 6, 7, 8], [ 9, 10,11]]))</p><p>torch.split(t2,[1,3],0)</p><p>[1,3]表示“第一块长度为1，第二块长度为3</p><p><strong>torch.cat的用法：多个张量合并在一起,默认按行(dim=0)【记住二维张量dim=0表示行】进行拼接，维度不匹配会报错</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">z = torch.arange(<span class="hljs-number">12</span>, dtype=torch.float32).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>k = torch.tensor([[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">1</span>]])<br>h = torch.cat((z,k), dim=<span class="hljs-number">0</span>), torch.cat((z,k), dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(h)<br><br>输出：<br>(tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>],<br>        [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">7.</span>],<br>        [ <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">11.</span>],<br>        [ <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>,  <span class="hljs-number">1.</span>]]),<br> tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">7.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">11.</span>,  <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>,  <span class="hljs-number">1.</span>]]))<br></code></pre></td></tr></table></figure><p>【-1】访问最后一个元素，【1：3】选择第一行和第二行的元素（左闭右开）</p><p><strong>张量的维度变化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>输出:<br>tensor([[ <span class="hljs-number">0.6480</span>, <span class="hljs-number">1.5947</span>, <span class="hljs-number">0.6264</span>, <span class="hljs-number">0.6051</span>],<br>[ <span class="hljs-number">1.6784</span>, <span class="hljs-number">0.2768</span>, -<span class="hljs-number">1.8780</span>, -<span class="hljs-number">0.1133</span>],<br>[-<span class="hljs-number">0.6442</span>, <span class="hljs-number">0.8570</span>, <span class="hljs-number">0.1677</span>, <span class="hljs-number">0.2378</span>]])<br>mask = x.ge(<span class="hljs-number">0.5</span>)<br>输出：<br>tensor([[ <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>],<br>[ <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>],<br>[<span class="hljs-literal">False</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]])<br><br>torch.masked_select(x,mask) <br>输出：<br>tensor([<span class="hljs-number">0.6480</span>, <span class="hljs-number">1.5947</span>, <span class="hljs-number">0.6264</span>, <span class="hljs-number">0.6051</span>, <span class="hljs-number">1.6784</span>, <span class="hljs-number">0.8570</span>])<br></code></pre></td></tr></table></figure><p><strong>删除不必要的维度</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.zeros(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)<br>torch.squeeze(t).shape<span class="hljs-comment">#剔除所有属性为1的维度</span><br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([3])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">b.squeeze(<span class="hljs-number">0</span>).shape <span class="hljs-comment">#挤压掉第0维</span><br>b.squeeze(-<span class="hljs-number">1</span>).shape  <span class="hljs-comment">#挤压掉最后一维</span><br>b.squeeze(<span class="hljs-number">1</span>).shape <span class="hljs-comment">#挤压掉第一维</span><br></code></pre></td></tr></table></figure><p><strong>unsqueeze手动升维</strong></p><p>调用方法1：torch.unsqueeze(t, dim=n)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.zeros(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 在第0位索引上升高一个维度变成五维</span><br>torch.unsqueeze(t,dim=<span class="hljs-number">0</span>).shape <br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([1, 1, 2, 1, 2])</p><p>调用方法2：a.unsqueeze(dim=n)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在0索引前面插入了一个额外的维度</span><br>a.unsqueeze(<span class="hljs-number">0</span>).shape <br><span class="hljs-comment"># 在末尾插入一个额外的维度，可以理解为像素的属性</span><br>a.unsqueeze(-<span class="hljs-number">1</span>).shape <br></code></pre></td></tr></table></figure><p><strong>expand:broadcasting #只改变理解方式，不增加数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">b = torch.rand(<span class="hljs-number">1</span>,<span class="hljs-number">32</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>b.expand(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>,<span class="hljs-number">14</span>,<span class="hljs-number">14</span>).shape   <br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 32, 14, 14])</p><p><strong>repeat的参数表示重复的次数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 4表示对0维重复4次，32表示对1维重复32次</span><br>b.repeat(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>).shape<br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 1024, 1, 1])</p><p><strong>矩阵转置</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>a.shape<br>a.t()<br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 3])</p><p><strong>维度转换</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># transpose实现维度两两交换</span><br>a = torch.rand(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)<br><span class="hljs-number">2</span> = a.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>).contiguous().view(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>*<span class="hljs-number">32</span>*<span class="hljs-number">32</span>).view(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>解释：</p><p>#transpose包含了要交换的两个维度[b,c,h,w]→[b,w,h,c]</p><p>#数据的维度顺序必须与存储顺序一致,用.contiguous把数据变成连续的</p><p>#.view(4,33232) [b,whc]</p><p>#.view(4,3,32,32) [b,w,h,c]</p><p>#.transpose(1,3) [b,c,g,w]</p><p><strong>permute维度转换</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">b = torch.rand(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">32</span>) <br>b.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>).shape    <br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 28, 32, 3])</p>]]></content>
    
    
    <categories>
      
      <category>pytorch 基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>使用卷积进行泛化</title>
    <link href="/2024/11/03/%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/"/>
    <url>/2024/11/03/%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="卷积神经网络基础概念">卷积神经网络基础概念</h2><p>卷积神经网络<em>(convolutional neuralnetwork，CNN)</em>是一种前馈神经网络，由一个或多个卷积层和顶端的全连通层，同时也包括关联权重和池化层。卷积具有两种性质：平移不变性和局部性。</p><p>重新考虑全连接层(全连接层的公式考虑为<span class="math inline">\(y =w^{T}X+b\)</span>)的概念，将输入和输出变形为矩阵，可以得到以下的公式：<span class="math display">\[[H]_{i,j} = [U]_{i,j}+ {\textstyle \sum_{k}^{}}  {\textstyle\sum_{l}^{}}[W] _{i,j,k,l}\times [X]_{k,l}\]</span> 其中公式解释如下：</p><ul><li><span class="math inline">\([H]_{i,j}\)</span>：表示隐藏层在位<span class="math inline">\((i,j)\)</span>的值。</li><li><span class="math inline">\([U]_{i，j}\)</span>表示一个偏置项</li><li><span class="math inline">\([W] _{i,j,k,l}\)</span>是一个四阶权重张量，表示输入像素<span class="math inline">\((k，l)\)</span>对隐藏层位置<span class="math inline">\((i,j)\)</span>的影响权重，<span class="math inline">\([W] _{i,j,k,l}\times[X]_{k,l}\)</span>表示像素<span class="math inline">\((K,L)\)</span>对隐藏层位置<span class="math inline">\((i,j)\)</span>的贡献。</li><li><span class="math inline">\([X]_{k,l}\)</span>是输入图像在位置<span class="math inline">\((i,j)\)</span>的像素位置。</li></ul><p>进行一个假设用<span class="math inline">\([V]_{i,j,a,b}\)</span>替换<span class="math inline">\([W]_{i,j,k,l}\)</span>，并假设：<span class="math inline">\(k =i+a\)</span>， <span class="math inline">\(l = i+a\)</span>，我们通过改变下标，得到全新的权重(v是w的重新索引)<span class="math inline">\(v_{i,j,a,b} =w_{i,j,i+a,j+b}\)</span>最终得到公式：</p><p><span class="math display">\[[H]_{i,j} = [U]_{i,j}+ {\textstyle \sum_{a}^{}}  {\textstyle\sum_{b}^{}}[V] _{i,j,a,b}\times [X]_{x+a,j+b}\]</span> 这实际上是一个典型的卷积操作，其中<span class="math inline">\([V] _{i,j,a,b}\)</span>是卷积核，<span class="math inline">\([X]_{x+a,j+b}\)</span>是图像在卷积核覆盖的区域内的像素。这种表示方式在图像处理和神经网络中被广泛使用，因为它利用图像的局部空间关系，同时减少模型的参数量。</p><h2 id="卷积的性质">卷积的性质</h2><h3 id="平移不变性">平移不变性</h3><p>引用上述第一个原则：平移不变性。及意味着检测对象在输入<span class="math inline">\(X\)</span>中的平移，应仅仅导致隐藏向量<span class="math inline">\(H\)</span>的平移，也就是说<span class="math inline">\(v\)</span>不应该依赖于<span class="math inline">\(j\)</span>。因此，公式可以简化为 <span class="math display">\[[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b}[\mathbf{X}]_{i+a, j+b}.\]</span></p><h3 id="局部性">局部性</h3><p>局部性，用来训练参数<span class="math inline">\([\mathbf{H}]_{i,j}\)</span>的相关信息，我们不应偏离到距<span class="math inline">\((i,j)\)</span>很远的地方。所以给出的解决方案是<strong><span class="math inline">\(|a|&gt; \Delta\)</span></strong>或者<span class="math inline">\(|b| &gt; \Delta\)</span>，设置<span class="math inline">\([\mathbf{V}]_{a, b} = 0\)</span>,及公式表示为：<span class="math display">\[[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b =-\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.\]</span></p><h2 id="卷积的数学公式">卷积的数学公式</h2><p>数学中，两个函数形如如下形式的(<span class="math inline">\(f, g:\mathbb{R}^d \to \mathbb{R}\)</span>)，定义为卷积： <span class="math display">\[(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z})d\mathbf{z}.\]</span> 卷积是把一个函数”翻转“并位移x，测量<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>之间的重叠。党为离散对象时，积分则变为求和，可以得到如下定义，该一维公式多用于文本、语言、时间序列的模型中。<span class="math display">\[(f * g)(i) = \sum_a f(a) g(i-a).\]</span> 以此类推，二维卷积的公式为： <span class="math display">\[(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).\]</span></p><h2 id="感受野的概念">感受野的概念</h2><p>卷积神经网络中每一层输出的特征图上的像素点在原始图像上映射的区域大小，<strong>第一层</strong>卷积层的输出特征图像素的<strong>感受野大小等于卷积核的大小</strong>，其它卷积层的输出特征的感受野的大小和它之前所有层的卷积核的大小和步长都有关。</p><figure><img src="image-20241107111532799.png" alt="感受野图例"><figcaption aria-hidden="true">感受野图例</figcaption></figure><p>感受野的计算公式： <span class="math display">\[r_{n} = r_{n-1}+(k_{n-1})\coprod_{i = 1}^{n-1}S_{i}\]</span> 其中<span class="math inline">\(n\ge 2\)</span></p><p><span class="math inline">\(k:\)</span>kernel size</p><p><span class="math inline">\(S_{n}:\)</span>Stride</p><p><span class="math inline">\(r_{n}:\)</span>receptive-field，感受野，n表示层数</p><figure><img src="image-20241107112843015.png" alt="感受野计算例子"><figcaption aria-hidden="true">感受野计算例子</figcaption></figure><p>计算过程：</p><p><span class="math inline">\(r_{1} = 11\)</span></p><p><span class="math inline">\(r_{2} = 11+(3-1)\times 4 =19\)</span></p><p><span class="math inline">\(r_{3} = 19+(5-1)\times 4\times 2 =51\)</span></p><h2 id="填充和步幅">填充和步幅</h2><p>卷积的输出形状填充<span class="math inline">\((padding)\)</span>和步幅<span class="math inline">\((stride)\)</span>的影响。</p><p><strong>填充</strong>，以下图为例在图像的边界填充元素：</p><figure><img src="image-20241108140413936.png" alt="在卷积周围填充0"><figcaption aria-hidden="true">在卷积周围填充0</figcaption></figure><p>如果我们添加<span class="math inline">\(p_{h}\)</span>行填充和<span class="math inline">\(p_{w}\)</span>列填充，则输出的形状为： <span class="math display">\[(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)。\]</span><strong>步幅</strong>，滑块滑动的距离被称为步幅，如下图为垂直步幅为3，水平步幅为2的二维互相关运算。</p><figure><img src="image-20241108142335406.png" alt="image-20241108142335406"><figcaption aria-hidden="true">image-20241108142335406</figcaption></figure><p>如果<span class="math inline">\(p_{h}\)</span>行填充和<span class="math inline">\(p_{w}\)</span>列填充，水平步幅为<span class="math inline">\(s_{w}\)</span>、垂直步幅为<span class="math inline">\(s_{h}\)</span>则输出的形状为： <span class="math display">\[\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times\lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.\]</span></p><h2 id="池化">池化</h2><p>卷积对位置信息较为敏感，用池化减少位置信息对卷积层：</p><figure><img src="image-20241108155255475.png" alt="最大池化层样例"><figcaption aria-hidden="true">最大池化层样例</figcaption></figure><p>简单池化层基础实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pool2d</span>(<span class="hljs-params">X, pool_size, mode=<span class="hljs-string">&#x27;max&#x27;</span></span>):<br>    <span class="hljs-comment"># 定义池化层的长、宽参数</span><br>    p_h, p_w = pool_size<br>    <span class="hljs-comment"># 初始化池化层的输出</span><br>    Y = torch.zeros((X.shape[<span class="hljs-number">0</span>] - p_h + <span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>] - p_w + <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;max&#x27;</span>:<br>                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="hljs-built_in">max</span>()<br>                <br>            <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;avg&#x27;</span>:<br>                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()<br></code></pre></td></tr></table></figure><h2 id="卷积的代码理解">卷积的代码理解</h2><p>一维卷积的运算：</p><figure><img src="image-20241107105657295.png" alt="单通道计算"><figcaption aria-hidden="true">单通道计算</figcaption></figure><p>多输入通道的计算：</p><figure><img src="image-20241107105809574.png" alt="两个输入通道的互相关计算"><figcaption aria-hidden="true">两个输入通道的互相关计算</figcaption></figure><p>定义一个卷积层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Conv2D</span>(nn.Block):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, kernel_size, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.weight = <span class="hljs-variable language_">self</span>.params.get(<span class="hljs-string">&#x27;weight&#x27;</span>, shape=kernel_size)<br>        <span class="hljs-variable language_">self</span>.bias = <span class="hljs-variable language_">self</span>.params.get(<span class="hljs-string">&#x27;bias&#x27;</span>, shape=(<span class="hljs-number">1</span>,))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> corr2d(x, <span class="hljs-variable language_">self</span>.weight.data()) + <span class="hljs-variable language_">self</span>.bias.data()<br></code></pre></td></tr></table></figure><p>定义一个可学习的卷积核：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核</span><br>conv2d = nn.Conv2D(<span class="hljs-number">1</span>, kernel_size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), use_bias=<span class="hljs-literal">False</span>)<br>conv2d.initialize()<br><br><span class="hljs-comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span><br><span class="hljs-comment"># 其中批量大小和通道数都为1</span><br><br><br>X = X.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>)<br>Y = Y.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>)<br>lr = <span class="hljs-number">3e-2</span>  <span class="hljs-comment"># 学习率</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    <span class="hljs-keyword">with</span> autograd.record():<br>        Y_hat = conv2d(X)<br>        l = (Y_hat - Y) ** <span class="hljs-number">2</span><br>    l.backward()<br>    <span class="hljs-comment"># 迭代卷积核</span><br>    conv2d.weight.data()[:] -= lr * conv2d.weight.grad()<br>    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;epoch <span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>, loss <span class="hljs-subst">&#123;<span class="hljs-built_in">float</span>(l.<span class="hljs-built_in">sum</span>()):<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">epoch</span> <span class="hljs-number">2</span>, loss <span class="hljs-number">4</span>.<span class="hljs-number">949</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">4</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">831</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">6</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">140</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">8</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">024</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">10</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">004</span><span class="hljs-meta"></span><br><span class="hljs-meta">[07:16:32] ../src/base.cc:48: GPU context requested, but no GPUs found.</span><br></code></pre></td></tr></table></figure></blockquote><p>带有步幅和填充的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">conv2d = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), padding=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), stride=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>博客搭建</title>
    <link href="/2024/11/02/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    <url>/2024/11/02/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="一.-搭建准备">一. 搭建准备</h2><p>搭建之前需要准备的软件：</p><p><a href="https://nodejs.cn/">nodejs</a></p><p><a href="https://git-scm.com/">git</a> ，git安装详见网上安装步骤</p><h2 id="二-安装hexo完成简单本地页面展示">二，安装hexo，完成简单本地页面展示</h2><p>1.以管理员身份进入cmd窗口输入指令：</p><p><code>npm install -g hexo-cli</code></p><p>2.先创建一个文件夹myblog，在这个文件夹下直接右键git bash打开</p><p>然后初始化hexo</p><p><code>hexo init  // 初始化 hexo g  // 生成静态文件 hexo s  // 启动服务预览</code></p><p>在浏览器中输入http://localhost:4000/即可看到</p><p><em>新建完成后，指定文件夹目录下有：</em></p><ul><li>node_modules: 依赖包</li><li>public：存放生成的页面</li><li>scaffolds：生成文章的一些模板</li><li>source：用来存放你的文章 t</li><li>hemes：主题</li><li>_config.yml: 博的配置文件</li></ul><h2 id="三-变更主题">三， 变更主题</h2><p>在github上搜索：https://github.com/fluid-dev/hexo-theme-fluid，下载文件</p><figure><img src="image-20241102204608486.png" alt="image-20241102204608486"><figcaption aria-hidden="true">image-20241102204608486</figcaption></figure><p>将文件拷贝到博客根目录themes文件内，同时在根目录创建_config.fluid.yml文件，</p><figure><img src="image-20241102204855804.png" alt="image-20241102204855804"><figcaption aria-hidden="true">image-20241102204855804</figcaption></figure><p>然后使用记事本打开<code>Blog</code>文件中的<code>_config.yml</code>文件，更换主题名称：</p><figure><img src="image-20241102205130911.png" alt="image-20241102205130911"><figcaption aria-hidden="true">image-20241102205130911</figcaption></figure><h2 id="四.-配置ssh-key">四. 配置SSH Key</h2><p>鼠标右击打开<code>Git Bash</code></p><figure><img src="0ab87d6b4d864f97817f818d0f9b6948.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>若还没有user.name 和user.email，先配置</p><p><code>git config --global user.name "你的GitHub用户名" git config --global user.email "你的GitHub注册邮箱"</code></p><h3 id="生成ssh-key">生成ssh key</h3><p>使用下面命令生成ssh-key</p><p><code>ssh-keygen -t rsa -C "xxx@xxx.com"  // 将 "xxx@xxx.com" 替换为你自己GitHub的邮箱地址</code></p><p>然后一直按 “enter”键，如下图</p><figure><img src="image-20241102205558493.png" alt="image-20241102205558493"><figcaption aria-hidden="true">image-20241102205558493</figcaption></figure><p>然后到c盘目录（C:）下查找<strong>.shh</strong>文件，复制<strong>catid_rsa.pub</strong>文件内的密钥，将内容<strong>全部</strong>复制，找到GithubSetting keys页面，新建new SSH Key</p><figure><img src="image-20241102210255510.png" alt="image-20241102210255510"><figcaption aria-hidden="true">image-20241102210255510</figcaption></figure><h3 id="检查是否设置成功">检查是否设置成功</h3><p><code>$ ssh -T git@github.com</code></p><p>看到<strong>successfully</strong>字样就成功了</p><h2 id="五.-连接hexo与github">五. 连接Hexo与GitHub</h2><p>打开blog文件中的_config.yml（即站点配置文件），翻到最后修改为：</p><blockquote><p>deploy: type: gitrepository:git@github.com:github邮箱/github邮箱.github.io.git branch:main</p></blockquote><p><img src="image-20241102210556676.png"></p><p>在github上新建一个仓库</p><figure><img src="image-20241102211126683.png" alt="image-20241102211126683"><figcaption aria-hidden="true">image-20241102211126683</figcaption></figure><p>最后安装Git部署插件：（在根目录下执行，而不是在git bash里）</p><p><code>npm install hexo-deployer-git --save</code></p><p>这时再输入以下命令：</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">hexo c   #清除缓存文件 db<span class="hljs-variable">.json</span> 和已生成的静态文件 public<br>hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo <span class="hljs-keyword">generate</span> 的缩写)<br>hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)<br></code></pre></td></tr></table></figure><p><strong>现在就可以使用xxx.github.io来访问你的博客啦</strong></p><p><strong>例如：我的用户名是85941837，那么我的博客地址就是<code>85941837.github.io</code></strong></p><h2 id="六.-配置域名">六. 配置域名</h2><p>在华为云上购买域名，在查询上搜索<strong>域名</strong></p><figure><img src="image-20241102211321606.png" alt="image-20241102211321606"><figcaption aria-hidden="true">image-20241102211321606</figcaption></figure><p>购买域名，第一次购买需要填写信息模板</p><figure><img src="image-20241102211450237.png" alt="image-20241102211450237"><figcaption aria-hidden="true">image-20241102211450237</figcaption></figure><p>然后进行域名解析，</p><figure><img src="image-20241102211640695.png" alt="image-20241102211640695"><figcaption aria-hidden="true">image-20241102211640695</figcaption></figure><p>点击快速添加域名解析，选择网站解析中提供的cname域名，将github上的xxx.github.io填写上即可。</p><figure><img src="image-20241102211820124.png" alt="image-20241102211820124"><figcaption aria-hidden="true">image-20241102211820124</figcaption></figure><h2 id="七.-其他">七. 其他</h2><h3 id="hexo-d-部署后总需要重新改域名解决办法">hexo d部署后总需要重新改域名解决办法</h3><p>在source目录下（不是<a href="https://so.csdn.net/so/search?q=hexo&amp;spm=1001.2101.3001.7020">hexo</a>根目录下），创建一个CNAME文件，可以用sublime创建，然后保存成（Allfiles格式）CNAME文件里写自己新的域名</p><figure><img src="image-20241102212052474.png" alt="image-20241102212052474"><figcaption aria-hidden="true">image-20241102212052474</figcaption></figure><p>hexo g 重新生成一下</p><p>hexo d 部署到github上</p><h3 id="hexo-上传图片本地不加载预览">hexo 上传图片本地不加载预览</h3><p><strong>修改配置文件</strong>，<strong>修改站点配置</strong><code>_config.yml</code>，将<code>post_asset_folder</code> 设置为 <code>true</code>，</p><p>然后安装插件：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">npm install hexo-asset-<span class="hljs-built_in">image</span> -- <span class="hljs-built_in">save</span><br></code></pre></td></tr></table></figure><p>如果你使用的是插件，当生成预览的时候，可能依旧无法正常查看图片。</p><p>具体的修改也很简单，我们只需要到 <code>node_modules</code> 中找到<code>hexo-asset-image</code>，并将 58、89 行的</p><figure><img src="hexo-post-publish-2409.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>修改为：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$(this)<span class="hljs-selector-class">.attr</span>(<span class="hljs-string">&#x27;src&#x27;</span>, <span class="hljs-attribute">src</span>);<br>console<span class="hljs-selector-class">.info</span> &amp;&amp; console<span class="hljs-selector-class">.info</span>(<span class="hljs-string">&quot;update link as:--&gt;&quot;</span> + <span class="hljs-attribute">src</span>);<br></code></pre></td></tr></table></figure><p>然后就能正常显示图片了 ( * ^ _^ * )</p>]]></content>
    
    
    <categories>
      
      <category>软件安装</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
