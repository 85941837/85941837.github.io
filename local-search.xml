<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>循环神经网络</title>
    <link href="/2024/11/14/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/11/14/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p>本内容参考动手学深度学习<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="动手学深度学习">[1]</span></a></sup>。</p><p>在学习循环神经网络$(Recurrent Neural Network, RNN)$之前先了解一下序列模型，序列模型是专门用于处理和预测序列数据的模型。在自然语言处理、音频处理和时间序列处理有广泛的应用。</p><p>举例来说，用$x_{t}$表示价格，即在时间步$t \in \mathbb{Z}^+$，观察价格$x_{t}$，假设交易员想在$t$日预测股市的价格，可以表现为：<br>$$<br>x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1).<br>$$</p><h2 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h2><p><strong>自回归模型</strong>：假设在现实情况下$x_{t-1}, \ldots, x_1$是不必要的，因此只需要满足长度为$\tau$的时间跨度， 即使用观测序列$x_{t-1}, \ldots, x_{t-\tau}$。当下获得的最直接的好处就是参数的数量总是不变的，至少在$t &gt; \tau$是如此，这种模型被称为<strong>自回归模型</strong>$（autoregressive models）$，因为它对自己执行回归。</p><p><strong>隐变量自回归模型</strong>：是保留一些对过去观测的总结$h_{t}$， 并且同时更新预测$\hat{x}<em>t$和总结$h</em>{t}$。这就产生了基于$\hat{x}<em>t &#x3D; P(x_t \mid h</em>{t})$估计$x_t$,以及公式$h_t &#x3D; g(h_{t-1}, x_{t-1})$更新模型，因为$h_{t}$从未被观测到，这种模型被称为<strong>隐变量自回归模型</strong>$（latent autoregressive models）$</p><h2 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h2><p>在自回归模型的近似法中，使用$x_{t-1}, \ldots, x_{t-\tau}$来估计$x_{t}$ ，而不是$x_{t-1}, \ldots, x_1$，只要这种是近似精确的，我们就说满足马尔可夫条件。特殊的，如果$\tau$&lt;1得到一阶马尔可夫模型$first-order Markov model）$<br>$$<br>P(x_1, \ldots, x_T) &#x3D; \prod_{t&#x3D;1}^T P(x_t \mid x_{t-1}) \text{ 当 } P(x_1 \mid x_0) &#x3D; P(x_1).<br>$$<br>基于条件概率公式，我们可以写出：<br>$$<br>P(x_1, \ldots, x_T) &#x3D; \prod_{t&#x3D;T}^1 P(x_t \mid x_{t+1}, \ldots, x_T).<br>$$</p><h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br>d2l.DATA_HUB[<span class="hljs-string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="hljs-string">&#x27;timemachine.txt&#x27;</span>,<br>                                <span class="hljs-string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_time_machine</span>():  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(d2l.download(<span class="hljs-string">&#x27;time_machine&#x27;</span>), <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()<br>    <span class="hljs-keyword">return</span> [re.sub(<span class="hljs-string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, line).strip().lower() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br><br>lines = read_time_machine()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;# 文本总行数: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(lines)&#125;</span>&#x27;</span>)<br><span class="hljs-built_in">print</span>(lines[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(lines[<span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">Downloading ../data/timemachine.txt <span class="hljs-built_in">from</span> <span class="hljs-keyword">http</span>://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...<br><span class="hljs-comment"># 文本总行数: 3221</span><br><span class="hljs-keyword">the</span> <span class="hljs-built_in">time</span> machine <span class="hljs-keyword">by</span> h g wells<br>twinkled <span class="hljs-keyword">and</span> his usually pale face was flushed <span class="hljs-keyword">and</span> animated <span class="hljs-keyword">the</span><br></code></pre></td></tr></table></figure></blockquote><h3 id="词元化"><a href="#词元化" class="headerlink" title="词元化"></a>词元化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">lines, token=<span class="hljs-string">&#x27;word&#x27;</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># &#x27;word&#x27;：按单词拆分（默认值）</span><br>    <span class="hljs-keyword">if</span> token == <span class="hljs-string">&#x27;word&#x27;</span>:<br>        <span class="hljs-keyword">return</span> [line.split() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-comment"># &#x27;char&#x27;：按字符拆分</span><br>    <span class="hljs-keyword">elif</span> token == <span class="hljs-string">&#x27;char&#x27;</span>:<br>        <span class="hljs-keyword">return</span> [<span class="hljs-built_in">list</span>(line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;错误：未知词元类型：&#x27;</span> + token)<br><br>tokens = tokenize(lines)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">11</span>):<br>    <span class="hljs-built_in">print</span>(tokens[i])<br></code></pre></td></tr></table></figure><p>例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 示例文本</span><br>lines = [<br>    <span class="hljs-string">&quot;Hello world&quot;</span>,<br>    <span class="hljs-string">&quot;This is a test.&quot;</span>,<br>    <span class="hljs-string">&quot;Tokenize me!&quot;</span>,<br>    <span class="hljs-string">&quot;How are you?&quot;</span>,<br>    <span class="hljs-string">&quot;I hope this works!&quot;</span>,<br>    <span class="hljs-string">&quot;Let&#x27;s see the output.&quot;</span>,<br>    <span class="hljs-string">&quot;This function is simple.&quot;</span>,<br>    <span class="hljs-string">&quot;Please split by words.&quot;</span>,<br>    <span class="hljs-string">&quot;Test the character tokenization.&quot;</span>,<br>    <span class="hljs-string">&quot;What do you think?&quot;</span>,<br>    <span class="hljs-string">&quot;Enjoy learning!&quot;</span><br>]<br><br><span class="hljs-comment"># 按单词拆分</span><br>tokens = tokenize(lines, token=<span class="hljs-string">&#x27;word&#x27;</span>)<br><br><span class="hljs-comment"># 输出前 11 行拆分结果</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">11</span>):<br>    <span class="hljs-built_in">print</span>(tokens[i])<br><br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><p>[‘Hello’, ‘world’]<br>[‘This’, ‘is’, ‘a’, ‘test.’]<br>[‘Tokenize’, ‘me!’]<br>[‘How’, ‘are’, ‘you?’]<br>[‘I’, ‘hope’, ‘this’, ‘works!’]<br>[‘Let’s’, ‘see’, ‘the’, ‘output.’]<br>[‘This’, ‘function’, ‘is’, ‘simple.’]<br>[‘Please’, ‘split’, ‘by’, ‘words.’]<br>[‘Test’, ‘the’, ‘character’, ‘tokenization.’]<br>[‘What’, ‘do’, ‘you’, ‘think?’]<br>[‘Enjoy’, ‘learning!’]</p></blockquote><h3 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h3><p>将字符串类型得词元映射到从0开始的数字索引中。根据词元出现的频率，分配数字索引。语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。同时增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“<pad>”）； 序列开始词元（“<bos>”）； 序列结束词元（“<eos>”）。</eos></bos></pad></unk></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Vocab</span>:  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokens=<span class="hljs-literal">None</span>, min_freq=<span class="hljs-number">0</span>, reserved_tokens=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># token词元列表</span><br>        <span class="hljs-keyword">if</span> tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            tokens = []<br>        <span class="hljs-comment"># 保留词元列表</span><br>        <span class="hljs-keyword">if</span> reserved_tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            reserved_tokens = []<br>        <span class="hljs-comment"># 按出现频率排序</span><br>        counter = count_corpus(tokens)<br>        <span class="hljs-variable language_">self</span>._token_freqs = <span class="hljs-built_in">sorted</span>(counter.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>],<br>                                   reverse=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 未知词元的索引为0</span><br>        <span class="hljs-variable language_">self</span>.idx_to_token = [<span class="hljs-string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens<br>        <span class="hljs-variable language_">self</span>.token_to_idx = &#123;token: idx<br>                             <span class="hljs-keyword">for</span> idx, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.idx_to_token)&#125;<br>        <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._token_freqs:<br>            <span class="hljs-keyword">if</span> freq &lt; min_freq:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> token <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.token_to_idx:<br>                <span class="hljs-variable language_">self</span>.idx_to_token.append(token)<br>                <span class="hljs-variable language_">self</span>.token_to_idx[token] = <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.idx_to_token) - <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.idx_to_token)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, tokens</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(tokens, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.token_to_idx.get(tokens, <span class="hljs-variable language_">self</span>.unk)<br>        <span class="hljs-keyword">return</span> [<span class="hljs-variable language_">self</span>.__getitem__(token) <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">to_tokens</span>(<span class="hljs-params">self, indices</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(indices, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.idx_to_token[indices]<br>        <span class="hljs-keyword">return</span> [<span class="hljs-variable language_">self</span>.idx_to_token[index] <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> indices]<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unk</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 未知词元的索引为0</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">token_freqs</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._token_freqs<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_corpus</span>(<span class="hljs-params">tokens</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 这里的tokens是1D列表或2D列表</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">isinstance</span>(tokens[<span class="hljs-number">0</span>], <span class="hljs-built_in">list</span>):<br>        <span class="hljs-comment"># 将词元列表展平成一个列表</span><br>        tokens = [token <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">return</span> collections.Counter(tokens)<br></code></pre></td></tr></table></figure><h3 id="整合所有功能"><a href="#整合所有功能" class="headerlink" title="整合所有功能"></a>整合所有功能</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_corpus_time_machine</span>(<span class="hljs-params">max_tokens=-<span class="hljs-number">1</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span><br>    lines = read_time_machine()<br>    tokens = tokenize(lines, <span class="hljs-string">&#x27;char&#x27;</span>)<br>    vocab = Vocab(tokens)<br>    <span class="hljs-comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span><br>    <span class="hljs-comment"># 所以将所有文本行展平到一个列表中</span><br>    corpus = [vocab[token] <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">if</span> max_tokens &gt; <span class="hljs-number">0</span>:<br>        corpus = corpus[:max_tokens]<br>    <span class="hljs-keyword">return</span> corpus, vocab<br><br>corpus, vocab = load_corpus_time_machine()<br><span class="hljs-built_in">len</span>(corpus), <span class="hljs-built_in">len</span>(vocab)<br></code></pre></td></tr></table></figure><h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>根据上面提到的自回归模型和马尔可夫模型，将语言模型定义为：<br>$$<br>P(x_1, x_2, \ldots, x_T) &#x3D; \prod_{t&#x3D;1}^T P(x_t  \mid  x_1, \ldots, x_{t-1}).<br>$$<br>由此，预测四个单词的文本序列的概率为：<br>$$<br>P(\text{deep}, \text{learning}, \text{is}, \text{fun}) &#x3D;  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is}).<br>$$<br>为了训练语言模型，需要计算单词的概率， 以及给定前面几个单词后出现某个单词的条件概率。训练中次的出现概率可表示为，其中$n(x)$和$n(x,x′)$分别是单个单词和连续单词对的出现次数。<br>$$<br>\hat{P}(\text{learning} \mid \text{deep}) &#x3D; \frac{n(\text{deep, learning})}{n(\text{deep})},<br>$$<br>通常，连续单词对要比“deep learning”出现频率要低很多，所以对于不常见的组合，要想找到足够的出现次数来获得估计会非常不容易。三个以上的单词组合，预测的结果会更差。为了缓解这种问题，Wood, F等人<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wood, F., Gasthaus, J., Archambeau, C., James, L., & Teh, Y. W. (2011). The sequence memoizer. *Communications of the ACM*, *54*(2), 91–98.">[2]</span></a></sup>提出如下公式。其中$\epsilon_1,\epsilon_2，\epsilon_3$为超参数， 以$\epsilon_1$为例：当$\epsilon_1 &#x3D;0$时，不应用平滑； 当ϵ1接近正无穷大时，$\hat{P}(x)$接近均匀概率分布1&#x2F;m。<br>$$<br>\begin{split}\begin{aligned}<br>    \hat{P}(x) &amp; &#x3D; \frac{n(x) + \epsilon_1&#x2F;m}{n + \epsilon_1}, \<br>    \hat{P}(x’ \mid x) &amp; &#x3D; \frac{n(x, x’) + \epsilon_2 \hat{P}(x’)}{n(x) + \epsilon_2}, \<br>    \hat{P}(x’’ \mid x,x’) &amp; &#x3D; \frac{n(x, x’,x’’) + \epsilon_3 \hat{P}(x’’)}{n(x, x’) + \epsilon_3}.<br>\end{aligned}\end{split}<br>$$<br>然而，这样的模型很容易变得无效，原因如下： 首先，我们需要存储所有的计数； 其次，这完全忽略了单词的意思。 例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中， 但是想根据上下文调整这类模型其实是相当困难的。 最后，长单词序列大部分是没出现过的， 因此一个模型如果只是简单地统计先前“看到”的单词序列频率， 那么模型面对这种问题肯定是表现不佳的。</p><h3 id="马尔可夫模型与n元语法"><a href="#马尔可夫模型与n元语法" class="headerlink" title="马尔可夫模型与n元语法"></a>马尔可夫模型与n元语法</h3><p> 如果$P(x_{t+1} \mid x_t, \ldots, x_1) &#x3D; P(x_{t+1} \mid x_t)$， 则序列上的分布满足一阶马尔可夫性质。 阶数越高，对应的依赖关系就越长。</p><p>$$<br>\begin{split}\begin{aligned}<br>P(x_1, x_2, x_3, x_4) &amp;&#x3D;  P(x_1) P(x_2) P(x_3) P(x_4),\<br>P(x_1, x_2, x_3, x_4) &amp;&#x3D;  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\<br>P(x_1, x_2, x_3, x_4) &amp;&#x3D;  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).<br>\end{aligned}\end{split}<br>$$</p><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p><em>循环神经网络</em>（recurrent neural networks，RNNs） 是具有隐状态的神经网络。与感知机不同的是，保存了前一个时间步的隐藏变量$\mathbf{H}<em>{t-1}$,并引入了一个新的权重参数$\mathbf{W}</em>{hh} \in \mathbb{R}^{h \times h}$当前时间步隐藏变量由当前时间步的输入 与前一个时间步的隐藏变量一起计算得出：</p><p><img src="image-20241116184514051.png" alt="image-20241116184514051"><br>$$<br>\mathbf{H}<em>t &#x3D; \phi(\mathbf{X}<em>t \mathbf{W}</em>{xh} + \mathbf{H}</em>{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h).<br>$$</p><p>$$<br>\mathbf{O}_t &#x3D; \mathbf{H}<em>t \mathbf{W}</em>{hq} + \mathbf{b}_q.<br>$$</p><h3 id="循环神经网络的损失函数-困惑度"><a href="#循环神经网络的损失函数-困惑度" class="headerlink" title="循环神经网络的损失函数(困惑度)"></a>循环神经网络的损失函数(困惑度)</h3><p>$$<br>\frac{1}{n} \sum_{t&#x3D;1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),<br>$$</p><p>其中P表示语言模型的预测概率，$x_{t}$表示真实值。</p><h3 id="循环神经网络的应用"><a href="#循环神经网络的应用" class="headerlink" title="循环神经网络的应用"></a>循环神经网络的应用</h3><p><img src="image-20241116185023652.png" alt="image-20241116185023652"></p><h3 id="代码实现-手动"><a href="#代码实现-手动" class="headerlink" title="代码实现(手动)"></a>代码实现(手动)</h3><h4 id="整体模型架构"><a href="#整体模型架构" class="headerlink" title="整体模型架构"></a>整体模型架构</h4><img src="%E7%BB%98%E5%9B%BE1.png" alt="绘图1" style="zoom:150%;"><h4 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>batch_size, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">35</span><br>train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)<br></code></pre></td></tr></table></figure><h4 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_params</span>(<span class="hljs-params">vocab_size, num_hiddens, device</span>):<br>    num_inputs = num_outputs = vocab_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">normal</span>(<span class="hljs-params">shape</span>):<br>        <span class="hljs-keyword">return</span> torch.randn(size=shape, device=device) * <span class="hljs-number">0.01</span><br><br>    <span class="hljs-comment"># 隐藏层参数</span><br>    W_xh = normal((num_inputs, num_hiddens))<br>    W_hh = normal((num_hiddens, num_hiddens))<br>    b_h = torch.zeros(num_hiddens, device=device)<br>    <span class="hljs-comment"># 输出层参数</span><br>    W_hq = normal((num_hiddens, num_outputs))<br>    b_q = torch.zeros(num_outputs, device=device)<br>    <span class="hljs-comment"># 附加梯度</span><br>    params = [W_xh, W_hh, b_h, W_hq, b_q]<br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>        param.requires_grad_(<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> params<br></code></pre></td></tr></table></figure><h4 id="在一个时间步内计算隐状态和输出"><a href="#在一个时间步内计算隐状态和输出" class="headerlink" title="在一个时间步内计算隐状态和输出"></a>在一个时间步内计算隐状态和输出</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">rnn</span>(<span class="hljs-params">inputs, state, params</span>):<br>    <span class="hljs-comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span><br>    W_xh, W_hh, b_h, W_hq, b_q = params<br>    H, = state<br>    outputs = []<br>    <span class="hljs-comment"># X的形状：(批量大小，词表大小)</span><br>    <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> inputs:<br>        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)<br>        Y = torch.mm(H, W_hq) + b_q<br>        outputs.append(Y)<br>    <span class="hljs-keyword">return</span> torch.cat(outputs, dim=<span class="hljs-number">0</span>), (H,)<br></code></pre></td></tr></table></figure><h4 id="创建类包装函数"><a href="#创建类包装函数" class="headerlink" title="创建类包装函数"></a>创建类包装函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNModelScratch</span>: <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, num_hiddens, device,</span><br><span class="hljs-params">                 get_params, init_state, forward_fn</span>):<br>        <span class="hljs-variable language_">self</span>.vocab_size, <span class="hljs-variable language_">self</span>.num_hiddens = vocab_size, num_hiddens<br>        <span class="hljs-variable language_">self</span>.params = get_params(vocab_size, num_hiddens, device)<br>        <span class="hljs-variable language_">self</span>.init_state, <span class="hljs-variable language_">self</span>.forward_fn = init_state, forward_fn<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X, state</span>):<br>        X = F.one_hot(X.T, <span class="hljs-variable language_">self</span>.vocab_size).<span class="hljs-built_in">type</span>(torch.float32)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.forward_fn(X, state, <span class="hljs-variable language_">self</span>.params)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">begin_state</span>(<span class="hljs-params">self, batch_size, device</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.init_state(batch_size, <span class="hljs-variable language_">self</span>.num_hiddens, device)<br></code></pre></td></tr></table></figure><h4 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_ch8</span>(<span class="hljs-params">prefix, num_preds, net, vocab, device</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span><br>    state = net.begin_state(batch_size=<span class="hljs-number">1</span>, device=device)<br>    outputs = [vocab[prefix[<span class="hljs-number">0</span>]]]<br>    get_input = <span class="hljs-keyword">lambda</span>: torch.tensor([outputs[-<span class="hljs-number">1</span>]], device=device).reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> prefix[<span class="hljs-number">1</span>:]:  <span class="hljs-comment"># 预热期</span><br>        _, state = net(get_input(), state)<br>        outputs.append(vocab[y])<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_preds):  <span class="hljs-comment"># 预测num_preds步</span><br>        y, state = net(get_input(), state)<br>        outputs.append(<span class="hljs-built_in">int</span>(y.argmax(dim=<span class="hljs-number">1</span>).reshape(<span class="hljs-number">1</span>)))<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> outputs])<br></code></pre></td></tr></table></figure><h4 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h4><p>$$<br>\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{|\mathbf{g}|}\right) \mathbf{g}.<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad_clipping</span>(<span class="hljs-params">net, theta</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        params = [p <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> net.parameters() <span class="hljs-keyword">if</span> p.requires_grad]<br>    <span class="hljs-keyword">else</span>:<br>        params = net.params<br>    norm = torch.sqrt(<span class="hljs-built_in">sum</span>(torch.<span class="hljs-built_in">sum</span>((p.grad ** <span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> params))<br>    <span class="hljs-keyword">if</span> norm &gt; theta:<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>            param.grad[:] *= theta / norm<br></code></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_epoch_ch8</span>(<span class="hljs-params">net, train_iter, loss, updater, device, use_random_iter</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span><br>    state, timer = <span class="hljs-literal">None</span>, d2l.Timer()<br>    metric = d2l.Accumulator(<span class="hljs-number">2</span>)  <span class="hljs-comment"># 训练损失之和,词元数量</span><br>    <span class="hljs-keyword">for</span> X, Y <span class="hljs-keyword">in</span> train_iter:<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> use_random_iter:<br>            <span class="hljs-comment"># 在第一次迭代或使用随机抽样时初始化state</span><br>            state = net.begin_state(batch_size=X.shape[<span class="hljs-number">0</span>], device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(state, <span class="hljs-built_in">tuple</span>):<br>                <span class="hljs-comment"># state对于nn.GRU是个张量</span><br>                state.detach_()<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span><br>                <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> state:<br>                    s.detach_()<br>        y = Y.T.reshape(-<span class="hljs-number">1</span>)<br>        X, y = X.to(device), y.to(device)<br>        y_hat, state = net(X, state)<br>        l = loss(y_hat, y.long()).mean()<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(updater, torch.optim.Optimizer):<br>            updater.zero_grad()<br>            l.backward()<br>            grad_clipping(net, <span class="hljs-number">1</span>)<br>            updater.step()<br>        <span class="hljs-keyword">else</span>:<br>            l.backward()<br>            grad_clipping(net, <span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># 因为已经调用了mean函数</span><br>            updater(batch_size=<span class="hljs-number">1</span>)<br>        metric.add(l * y.numel(), y.numel())<br>    <span class="hljs-keyword">return</span> math.exp(metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]), metric[<span class="hljs-number">1</span>] / timer.stop()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_ch8</span>(<span class="hljs-params">net, train_iter, vocab, lr, num_epochs, device,</span><br><span class="hljs-params">              use_random_iter=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span><br>    loss = nn.CrossEntropyLoss()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, ylabel=<span class="hljs-string">&#x27;perplexity&#x27;</span>,<br>                            legend=[<span class="hljs-string">&#x27;train&#x27;</span>], xlim=[<span class="hljs-number">10</span>, num_epochs])<br>    <span class="hljs-comment"># 初始化</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        updater = torch.optim.SGD(net.parameters(), lr)<br>    <span class="hljs-keyword">else</span>:<br>        updater = <span class="hljs-keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)<br>    predict = <span class="hljs-keyword">lambda</span> prefix: predict_ch8(prefix, <span class="hljs-number">50</span>, net, vocab, device)<br>    <span class="hljs-comment"># 训练和预测</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        ppl, speed = train_epoch_ch8(<br>            net, train_iter, loss, updater, device, use_random_iter)<br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">&#x27;time traveller&#x27;</span>))<br>            animator.add(epoch + <span class="hljs-number">1</span>, [ppl])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;困惑度 <span class="hljs-subst">&#123;ppl:<span class="hljs-number">.1</span>f&#125;</span>, <span class="hljs-subst">&#123;speed:<span class="hljs-number">.1</span>f&#125;</span> 词元/秒 <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">&#x27;time traveller&#x27;</span>))<br>    <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">&#x27;traveller&#x27;</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs, lr = <span class="hljs-number">500</span>, <span class="hljs-number">1</span><br>train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())<br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vim">困惑度 <span class="hljs-number">1.0</span>, <span class="hljs-number">67212.6</span> 词元/秒 cud<span class="hljs-variable">a:0</span><br>time traveller <span class="hljs-keyword">for</span> <span class="hljs-keyword">so</span> it will <span class="hljs-keyword">be</span> convenient <span class="hljs-keyword">to</span> speak of himwas <span class="hljs-keyword">e</span><br>travelleryou can show black <span class="hljs-keyword">is</span> white by <span class="hljs-keyword">argument</span> said filby<br></code></pre></td></tr></table></figure></blockquote><h3 id="代码简洁实现"><a href="#代码简洁实现" class="headerlink" title="代码简洁实现"></a>代码简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>batch_size, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">35</span><br>train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">num_hiddens = <span class="hljs-number">256</span><br>rnn_layer = nn.RNN(<span class="hljs-built_in">len</span>(vocab), num_hiddens)<br><span class="hljs-comment"># 形状是（隐藏层数，批量大小，隐藏单元数）</span><br>state = torch.zeros((<span class="hljs-number">1</span>, batch_size, num_hiddens))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(num_steps, batch_size, <span class="hljs-built_in">len</span>(vocab)))<br>Y, state_new = rnn_layer(X, state)<br>Y.shape, state_new.shape<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNModel</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, rnn_layer, vocab_size, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(RNNModel, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.rnn = rnn_layer<br>        <span class="hljs-variable language_">self</span>.vocab_size = vocab_size<br>        <span class="hljs-variable language_">self</span>.num_hiddens = <span class="hljs-variable language_">self</span>.rnn.hidden_size<br>        <span class="hljs-comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1,需要构造自己的输出层</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.rnn.bidirectional:<br>            <span class="hljs-variable language_">self</span>.num_directions = <span class="hljs-number">1</span><br>            <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-variable language_">self</span>.num_hiddens, <span class="hljs-variable language_">self</span>.vocab_size)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.num_directions = <span class="hljs-number">2</span><br>            <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-variable language_">self</span>.num_hiddens * <span class="hljs-number">2</span>, <span class="hljs-variable language_">self</span>.vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, state</span>):<br>        X = F.one_hot(inputs.T.long(), <span class="hljs-variable language_">self</span>.vocab_size)<br>        X = X.to(torch.float32)<br>        Y, state = <span class="hljs-variable language_">self</span>.rnn(X, state)<br>        <span class="hljs-comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span><br>        <span class="hljs-comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span><br>        output = <span class="hljs-variable language_">self</span>.linear(Y.reshape((-<span class="hljs-number">1</span>, Y.shape[-<span class="hljs-number">1</span>])))<br>        <span class="hljs-keyword">return</span> output, state<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">begin_state</span>(<span class="hljs-params">self, device, batch_size=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(<span class="hljs-variable language_">self</span>.rnn, nn.LSTM):<br>            <span class="hljs-comment"># nn.GRU以张量作为隐状态</span><br>            <span class="hljs-keyword">return</span>  torch.zeros((<span class="hljs-variable language_">self</span>.num_directions * <span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                                 batch_size, <span class="hljs-variable language_">self</span>.num_hiddens),<br>                                device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># nn.LSTM以元组作为隐状态</span><br>            <span class="hljs-keyword">return</span> (torch.zeros((<br>                <span class="hljs-variable language_">self</span>.num_directions * <span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                batch_size, <span class="hljs-variable language_">self</span>.num_hiddens), device=device),<br>                    torch.zeros((<br>                        <span class="hljs-variable language_">self</span>.num_directions * <span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                        batch_size, <span class="hljs-variable language_">self</span>.num_hiddens), device=device))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">device = d2l.try_gpu()<br>net = RNNModel(rnn_layer, vocab_size=<span class="hljs-built_in">len</span>(vocab))<br>net = net.to(device)<br>d2l.predict_ch8(<span class="hljs-string">&#x27;time traveller&#x27;</span>, <span class="hljs-number">10</span>, net, vocab, device)<br></code></pre></td></tr></table></figure><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>动手学深度学习<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Wood, F., Gasthaus, J., Archambeau, C., James, L., &amp; Teh, Y. W. (2011). The sequence memoizer. <em>Communications of the ACM</em>, <em>54</em>(2), 91–98.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>pytorch 基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>卷积神经网络LeNet5</title>
    <link href="/2024/11/11/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CLeNet/"/>
    <url>/2024/11/11/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CLeNet/</url>
    
    <content type="html"><![CDATA[<p>本文使用<strong>LeNet5</strong>识别手写数字。</p><h2 id="LetNet-5-的基本结构"><a href="#LetNet-5-的基本结构" class="headerlink" title="LetNet-5 的基本结构"></a>LetNet-5 的基本结构</h2><p>LeNet-5包含7层网络结构（不含输入层），包含两个卷积层、两个降采样层（池化层）、两个全连接层和输出层。</p><p><img src="image-20241111161255154.png"></p><h3 id="1-输入层-input-layer"><a href="#1-输入层-input-layer" class="headerlink" title="1.输入层(input layer)"></a>1.输入层(input layer)</h3><p>输入层的大小为32×32手写图像，在实际应用中，通常会对图像进行预处理，如对像素进行归一化。</p><h3 id="2、卷积层C1（Convolutional-layer-C1）"><a href="#2、卷积层C1（Convolutional-layer-C1）" class="headerlink" title="2、卷积层C1（Convolutional layer C1）"></a>2、卷积层C1（Convolutional layer C1）</h3><p><img src="image-20241111164556129.png"></p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>根据代码的含义，通道数从<strong>1变6</strong>，原图的尺寸从32×32变为28×28，这与卷积核大小、步幅和$padding$有关，其输出的特征图像尺寸如下：<br>$$<br>output size &#x3D; \frac{W-kernelsize+2×padding}{stride}+1<br>$$<br>其中，$W$表示输入图像的宽度。<br>$$<br>output size &#x3D; \frac{32-5+2×0}{1}+1 &#x3D; 28<br>$$<br>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Sigmoid()<br></code></pre></td></tr></table></figure><p>在卷积操作完成之后会进行<strong>归一化（Batch Normalization）</strong>操作提高神经网络和加速收敛</p><h3 id="3、采样层S2（Subsampling-layer-S2）"><a href="#3、采样层S2（Subsampling-layer-S2）" class="headerlink" title="3、采样层S2（Subsampling layer S2）"></a>3、采样层S2（Subsampling layer S2）</h3><p><img src="image-20241111164616015.png"></p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p>通道数没有发生过改变，计算公式如下：<br>$$<br>outputsize &#x3D;\frac{W-poolsize}{stride}+1 &#x3D; \frac{28-2}{2}+1 &#x3D; 14<br>$$<br>然后同上，进行批量归一化。</p><h3 id="4、卷积层C3（Convolutional-layer-C3）"><a href="#4、卷积层C3（Convolutional-layer-C3）" class="headerlink" title="4、卷积层C3（Convolutional layer C3）"></a>4、卷积层C3（Convolutional layer C3）</h3><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p>通道数由6变16，输出特征图尺寸为10x10，具体计算如下：<br>$$<br>output size &#x3D; \frac{W-kernelsize+2×padding}{stride}+1 &#x3D; \frac{14-5+2×0}{1}+1 &#x3D; 10<br>$$<br>该层较为特殊，16 个卷积核并不是都与 S2 的 6 个通道层进行卷积操作，如下图所示，C3 的前六个特征图（0,1,2,3,4,5）由 S2 的相邻三个特征图作为输入，对应的卷积核尺寸为：5x5x3；接下来的 6 个特征图（6,7,8,9,10,11）由 S2 的相邻四个特征图作为输入对应的卷积核尺寸为：5x5x4；接下来的 3 个特征图（12,13,14）号特征图由 S2 间断的四个特征图作为输入对应的卷积核尺寸为：5x5x4；最后的 15 号特征图由 S2 全部(6 个)特征图作为输入，对应的卷积核尺寸为：5x5x6。</p><p><img src="image-20241111170703849.png"></p><h3 id="5、采样层S4（Subsampling-layer-S4）"><a href="#5、采样层S4（Subsampling-layer-S4）" class="headerlink" title="5、采样层S4（Subsampling layer S4）"></a>5、采样层S4（Subsampling layer S4）</h3><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br></code></pre></td></tr></table></figure><p>特征图尺寸由10×10变成5×5，输出通道还是<strong>16</strong>。<br>$$<br>outputsize &#x3D; \frac{W-kernelsize}{stride}+1 &#x3D; \frac{10-2}{2} +1 &#x3D; 5<br>$$</p><h3 id="6、全连接层C5（Fully-connected-layer-C5）"><a href="#6、全连接层C5（Fully-connected-layer-C5）" class="headerlink" title="6、全连接层C5（Fully connected layer C5）"></a>6、全连接层C5（Fully connected layer C5）</h3><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br></code></pre></td></tr></table></figure><p>C5将每个大小为 5×5 的特征图拉成一个长度为400**(维度为1X16X5X5)**的向量，并通过一个带有120个神经元的全连接层进行连接。120是由LeNet-5的设计者根据实验得到的最佳值。</p><h3 id="7、全连接层F6（Fully-connected-layer-F6）"><a href="#7、全连接层F6（Fully-connected-layer-F6）" class="headerlink" title="7、全连接层F6（Fully connected layer F6）"></a>7、全连接层F6（Fully connected layer F6）</h3><p>全连接层F6将120个神经元连接到84个神经元。</p><h3 id="8、输出层（Output-layer）"><a href="#8、输出层（Output-layer）" class="headerlink" title="8、输出层（Output layer）"></a>8、输出层（Output layer）</h3><p>输出层由10个神经元组成，每个神经元对应0-9中的一个数字，并输出最终的分类结果。</p><h2 id="搭建网络"><a href="#搭建网络" class="headerlink" title="搭建网络"></a>搭建网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>net = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载训练集</span><br>train_dataset = torchvision.datasets.MNIST(root = <span class="hljs-string">&#x27;./data&#x27;</span>,<span class="hljs-comment"># 数据集保存路径</span><br>                                           train = <span class="hljs-literal">True</span>,<span class="hljs-comment"># 是否为训练集</span><br>                                           <span class="hljs-comment"># 数据预处理</span><br>                                           transform = transforms.Compose([<br>                                                  transforms.Resize((<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)),<br>                                                  transforms.ToTensor(),<br>                                                  transforms.Normalize(mean = (<span class="hljs-number">0.1307</span>,), <br>                     std = (<span class="hljs-number">0.3081</span>,))]),<br>                                           download = <span class="hljs-literal">True</span>)<span class="hljs-comment">#是否下载</span><br> <br><span class="hljs-comment"># 加载测试集</span><br>test_dataset = torchvision.datasets.MNIST(root = <span class="hljs-string">&#x27;./data&#x27;</span>,<br>                                          train = <span class="hljs-literal">False</span>,<br>                                          transform = transforms.Compose([<br>                                                  transforms.Resize((<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)),<br>                                                  transforms.ToTensor(),<br>                                                  transforms.Normalize(mean = (<span class="hljs-number">0.1325</span>,), <br>                     std = (<span class="hljs-number">0.3105</span>,))]),<br>                                          download=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 一次抓64张牌</span><br>batch_size = <span class="hljs-number">64</span><br><span class="hljs-comment"># 加载训练数据</span><br>train_loader = torch.utils.data.DataLoader(dataset = train_dataset,<br>                                           batch_size = batch_size,<br>                                           shuffle = <span class="hljs-literal">True</span>)<span class="hljs-comment"># 是否打乱</span><br><span class="hljs-comment"># 加载测试数据</span><br>test_loader = torch.utils.data.DataLoader(dataset = test_dataset,<br>                                           batch_size = batch_size,<br>                                           shuffle = <span class="hljs-literal">False</span>)<span class="hljs-comment"># 是否打乱</span><br></code></pre></td></tr></table></figure><ul><li>测试阶段的<code>shuffle=False</code>：在测试阶段，通常不需要打乱数据的顺序。测试时模型是在未见过的数据上进行评估，因此希望模型看到的是原始数据的有序顺序，以便能够更好地评估模型的泛化性能。如果在测试时也打乱数据，可能会导致模型在评估时看到的数据分布与实际场景不一致。（其实如果是<code>True</code>影响也不大）</li></ul><h2 id="设置评价指标"><a href="#设置评价指标" class="headerlink" title="设置评价指标"></a>设置评价指标</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_accuracy_gpu</span>(<span class="hljs-params">net, data_iter, device=<span class="hljs-literal">None</span></span>): <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        net.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 设置为评估模式</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> device:<br>            device = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(net.parameters())).device<br>    <span class="hljs-comment"># 正确预测的数量，总预测的数量</span><br>    metric = d2l.Accumulator(<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(X, <span class="hljs-built_in">list</span>):<br>                <span class="hljs-comment"># BERT微调所需的（之后将介绍）</span><br>                X = [x.to(device) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X]<br>            <span class="hljs-keyword">else</span>:<br>                X = X.to(device)<br>            y = y.to(device)<br>            metric.add(d2l.accuracy(net(X), y), y.numel())<br>    <span class="hljs-keyword">return</span> metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_ch6</span>(<span class="hljs-params">net, train_iter, test_iter, num_epochs, lr, device</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear <span class="hljs-keyword">or</span> <span class="hljs-built_in">type</span>(m) == nn.Conv2d:<br>            nn.init.xavier_uniform_(m.weight)<br>    net.apply(init_weights)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;training on&#x27;</span>, device)<br>    net.to(device)<br>    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<br>    loss = nn.CrossEntropyLoss()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, xlim=[<span class="hljs-number">1</span>, num_epochs],<br>                            legend=[<span class="hljs-string">&#x27;train loss&#x27;</span>, <span class="hljs-string">&#x27;train acc&#x27;</span>, <span class="hljs-string">&#x27;test acc&#x27;</span>])<br>    timer, num_batches = d2l.Timer(), <span class="hljs-built_in">len</span>(train_iter)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        <span class="hljs-comment"># 训练损失之和，训练准确率之和，样本数</span><br>        metric = d2l.Accumulator(<span class="hljs-number">3</span>)<br>        net.train()<br>        <span class="hljs-keyword">for</span> i, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_iter):<br>            timer.start()<br>            optimizer.zero_grad()<br>            X, y = X.to(device), y.to(device)<br>            y_hat = net(X)<br>            l = loss(y_hat, y)<br>            l.backward()<br>            optimizer.step()<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                metric.add(l * X.shape[<span class="hljs-number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="hljs-number">0</span>])<br>            timer.stop()<br>            train_l = metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">2</span>]<br>            train_acc = metric[<span class="hljs-number">1</span>] / metric[<span class="hljs-number">2</span>]<br>            <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % (num_batches // <span class="hljs-number">5</span>) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> i == num_batches - <span class="hljs-number">1</span>:<br>                animator.add(epoch + (i + <span class="hljs-number">1</span>) / num_batches,<br>                             (train_l, train_acc, <span class="hljs-literal">None</span>))<br>        test_acc = evaluate_accuracy_gpu(net, test_iter)<br>        animator.add(epoch + <span class="hljs-number">1</span>, (<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, test_acc))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;loss <span class="hljs-subst">&#123;train_l:<span class="hljs-number">.3</span>f&#125;</span>, train acc <span class="hljs-subst">&#123;train_acc:<span class="hljs-number">.3</span>f&#125;</span>, &#x27;</span><br>          <span class="hljs-string">f&#x27;test acc <span class="hljs-subst">&#123;test_acc:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;metric[<span class="hljs-number">2</span>] * num_epochs / timer.<span class="hljs-built_in">sum</span>():<span class="hljs-number">.1</span>f&#125;</span> examples/sec &#x27;</span><br>          <span class="hljs-string">f&#x27;on <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><ul><li><code>animator = d2l.Animator(xlabel=&#39;epoch&#39;, xlim=[1, num_epochs],legend=[&#39;train loss&#39;, &#39;train acc&#39;, &#39;test acc&#39;])</code>此代码为绘制折线图</li><li>以上代码个 epoch 结束时都会计算并得到一次测试集的准确率，并将其记录下来</li></ul><h2 id="最终得到训练结果"><a href="#最终得到训练结果" class="headerlink" title="最终得到训练结果"></a>最终得到训练结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs = <span class="hljs-number">0.9</span>, <span class="hljs-number">10</span><br>train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><blockquote><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">loss</span> <span class="hljs-number">0</span>.<span class="hljs-number">469</span>, train acc <span class="hljs-number">0</span>.<span class="hljs-number">823</span>, test acc <span class="hljs-number">0</span>.<span class="hljs-number">779</span><br><span class="hljs-attribute">55296</span>.<span class="hljs-number">6</span> examples/sec <span class="hljs-literal">on</span> cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></blockquote>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>张量的基本操作</title>
    <link href="/2024/11/09/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <url>/2024/11/09/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="一-张量的基础概念"><a href="#一-张量的基础概念" class="headerlink" title="一. 张量的基础概念"></a>一. 张量的基础概念</h2><p>在学深度学习里，<strong>Tensor实际上就是一个多维数组（multidimensional array）</strong>，是一种最基础的数据结构。</p><h2 id="二-张量常用操作"><a href="#二-张量常用操作" class="headerlink" title="二. 张量常用操作"></a>二. 张量常用操作</h2><h4 id="访问某一个元素（最后一个跳跃访问，每三行访问一个元素，每两列访问一个元素）"><a href="#访问某一个元素（最后一个跳跃访问，每三行访问一个元素，每两列访问一个元素）" class="headerlink" title="访问某一个元素（最后一个跳跃访问，每三行访问一个元素，每两列访问一个元素）"></a><strong>访问某一个元素（最后一个跳跃访问，每三行访问一个元素，每两列访问一个元素）</strong></h4><p><img src="image-20241109210857386.png" alt="image-20241109210857386"></p><p><strong>构造一个张量：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>a = torch.ones(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p><strong>使用ndim查看张量的维度：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>t1.ndim <br></code></pre></td></tr></table></figure><p>输出：1</p><p><strong>使用shape查看形状</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>t1.shape<br></code></pre></td></tr></table></figure><p><strong>由两个形状相同的二维数组创建一个三维的张量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a1 = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">4</span>]])<br>a2 = np.array([[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">8</span>]])<br>t3 = torch.tensor([a1,a2])<br></code></pre></td></tr></table></figure><p>输出：结果是一个三位向量</p><p>tensor([[[1, 2, 2],<br>[3, 4, 4]],<br>[[5, 6, 6],<br>[7, 8, 8]]], dtype&#x3D;torch.int32)</p><p><strong>flatten拉平，将任意维度张量转化为一维张量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t2 = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>t2.flatten()<br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([1, 2, 3, 4])</p><p><strong>reshape方法，任意变形</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>t1.reshape(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[1],[2]])</p><p>torch.Size([2, 1])</p><p><strong>特殊张量创建：****全零张量 .zeros()、全1张量 .ones()、单位矩阵 .eyes()、对角矩阵 .diag(一维张量)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.eye(<span class="hljs-number">5</span>)<br>t1 = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br>torch.diag(t1)<br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[1., 0., 0., 0., 0.],<br>[0., 1., 0., 0., 0.],<br>[0., 0., 1., 0., 0.],<br>[0., 0., 0., 1., 0.],<br>[0., 0., 0., 0., 1.]])</p><p>tensor([[1, 0],</p><p>[0, 2]])</p><p><strong>服从0-1均匀分布的张量rand</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<span class="hljs-comment">#2行3列的随机数所构成的张量</span><br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[-0.8110, -1.1295, -0.2913],<br>[-1.1786, -0.8882, 0.2433]])</p><p><strong>arange&#x2F;linspace:生成数列</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.arange(<span class="hljs-number">5</span>)<br>torch.arange(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>) <span class="hljs-comment">#从1到5，每隔0.5取一个数</span><br>torch.linspace(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>) <span class="hljs-comment">#从1取到5，等距取三个数</span><br></code></pre></td></tr></table></figure><p><strong>empty，初始化指定形状矩阵</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.full([<span class="hljs-number">2</span>,<span class="hljs-number">4</span>],<span class="hljs-number">2</span>) <span class="hljs-comment">#2行4列，数值为2</span><br></code></pre></td></tr></table></figure><p>输出：</p><p>tensor([[2, 2, 2, 2],<br>[2, 2, 2, 2]])</p><p><strong>创建指定形状的数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.full_like(t1,<span class="hljs-number">2</span>) <span class="hljs-comment">#根据t1的形状，填充数值2</span><br>torch.randint_like(t2,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>) <span class="hljs-comment">#在1到10中随机抽取一些整数，并将其填充进t2的形状中去</span><br></code></pre></td></tr></table></figure><p><strong>张量与其它相关类型(数组、列表)之间转化方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>])<br>t1.numpy()<br>t1.tolist()<br></code></pre></td></tr></table></figure><p>输出：</p><p>array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype&#x3D;int64)</p><p>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</p><p><strong>切片，一小点需要注意</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">t2[[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>],<span class="hljs-number">1</span>] <span class="hljs-comment"># 第一行和第3行的第2列元素</span><br></code></pre></td></tr></table></figure><p><strong>分块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tc = torch.chunk(t2,<span class="hljs-number">4</span>,dim=<span class="hljs-number">0</span>) <br></code></pre></td></tr></table></figure><p>输出：<br>(tensor([[0, 1, 2]]),<br>tensor([[3, 4, 5]]),<br>tensor([[6, 7, 8]]),<br>tensor([[ 9, 10, 11]]))</p><p><strong>拆分****：split函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t2 = torch.arange(<span class="hljs-number">12</span>).reshape(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>)<br>torch.split(t2,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>)  <br></code></pre></td></tr></table></figure><p>输出：</p><p>(tensor([[0, 1, 2],<br>[3, 4, 5]]),<br>tensor([[ 6, 7, 8],<br>[ 9, 10, 11]]))</p><p>torch.split(t2,[1,3],0)</p><p>[1,3]表示“第一块长度为1，第二块长度为3</p><p><strong>torch.cat 的用法：多个张量合并在一起,默认按行(dim&#x3D;0)【记住二维张量dim&#x3D;0表示行】进行拼接，维度不匹配会报错</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">z = torch.arange(<span class="hljs-number">12</span>, dtype=torch.float32).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>k = torch.tensor([[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">1</span>]])<br>h = torch.cat((z,k), dim=<span class="hljs-number">0</span>), torch.cat((z,k), dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(h)<br><br>输出：<br>(tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>],<br>        [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">7.</span>],<br>        [ <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">11.</span>],<br>        [ <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>,  <span class="hljs-number">1.</span>]]),<br> tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">7.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">11.</span>,  <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>,  <span class="hljs-number">9.</span>,  <span class="hljs-number">1.</span>]]))<br></code></pre></td></tr></table></figure><p>【-1】访问最后一个元素，【1：3】选择第一行和第二行的元素（左闭右开）</p><p><strong>张量的维度变化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>输出:<br>tensor([[ <span class="hljs-number">0.6480</span>, <span class="hljs-number">1.5947</span>, <span class="hljs-number">0.6264</span>, <span class="hljs-number">0.6051</span>],<br>[ <span class="hljs-number">1.6784</span>, <span class="hljs-number">0.2768</span>, -<span class="hljs-number">1.8780</span>, -<span class="hljs-number">0.1133</span>],<br>[-<span class="hljs-number">0.6442</span>, <span class="hljs-number">0.8570</span>, <span class="hljs-number">0.1677</span>, <span class="hljs-number">0.2378</span>]])<br>mask = x.ge(<span class="hljs-number">0.5</span>)<br>输出：<br>tensor([[ <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>],<br>[ <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>],<br>[<span class="hljs-literal">False</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]])<br><br>torch.masked_select(x,mask) <br>输出：<br>tensor([<span class="hljs-number">0.6480</span>, <span class="hljs-number">1.5947</span>, <span class="hljs-number">0.6264</span>, <span class="hljs-number">0.6051</span>, <span class="hljs-number">1.6784</span>, <span class="hljs-number">0.8570</span>])<br></code></pre></td></tr></table></figure><p><strong>删除不必要的维度</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.zeros(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)<br>torch.squeeze(t).shape<span class="hljs-comment">#剔除所有属性为1的维度</span><br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([3])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">b.squeeze(<span class="hljs-number">0</span>).shape <span class="hljs-comment">#挤压掉第0维</span><br>b.squeeze(-<span class="hljs-number">1</span>).shape  <span class="hljs-comment">#挤压掉最后一维</span><br>b.squeeze(<span class="hljs-number">1</span>).shape <span class="hljs-comment">#挤压掉第一维</span><br></code></pre></td></tr></table></figure><p><strong>unsqueeze手动升维</strong></p><p>调用方法1：torch.unsqueeze(t, dim&#x3D;n)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.zeros(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 在第0位索引上升高一个维度变成五维</span><br>torch.unsqueeze(t,dim=<span class="hljs-number">0</span>).shape <br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([1, 1, 2, 1, 2])</p><p>调用方法2：a.unsqueeze(dim&#x3D;n)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在0索引前面插入了一个额外的维度</span><br>a.unsqueeze(<span class="hljs-number">0</span>).shape <br><span class="hljs-comment"># 在末尾插入一个额外的维度，可以理解为像素的属性</span><br>a.unsqueeze(-<span class="hljs-number">1</span>).shape <br></code></pre></td></tr></table></figure><p><strong>expand:broadcasting #只改变理解方式，不增加数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">b = torch.rand(<span class="hljs-number">1</span>,<span class="hljs-number">32</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>b.expand(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>,<span class="hljs-number">14</span>,<span class="hljs-number">14</span>).shape   <br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 32, 14, 14])</p><p><strong>repeat的参数表示重复的次数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 4表示对0维重复4次，32表示对1维重复32次</span><br>b.repeat(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>).shape<br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 1024, 1, 1])</p><p><strong>矩阵转置</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>a.shape<br>a.t()<br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 3])</p><p><strong>维度转换</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># transpose实现维度两两交换</span><br>a = torch.rand(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)<br><span class="hljs-number">2</span> = a.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>).contiguous().view(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>*<span class="hljs-number">32</span>*<span class="hljs-number">32</span>).view(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>解释：</p><p>#transpose包含了要交换的两个维度[b,c,h,w]→[b,w,h,c]</p><p>#数据的维度顺序必须与存储顺序一致,用.contiguous把数据变成连续的</p><p>#.view(4,33232) [b,whc]</p><p>#.view(4,3,32,32) [b,w,h,c]</p><p>#.transpose(1,3) [b,c,g,w]</p><p><strong>permute维度转换</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">b = torch.rand(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">32</span>) <br>b.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>).shape    <br></code></pre></td></tr></table></figure><p>输出：</p><p>torch.Size([4, 28, 32, 3])</p>]]></content>
    
    
    <categories>
      
      <category>pytorch 基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>使用卷积进行泛化</title>
    <link href="/2024/11/03/%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/"/>
    <url>/2024/11/03/%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E6%B3%9B%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="卷积神经网络基础概念"><a href="#卷积神经网络基础概念" class="headerlink" title="卷积神经网络基础概念"></a>卷积神经网络基础概念</h2><p>卷积神经网络*(convolutional neural network，CNN)*是一种前馈神经网络，由一个或多个卷积层和顶端的全连通层，同时也包括关联权重和池化层。卷积具有两种性质：平移不变性和局部性。</p><p>重新考虑全连接层(全连接层的公式考虑为$y &#x3D; w^{T}X+b$)的概念，将输入和输出变形为矩阵，可以得到以下的公式：<br>$$<br>[H]<em>{i,j} &#x3D; [U]</em>{i,j}+ {\textstyle \sum_{k}^{}}  {\textstyle \sum_{l}^{}}[W] <em>{i,j,k,l}\times [X]</em>{k,l}<br>$$<br>其中公式解释如下：</p><ul><li>$[H]_{i,j}$：表示隐藏层在位$(i,j)$的值。</li><li>$[U]_{i，j}$表示一个偏置项</li><li>$[W] _{i,j,k,l}$ 是一个四阶权重张量，表示输入像素$(k，l)$对隐藏层位置$(i,j)$的影响权重，$[W] <em>{i,j,k,l}\times [X]</em>{k,l}$表示像素$(K,L)$对隐藏层位置$(i,j)$的贡献。</li><li>$[X]_{k,l}$是输入图像在位置$(i,j)$的像素位置。</li></ul><p>进行一个假设用$[V] <em>{i,j,a,b}$替换$[W] <em>{i,j,k,l}$，并假设：$k &#x3D; i+a$， $l &#x3D; i+a$ ，我们通过改变下标，得到全新的权重(v是w的重新索引)$v</em>{i,j,a,b} &#x3D; w</em>{i,j,i+a,j+b}$最终得到公式：</p><p>$$<br>[H]<em>{i,j} &#x3D; [U]</em>{i,j}+ {\textstyle \sum_{a}^{}}  {\textstyle \sum_{b}^{}}[V] <em>{i,j,a,b}\times [X]</em>{x+a,j+b}<br>$$<br>这实际上是一个典型的卷积操作，其中$[V] <em>{i,j,a,b}$是卷积核，$[X]</em>{x+a,j+b}$是图像在卷积核覆盖的区域内的像素。这种表示方式在图像处理和神经网络中被广泛使用，因为它利用图像的局部空间关系，同时减少模型的参数量。</p><h2 id="卷积的性质"><a href="#卷积的性质" class="headerlink" title="卷积的性质"></a>卷积的性质</h2><h3 id="平移不变性"><a href="#平移不变性" class="headerlink" title="平移不变性"></a>平移不变性</h3><p>引用上述第一个原则：平移不变性。及意味着检测对象在输入$X$中的平移，应仅仅导致隐藏向量$H$的平移，也就是说$v$不应该依赖于$j$。因此，公式可以简化为<br>$$<br>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum_a\sum_b [\mathbf{V}]</em>{a, b} [\mathbf{X}]_{i+a, j+b}.<br>$$</p><h3 id="局部性"><a href="#局部性" class="headerlink" title="局部性"></a>局部性</h3><p>局部性，用来训练参数$[\mathbf{H}]<em>{i, j}$的相关信息，我们不应偏离到距$(i,j)$很远的地方。所以给出的解决方案是**$|a|&gt; \Delta$**或者$|b| &gt; \Delta$，设置$[\mathbf{V}]</em>{a, b} &#x3D; 0$,及公式表示为：<br>$$<br>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum</em>{a &#x3D; -\Delta}^{\Delta} \sum_{b &#x3D; -\Delta}^{\Delta} [\mathbf{V}]<em>{a, b}  [\mathbf{X}]</em>{i+a, j+b}.<br>$$</p><h2 id="卷积的数学公式"><a href="#卷积的数学公式" class="headerlink" title="卷积的数学公式"></a>卷积的数学公式</h2><p>数学中，两个函数形如如下形式的($f, g: \mathbb{R}^d \to \mathbb{R}$)，定义为卷积：<br>$$<br>(f * g)(\mathbf{x}) &#x3D; \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.<br>$$<br>卷积是把一个函数”翻转“并位移x，测量$f$和$g$之间的重叠。党为离散对象时，积分则变为求和，可以得到如下定义，该一维公式多用于文本、语言、时间序列的模型中。<br>$$<br>(f * g)(i) &#x3D; \sum_a f(a) g(i-a).<br>$$<br>以此类推，二维卷积的公式为：<br>$$<br>(f * g)(i, j) &#x3D; \sum_a\sum_b f(a, b) g(i-a, j-b).<br>$$</p><h2 id="感受野的概念"><a href="#感受野的概念" class="headerlink" title="感受野的概念"></a>感受野的概念</h2><p>卷积神经网络中每一层输出的特征图上的像素点在原始图像上映射的区域大小，<strong>第一层</strong>卷积层的输出特征图像素的<strong>感受野大小等于卷积核的大小</strong>，其它卷积层的输出特征的感受野的大小和它之前所有层的卷积核的大小和步长都有关。</p><p><img src="image-20241107111532799.png" alt="感受野图例"></p><p>感受野的计算公式：<br>$$<br>r_{n} &#x3D; r_{n-1}+(k_{n-1})\coprod_{i &#x3D; 1}^{n-1}S_{i}<br>$$<br>其中$n\ge 2$</p><p>$k:$kernel size</p><p>$S_{n}:$Stride</p><p>$r_{n}:$receptive-field，感受野，n表示层数</p><p><img src="image-20241107112843015.png" alt="感受野计算例子"></p><p>计算过程：</p><p>$r_{1} &#x3D; 11$</p><p>$r_{2} &#x3D; 11+(3-1)\times 4 &#x3D; 19$</p><p>$r_{3} &#x3D; 19+(5-1)\times 4\times 2 &#x3D; 51$</p><h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>卷积的输出形状填充$(padding)$和步幅$(stride)$的影响。</p><p><strong>填充</strong>，以下图为例在图像的边界填充元素：</p><p><img src="image-20241108140413936.png" alt="在卷积周围填充0"></p><p>如果我们添加$p_{h}$行填充和$p_{w}$列填充，则输出的形状为：<br>$$<br>(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)。<br>$$<br><strong>步幅</strong>，滑块滑动的距离被称为步幅，如下图为垂直步幅为3，水平步幅为2的二维互相关运算。</p><p><img src="image-20241108142335406.png" alt="image-20241108142335406"></p><p>如果$p_{h}$行填充和$p_{w}$列填充，水平步幅为$s_{w}$、垂直步幅为$s_{h}$则输出的形状为：<br>$$<br>\lfloor(n_h-k_h+p_h+s_h)&#x2F;s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)&#x2F;s_w\rfloor.<br>$$</p><h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><p>卷积对位置信息较为敏感，用池化减少位置信息对卷积层：</p><p><img src="image-20241108155255475.png" alt="最大池化层样例"></p><p>简单池化层基础实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pool2d</span>(<span class="hljs-params">X, pool_size, mode=<span class="hljs-string">&#x27;max&#x27;</span></span>):<br>    <span class="hljs-comment"># 定义池化层的长、宽参数</span><br>    p_h, p_w = pool_size<br>    <span class="hljs-comment"># 初始化池化层的输出</span><br>    Y = torch.zeros((X.shape[<span class="hljs-number">0</span>] - p_h + <span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>] - p_w + <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;max&#x27;</span>:<br>                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="hljs-built_in">max</span>()<br>                <br>            <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;avg&#x27;</span>:<br>                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()<br></code></pre></td></tr></table></figure><h2 id="卷积的代码理解"><a href="#卷积的代码理解" class="headerlink" title="卷积的代码理解"></a>卷积的代码理解</h2><p>一维卷积的运算：</p><p><img src="image-20241107105657295.png" alt="单通道计算"></p><p>多输入通道的计算：</p><p><img src="image-20241107105809574.png" alt="两个输入通道的互相关计算"></p><p>定义一个卷积层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Conv2D</span>(nn.Block):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, kernel_size, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.weight = <span class="hljs-variable language_">self</span>.params.get(<span class="hljs-string">&#x27;weight&#x27;</span>, shape=kernel_size)<br>        <span class="hljs-variable language_">self</span>.bias = <span class="hljs-variable language_">self</span>.params.get(<span class="hljs-string">&#x27;bias&#x27;</span>, shape=(<span class="hljs-number">1</span>,))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> corr2d(x, <span class="hljs-variable language_">self</span>.weight.data()) + <span class="hljs-variable language_">self</span>.bias.data()<br></code></pre></td></tr></table></figure><p>定义一个可学习的卷积核：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核</span><br>conv2d = nn.Conv2D(<span class="hljs-number">1</span>, kernel_size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), use_bias=<span class="hljs-literal">False</span>)<br>conv2d.initialize()<br><br><span class="hljs-comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span><br><span class="hljs-comment"># 其中批量大小和通道数都为1</span><br><br><br>X = X.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>)<br>Y = Y.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>)<br>lr = <span class="hljs-number">3e-2</span>  <span class="hljs-comment"># 学习率</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    <span class="hljs-keyword">with</span> autograd.record():<br>        Y_hat = conv2d(X)<br>        l = (Y_hat - Y) ** <span class="hljs-number">2</span><br>    l.backward()<br>    <span class="hljs-comment"># 迭代卷积核</span><br>    conv2d.weight.data()[:] -= lr * conv2d.weight.grad()<br>    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;epoch <span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>, loss <span class="hljs-subst">&#123;<span class="hljs-built_in">float</span>(l.<span class="hljs-built_in">sum</span>()):<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出：</p><blockquote><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">epoch</span> <span class="hljs-number">2</span>, loss <span class="hljs-number">4</span>.<span class="hljs-number">949</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">4</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">831</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">6</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">140</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">8</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">024</span><br><span class="hljs-attribute">epoch</span> <span class="hljs-number">10</span>, loss <span class="hljs-number">0</span>.<span class="hljs-number">004</span><span class="hljs-meta"></span><br><span class="hljs-meta">[07:16:32] ../src/base.cc:48: GPU context requested, but no GPUs found.</span><br></code></pre></td></tr></table></figure></blockquote><p>带有步幅和填充的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">conv2d = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), padding=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), stride=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>博客搭建</title>
    <link href="/2024/11/02/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    <url>/2024/11/02/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="一-搭建准备"><a href="#一-搭建准备" class="headerlink" title="一. 搭建准备"></a>一. 搭建准备</h2><p> 搭建之前需要准备的软件：</p><p><a href="https://nodejs.cn/">nodejs</a> </p><p><a href="https://git-scm.com/">git</a> ，git安装详见网上安装步骤</p><h2 id="二，-安装hexo，完成简单本地页面展示"><a href="#二，-安装hexo，完成简单本地页面展示" class="headerlink" title="二， 安装hexo，完成简单本地页面展示"></a>二， 安装hexo，完成简单本地页面展示</h2><p>1.以管理员身份进入cmd窗口输入指令：</p><p><code>npm install -g hexo-cli</code></p><p>2.先创建一个文件夹myblog，在这个文件夹下直接右键git bash打开</p><p>然后初始化hexo</p><p><code>hexo init  // 初始化 hexo g  // 生成静态文件 hexo s  // 启动服务预览</code></p><p>在浏览器中输入<a href="http://localhost:4000/%E5%8D%B3%E5%8F%AF%E7%9C%8B%E5%88%B0">http://localhost:4000/即可看到</a></p><p> <em>新建完成后，指定文件夹目录下有：</em></p><ul><li>node_modules: 依赖包 </li><li>public：存放生成的页面 </li><li>scaffolds：生成文章的一些模板 </li><li>source：用来存放你的文章 t</li><li>hemes：主题 </li><li>_config.yml: 博的配置文件</li></ul><h2 id="三，-变更主题"><a href="#三，-变更主题" class="headerlink" title="三， 变更主题"></a>三， 变更主题</h2><p>在github上搜索：<a href="https://github.com/fluid-dev/hexo-theme-fluid%EF%BC%8C%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6">https://github.com/fluid-dev/hexo-theme-fluid，下载文件</a></p><p><img src="image-20241102204608486.png" alt="image-20241102204608486"></p><p>将文件拷贝到博客根目录themes文件内，同时在根目录创建_config.fluid.yml文件，</p><p><img src="image-20241102204855804.png" alt="image-20241102204855804"></p><p>然后使用记事本打开<code>Blog</code>文件中的<code>_config.yml</code>文件，更换主题名称：</p><p><img src="image-20241102205130911.png" alt="image-20241102205130911"></p><h2 id="四-配置SSH-Key"><a href="#四-配置SSH-Key" class="headerlink" title="四. 配置SSH Key"></a>四. 配置SSH Key</h2><p>鼠标右击打开<code>Git Bash</code></p><p><img src="0ab87d6b4d864f97817f818d0f9b6948.png" alt="img"></p><p>若还没有user.name 和user.email，先配置</p><p><code>git config --global user.name &quot;你的GitHub用户名&quot; git config --global user.email &quot;你的GitHub注册邮箱&quot;</code></p><h3 id="生成ssh-key"><a href="#生成ssh-key" class="headerlink" title="生成ssh key"></a>生成ssh key</h3><p>使用下面命令生成ssh-key</p><p><code>ssh-keygen -t rsa -C &quot;xxx@xxx.com&quot;  // 将 &quot;xxx@xxx.com&quot; 替换为你自己GitHub的邮箱地址</code></p><p>然后一直按 “enter”键，如下图</p><p><img src="image-20241102205558493.png" alt="image-20241102205558493"></p><p>然后到c盘目录（C:\Users\用户名）下查找**.shh<strong>文件，复制</strong>cat id_rsa.pub<strong>文件内的密钥，将内容</strong>全部**复制，找到Github Setting keys页面，新建new SSH Key</p><p><img src="image-20241102210255510.png" alt="image-20241102210255510"></p><h3 id="检查是否设置成功"><a href="#检查是否设置成功" class="headerlink" title="检查是否设置成功"></a>检查是否设置成功</h3><p><code>$ ssh -T git@github.com</code></p><p>看到<strong>successfully</strong>字样就成功了</p><h2 id="五-连接Hexo与GitHub"><a href="#五-连接Hexo与GitHub" class="headerlink" title="五. 连接Hexo与GitHub"></a>五. 连接Hexo与GitHub</h2><p>打开blog文件中的_config.yml（即站点配置文件），翻到最后修改为：</p><blockquote><p>deploy:<br>     type: git<br>     repository:<a href="mailto:&#103;&#105;&#x74;&#64;&#x67;&#x69;&#x74;&#104;&#x75;&#x62;&#x2e;&#99;&#x6f;&#109;">&#103;&#105;&#x74;&#64;&#x67;&#x69;&#x74;&#104;&#x75;&#x62;&#x2e;&#99;&#x6f;&#109;</a>:github邮箱&#x2F;github邮箱.github.io.git<br>     branch: main</p></blockquote><p><img src="image-20241102210556676.png"></p><p>在github上新建一个仓库</p><p><img src="image-20241102211126683.png" alt="image-20241102211126683"></p><p>最后安装Git部署插件：（在根目录下执行，而不是在git bash里）</p><p><code>npm install hexo-deployer-git --save</code></p><p>这时再输入以下命令：</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">hexo c   #清除缓存文件 db<span class="hljs-variable">.json</span> 和已生成的静态文件 public<br>hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo <span class="hljs-keyword">generate</span> 的缩写)<br>hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)<br></code></pre></td></tr></table></figure><p><strong>现在就可以使用xxx.github.io来访问你的博客啦</strong></p><p> <strong>例如：我的用户名是85941837，那么我的博客地址就是<code>85941837.github.io</code></strong></p><h2 id="六-配置域名"><a href="#六-配置域名" class="headerlink" title="六. 配置域名"></a>六. 配置域名</h2><p>在华为云上购买域名，在查询上搜索<strong>域名</strong></p><p><img src="image-20241102211321606.png" alt="image-20241102211321606"></p><p>购买域名，第一次购买需要填写信息模板</p><p><img src="image-20241102211450237.png" alt="image-20241102211450237"></p><p>然后进行域名解析，</p><p><img src="image-20241102211640695.png" alt="image-20241102211640695"></p><p>点击快速添加域名解析，选择网站解析中提供的cname域名，将github上的xxx.github.io填写上即可。</p><p><img src="image-20241102211820124.png" alt="image-20241102211820124"></p><h2 id="七-其他"><a href="#七-其他" class="headerlink" title="七. 其他"></a>七. 其他</h2><h3 id="hexo-d-部署后总需要重新改域名解决办法"><a href="#hexo-d-部署后总需要重新改域名解决办法" class="headerlink" title="hexo d 部署后总需要重新改域名解决办法"></a>hexo d 部署后总需要重新改域名解决办法</h3><p>在source目录下（不是<a href="https://so.csdn.net/so/search?q=hexo&spm=1001.2101.3001.7020">hexo</a>根目录下），创建一个CNAME文件，可以用sublime创建，然后保存成（All files格式）CNAME文件里写自己新的域名</p><p><img src="image-20241102212052474.png" alt="image-20241102212052474"></p><p>hexo g 重新生成一下</p><p>hexo d 部署到github上</p><h3 id="hexo-上传图片本地不加载预览"><a href="#hexo-上传图片本地不加载预览" class="headerlink" title="hexo 上传图片本地不加载预览"></a>hexo 上传图片本地不加载预览</h3><p><strong>修改配置文件</strong>，<strong>修改站点配置</strong><code>_config.yml</code>，将 <code>post_asset_folder</code> 设置为 <code>true</code>，</p><p>然后安装插件：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">npm install hexo-asset-<span class="hljs-built_in">image</span> -- <span class="hljs-built_in">save</span><br></code></pre></td></tr></table></figure><p>如果你使用的是插件，当生成预览的时候，可能依旧无法正常查看图片。</p><p>具体的修改也很简单，我们只需要到 <code>node_modules</code> 中找到 <code>hexo-asset-image</code>，并将 58、89 行的</p><p><img src="hexo-post-publish-2409.png" alt="img"></p><p>修改为：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$(this)<span class="hljs-selector-class">.attr</span>(<span class="hljs-string">&#x27;src&#x27;</span>, <span class="hljs-attribute">src</span>);<br>console<span class="hljs-selector-class">.info</span> &amp;&amp; console<span class="hljs-selector-class">.info</span>(<span class="hljs-string">&quot;update link as:--&gt;&quot;</span> + <span class="hljs-attribute">src</span>);<br></code></pre></td></tr></table></figure><p>然后就能正常显示图片了 ( * ^ _^ * )</p>]]></content>
    
    
    <categories>
      
      <category>博客搭建</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
